oak-f64e8adc$$PropertyIndex cost calculation is faulty$$The cost calculation can easily go out of bounds when it needs to estimate (whenever there are more than 100 nodes). The high value it returns can be higher than the traversal index which has a max of 10M, but can be less smaller.   For example:   100 nodes in the index:   with a single level /content cost is 6250000   adding a second level /content/data cost jumps to 1.544804416E9    101 nodes in the index:   with a single level /content cost is 100   adding a second level /content/data stays at 100    100 nodes, 12 levels deep, cost is 2.147483647E9   101 nodes, 12 levels deep, cost is 6.7108864E7$$patch1-oak-f64e8adc_Developer_PatchNaturalnessYe$$update the overall width calculation of all matching nodes. removed finished flag from ContentMirrorStoreStrategy. removed widthCount > 0. $$1
oak-e12e2052$$IndexRule not respecting inheritence based on mixins$$IndexRule are meant to be applied based on both primaryType and minin type based inheritance. Currently it appears that only primaryType based inheritance is working$$patch1-oak-e12e2052_Developer_PatchNaturalnessYe$$"Revert "" update index definition """. Fixed small issue with MIME type in IndexDefinition. $$1
oak-60186813$$Long overflow in PermissionEntryProviderImpl$$PermissionEntryProviderImpl#init can end up in a Long overflow if the underlying implementation does not know the exact value of the number children, and the child node count is higher than maxSize.  I will attach a patch with a test case$$patch1-oak-60186813_Developer_PatchNaturalnessYe$$added missing import. Fixed a warning .. "Revert "" update cache to handle large principals """. removed empty line for testing. $$1
oak-4af0d4ee$$ArrayIndexOutOfBoundsException in Segment.getRefId()$$It looks like there is some SegmentMK bug that causes the {{Segment.getRefId()}} to throw an {{ArrayIndexOutOfBoundsException}} in some fairly rare corner cases. The data was originally migrated into oak via the crx2oak tool mentioned here: http://docs.adobe.com/docs/en/aem/6-0/deploy/upgrade.html That tool uses *oak-core-1.0.0* creating an oak instance.  Similar to OAK-1566 this system was using FileDataStore with SegmentNodeStore.  In this case the error is seen when running offline compaction using oak-run-1.1-SNAPSHOT.jar (latest).  {code:none} > java -Xmx4096m -jar oak-run-1.1-SNAPSHOT.jar compact /oak/crx-quickstart/repository/segmentstore Apache Jackrabbit Oak 1.1-SNAPSHOT Compacting /wcm/cq-author/crx-quickstart/repository/segmentstore before [data00055a.tar, data00064a.tar, data00045b.tar, data00005a.tar, data00018a.tar, data00022a.tar, data00047a.tar, data00037a.tar, data00049a.tar, data00014a.tar, data00066a.tar, data00020a.tar, data00058a.tar, data00065a.tar, data00069a.tar, data00012a.tar, data00009a.tar, data00060a.tar, data00041a.tar, data00016a.tar, data00072a.tar, data00048a.tar, data00061a.tar, data00053a.tar, data00038a.tar, data00001a.tar, data00034a.tar, data00003a.tar, data00052a.tar, data00006a.tar, data00027a.tar, data00031a.tar, data00056a.tar, data00035a.tar, data00063a.tar, data00068a.tar, data00008v.tar, data00010a.tar, data00043b.tar, data00021a.tar, data00017a.tar, data00024a.tar, data00054a.tar, data00051a.tar, data00057a.tar, data00059a.tar, data00036a.tar, data00033a.tar, data00019a.tar, data00046a.tar, data00067a.tar, data00004a.tar, data00044a.tar, data00013a.tar, data00070a.tar, data00026a.tar, data00002a.tar, data00011a.tar, journal.log, data00030a.tar, data00042a.tar, data00025a.tar, data00062a.tar, data00023a.tar, data00071a.tar, data00032b.tar, data00040a.tar, data00015a.tar, data00029a.tar, data00050a.tar, data00000a.tar, data00007a.tar, data00028a.tar, data00039a.tar] -> compacting Exception in thread "main" java.lang.ArrayIndexOutOfBoundsException: 206 at org.apache.jackrabbit.oak.plugins.segment.Segment.getRefId(Segment.java:191) at org.apache.jackrabbit.oak.plugins.segment.Segment.internalReadRecordId(Segment.java:299) at org.apache.jackrabbit.oak.plugins.segment.Segment.readRecordId(Segment.java:295) at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.getTemplateId(SegmentNodeState.java:69) at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.getTemplate(SegmentNodeState.java:78) at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.getProperties(SegmentNodeState.java:150) at org.apache.jackrabbit.oak.plugins.memory.EmptyNodeState.compareAgainstEmptyState(EmptyNodeState.java:154) at org.apache.jackrabbit.oak.plugins.segment.Compactor CompactDiff.childNodeAdded(Compactor.java:124) at org.apache.jackrabbit.oak.plugins.memory.EmptyNodeState.compareAgainstEmptyState(EmptyNodeState.java:160) at org.apache.jackrabbit.oak.plugins.segment.Compactor CompactDiff.childNodeAdded(Compactor.java:124) at org.apache.jackrabbit.oak.plugins.memory.EmptyNodeState.compareAgainstEmptyState(EmptyNodeState.java:160) at org.apache.jackrabbit.oak.plugins.segment.Compactor CompactDiff.childNodeAdded(Compactor.java:124) at org.apache.jackrabbit.oak.plugins.memory.EmptyNodeState.compareAgainstEmptyState(EmptyNodeState.java:160) at org.apache.jackrabbit.oak.plugins.segment.Compactor CompactDiff.childNodeAdded(Compactor.java:124) at org.apache.jackrabbit.oak.plugins.memory.EmptyNodeState.compareAgainstEmptyState(EmptyNodeState.java:160) at org.apache.jackrabbit.oak.plugins.segment.Compactor CompactDiff.childNodeAdded(Compactor.java:124) at org.apache.jackrabbit.oak.plugins.memory.EmptyNodeState.compareAgainstEmptyState(EmptyNodeState.java:160) at org.apache.jackrabbit.oak.plugins.segment.Compactor CompactDiff.childNodeAdded(Compactor.java:124) at org.apache.jackrabbit.oak.plugins.memory.EmptyNodeState.compareAgainstEmptyState(EmptyNodeState.java:160) at org.apache.jackrabbit.oak.plugins.segment.Compactor CompactDiff.childNodeAdded(Compactor.java:124) at org.apache.jackrabbit.oak.plugins.memory.EmptyNodeState.compareAgainstEmptyState(EmptyNodeState.java:160) at org.apache.jackrabbit.oak.plugins.segment.Compactor CompactDiff.childNodeAdded(Compactor.java:124) at org.apache.jackrabbit.oak.plugins.memory.EmptyNodeState.compareAgainstEmptyState(EmptyNodeState.java:160) at org.apache.jackrabbit.oak.plugins.segment.Compactor CompactDiff.childNodeAdded(Compactor.java:124) at org.apache.jackrabbit.oak.plugins.memory.EmptyNodeState.compareAgainstEmptyState(EmptyNodeState.java:160) at org.apache.jackrabbit.oak.plugins.segment.Compactor CompactDiff.childNodeAdded(Compactor.java:124) at org.apache.jackrabbit.oak.plugins.memory.EmptyNodeState.compareAgainstEmptyState(EmptyNodeState.java:160) at org.apache.jackrabbit.oak.plugins.segment.Compactor CompactDiff.childNodeAdded(Compactor.java:124) at org.apache.jackrabbit.oak.plugins.memory.EmptyNodeState.compareAgainstEmptyState(EmptyNodeState.java:160) at org.apache.jackrabbit.oak.plugins.segment.Compactor CompactDiff.childNodeAdded(Compactor.java:124) at org.apache.jackrabbit.oak.plugins.memory.EmptyNodeState.compareAgainstEmptyState(EmptyNodeState.java:160) at org.apache.jackrabbit.oak.plugins.segment.Compactor CompactDiff.childNodeAdded(Compactor.java:124) at org.apache.jackrabbit.oak.plugins.memory.EmptyNodeState.compareAgainstEmptyState(EmptyNodeState.java:160) at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.compareAgainstBaseState(SegmentNodeState.java:395) at org.apache.jackrabbit.oak.plugins.segment.Compactor.process(Compactor.java:80) at org.apache.jackrabbit.oak.plugins.segment.Compactor.compact(Compactor.java:85) at org.apache.jackrabbit.oak.plugins.segment.file.FileStore.compact(FileStore.java:438) at org.apache.jackrabbit.oak.run.Main.compact(Main.java:311) at org.apache.jackrabbit.oak.run.Main.main(Main.java:133) {code}$$patch1-oak-4af0d4ee_Developer_PatchNaturalnessYe$$added missing import. added checkstyle. Improved JDK5 backwards compatibility with JDK7. $$1
oak-7fca85bf$$IllegalStateException for ValueMap on _revisions$$An IllegalStateException may be thrown by the MergeSortedIterator when _revisions on the root document are read with the ValueMap implementation. It only happens when the local _revisions map has entries that are lower than the most recent split document.$$patch1-oak-7fca85bf_Developer_PatchNaturalnessYe$$"avoid "" invalid state "" assertion ( IDEA - 152896 )". "Revert "" update ValueMap """. removed unused import. workflow improvements - added ability to sort by local map. $$1
oak-916cd92f$$Binaries might get removed by garbage collection while still referenced$$The [Microkernel contract|http://svn.apache.org/repos/asf/jackrabbit/oak/trunk/oak-mk-api/src/main/java/org/apache/jackrabbit/mk/api/MicroKernel.java] specifies a specific format for references to binaries: ":blobId:<blobId>". Currently oak-core uses a different format and thus risks premature garbage collection of such binaries.$$patch1-oak-916cd92f_Developer_PatchNaturalnessYe$$"Add "" blobId "" to TypeCodes map". "OAK - 428 "" Hide "" blobId "" in jsonString". "Revert "" removed "" OBE - added missing bracket """. $$1
oak-f303c916$$SegmentWriter saves references to external blobs$$The new {{SegmentWriteOperation#internalWriteStream}} method checks whether the input stream to write is a {{SegmentStream}}. If it's, writer will reuse existing block ids, rather than storing the whole stream.  It should also check whether the blocks in {{SegmentStream}} comes from the same tracker / segment store. Otherwise this will create invalid references if someone invokes the {{internalWriteStream()}} method with a {{SegmentStream}} created externally.$$patch1-oak-f303c916_Developer_PatchNaturalnessYe$$Fix an issue with the SegmentWriter not checking for block ids in an old gen store .. Remove unnecessary code. $$1
oak-2a489d05$$QueryEngine adding invalid property restriction for fulltext query$$QueryEngine inserts a property restriction of "is not null" for any property used in fulltext constraint. For e.g. for query  {noformat} select * from [nt:unstructured] where CONTAINS([jcr:content/metadata/comment], 'december') {noformat}  A property restriction would be added for {{jcr:content/metadata/comment}}. However currently due to bug in {{FulltextSearchImpl}} [1] the property name generated is {{comment/jcr:content/metadata}}.  {code} @Override     public void restrict(FilterImpl f) {         if (propertyName != null) {             if (f.getSelector().equals(selector)) {                 String p = propertyName;                 if (relativePath != null) {                     p = PathUtils.concat(p, relativePath);                 }                                 p = normalizePropertyName(p);                 restrictPropertyOnFilter(p, f);             }         }         f.restrictFulltextCondition(fullTextSearchExpression.currentValue().getValue(Type.STRING));     } {code}  This happens because {{relativePath}} is passed as second param to {{PathUtils.concat}}. It should be first param  [1] https://github.com/apache/jackrabbit-oak/blob/1.4/oak-core/src/main/java/org/apache/jackrabbit/oak/query/ast/FullTextSearchImpl.java#L275-L286$$patch1-oak-2a489d05_Developer_PatchNaturalnessYe$$"Revert "" fix failing performance test """. $$1
oak-dfa87520$$Lucene index rules should be case insensitive$$Following the lucene index definitions update, the ignored properties are upgraded as a lower case version, but the rest of the lucene bits (indexing) still take the case into account, resulting in the exclude rules being ignored, and properties being indexed.$$patch1-oak-dfa87520_Developer_PatchNaturalnessYe$$added missing import. added case insensitive map of property definition to IndexDefinition. ignore case for property names. convert property names to lower case as match in JDK7. $$1
oak-4416a9f8$$Write operations on Property do not check checked-out state of Node$$Write operations on Property do not check the checked-out state. The same is true for Node.setProperty(..., null).$$patch1-oak-4416a9f8_Developer_PatchNaturalnessYe$$throw version exception if property is checked in. added missing import. Clarify remove method to throw if node is checked out .. Clarify usage of method checkPreconditions ( ) in PropertyImpl # performVoid ( ). Clarify usage of method checkPreconditions ( ) in PropertyImpl # performVoid ( ). $$1
oak-36fe017c$$DocumentNodeStore.dispose() may leave repository in an inconsistent state$$The repository may become inconsistent when a commit happens while the DocumentNodeStore is disposed.   The node store writes back pending _lastRevs and then unset the active flag in the clusterNodes collection. It is possible a commit gets through even after the _lastRevs had been updated and the active flag is cleared. This means the missing _lastRev updates will not be recovered on a restart or by another cluster node.$$patch1-oak-36fe017c_Developer_PatchNaturalnessYe$$add more code on disposing of DocumentNodeStore. added checkOpen ( ) to checkstyle. added checkOpen ( ) to checkstyle. always run in parallel. Added private method to prevent DocStore from being disposed .. $$1
oak-306a9e00$$QueryParse exception when fulltext search performed with term having '/'$$Running the below query, results in Exception pointed by [1]  /jcr:root/content/dam//element(*,dam:Asset)[jcr:contains(jcr:content/metadata/@cq:tags, 'stockphotography:business/business_abstract')] order by @jcr:created descending  Also if you remove the node at /oak:index/damAssetLucene/indexRules/dam:Asset/properties/cqTags  and re-index the /oak:index/damAssetLucene index, the query works.  Seems '/' is special character and needs to be escaped by Oak.  [1] {noformat} Caused by: org.apache.lucene.queryparser.flexible.core.QueryNodeParseException: Syntax Error, cannot parse stockphotography\:business/business_abstract: Lexical error at line 1, column 45.  Encountered: <EOF> after : "/business_abstract"  at org.apache.lucene.queryparser.flexible.standard.parser.StandardSyntaxParser.parse(StandardSyntaxParser.java:74) at org.apache.lucene.queryparser.flexible.core.QueryParserHelper.parse(QueryParserHelper.java:250) at org.apache.lucene.queryparser.flexible.standard.StandardQueryParser.parse(StandardQueryParser.java:168) at org.apache.jackrabbit.oak.plugins.index.lucene.LucenePropertyIndex.tokenToQuery(LucenePropertyIndex.java:1260) ... 138 common frames omitted Caused by: org.apache.lucene.queryparser.flexible.standard.parser.TokenMgrError: Lexical error at line 1, column 45.  Encountered: <EOF> after : "/business_abstract" at org.apache.lucene.queryparser.flexible.standard.parser.StandardSyntaxParserTokenManager.getNextToken(StandardSyntaxParserTokenManager.java:937) at org.apache.lucene.queryparser.flexible.standard.parser.StandardSyntaxParser.jj_scan_token(StandardSyntaxParser.java:945) at org.apache.lucene.queryparser.flexible.standard.parser.StandardSyntaxParser.jj_3R_4(StandardSyntaxParser.java:827) at org.apache.lucene.queryparser.flexible.standard.parser.StandardSyntaxParser.jj_3_2(StandardSyntaxParser.java:739) at org.apache.lucene.queryparser.flexible.standard.parser.StandardSyntaxParser.jj_2_2(StandardSyntaxParser.java:730) at org.apache.lucene.queryparser.flexible.standard.parser.StandardSyntaxParser.Clause(StandardSyntaxParser.java:318) at org.apache.lucene.queryparser.flexible.standard.parser.StandardSyntaxParser.ModClause(StandardSyntaxParser.java:303) at org.apache.lucene.queryparser.flexible.standard.parser.StandardSyntaxParser.ConjQuery(StandardSyntaxParser.java:234) at org.apache.lucene.queryparser.flexible.standard.parser.StandardSyntaxParser.DisjQuery(StandardSyntaxParser.java:204) at org.apache.lucene.queryparser.flexible.standard.parser.StandardSyntaxParser.Query(StandardSyntaxParser.java:166) at org.apache.lucene.queryparser.flexible.standard.parser.StandardSyntaxParser.TopLevelQuery(StandardSyntaxParser.java:147) at org.apache.lucene.queryparser.flexible.standard.parser.StandardSyntaxParser.parse(StandardSyntaxParser.java:65) ... 141 common frames omitted {noformat}$$patch1-oak-306a9e00_Developer_PatchNaturalnessYe$$Oops , now have opengl opengl functionality. added TODO for regex processing. $$1
oak-342809f7$$Inconsistent handling of invalid names/paths$$Passing an invalid name to a JCR method might or might not throw a {{RepositoryException}} depending on whether name re-mappings exist or not:  {code} session.itemExists("/jcr:cont]ent"); {code}  returns {{false}} if no name re-mappings exist but throws a {{RepositoryException}} otherwise.$$patch1-oak-342809f7_Developer_PatchNaturalnessYe$$don ' t allow space - char in namespaces. removed unused variable. removed erroneous legacy comment. "Revert "" allow whitespace in name "" state """. "Revert "" allow leading spaces in name "" section". "Revert "" removed """. remove unused variable. "Revert "" reset trailing spaces """. "Revert "" Invalidating a file with trailing spaces """. JcrPathParser uses backslash instead of backslash. Fixed error in JcrPathParser. we should probably use /. $$1
oak-dd3437d4$$ConcurrentConflictTest fails occasionally$$Occurs every now and then on buildbot. E.g.: http://ci.apache.org/builders/oak-trunk-win7/builds/16$$patch1-oak-dd3437d4_Developer_PatchNaturalnessYe$$Fix # 322. update readme. Fix revision validity check. update workflow. $$1
oak-192ee9e4$$Document split suppressed with steady load on many cluster nodes$$Document split is suppressed when there is a steady write load on many cluster nodes. The document grows bigger over time and leads to poor performance.$$patch1-oak-192ee9e4_Developer_PatchNaturalnessYe$$getMemory ( ) * SPLIT_RATIO + = 1e6a2c. $$1
oak-55a4f738$$Revisit PrivilegeDefinitionStore's use of null as a child name parameter$$As discussed on OAK-635, I'm extracting the PrivilegeDefinitionStore code&patch into a dedicated issue.  Following the discussion on the dev list, I've filed it as a bug, as nulls are not considered valid input parameters.$$patch1-oak-55a4f738_Developer_PatchNaturalnessYe$$added checkNotNull check. added checkNotNull check. avoid merging of same workspaces in Ace workflow. $$1
oak-374e3f3d$$Simple versionable nodes are invalid after migration$$OAK-3836 introduces a support for migrating {{mix:simpleVersionable}} nodes from JCR2 to {{mix:versionable}} nodes in Oak. It changes the mixin type, however it doesn't add required properties: {{jcr:versionHistory}}, {{jcr:baseVersion}} and {{jcr:predecessors}}. As a result, versioning-related methods invoked on such nodes doesn't work correctly.$$patch1-oak-374e3f3d_Developer_PatchNaturalnessYe$$added missing import. add editor hook for security related editor. added static modifier to VersionablePropertiesEditor. $$1
oak-fcd64766$$Mixin based rules not working for relative properties$$If an indexing rule is defined for mixin then it does not work as expected for relative properties.  Issue here being that most of logic in Aggregate class (which is used for relative property handling also) relies on nodes primaryType and does not account for mixin type$$patch1-oak-fcd64766_Developer_PatchNaturalnessYe$$added missing import. added getMixinNames method. "Revert "" update to lucene 1 . 2 . 0 """. "Revert "" update aggregate to exclude unused imports """. Fixing issue # 14061. added matchingType ( ) for node state. "Revert "" add missing "" line to end of YAML file". added check for aggregates from mixins. $$1
oak-25850476$$Revision GC fails when split documents with very long paths are present$$My company is using the MongoDB microkernel with Oak, and we've noticed that the daily revision GC is failing with errors like this: {code} 13.07.2015 13:06:16.261 *ERROR* [pool-7-thread-1-Maintenance Queue(com/adobe/granite/maintenance/job/RevisionCleanupTask)] org.apache.jackrabbit.oak.management.ManagementOperation Revision garbage collection failed java.lang.IllegalArgumentException: 13:h113f9d0fe7ac0f87fa06397c37b9ffd4b372eeb1ec93e0818bb4024a32587820 at org.apache.jackrabbit.oak.plugins.document.Revision.fromString(Revision.java:236) at org.apache.jackrabbit.oak.plugins.document.SplitDocumentCleanUp.disconnect(SplitDocumentCleanUp.java:84) at org.apache.jackrabbit.oak.plugins.document.SplitDocumentCleanUp.disconnect(SplitDocumentCleanUp.java:56) at org.apache.jackrabbit.oak.plugins.document.VersionGCSupport.deleteSplitDocuments(VersionGCSupport.java:53) at org.apache.jackrabbit.oak.plugins.document.VersionGarbageCollector.collectSplitDocuments(VersionGarbageCollector.java:117) at org.apache.jackrabbit.oak.plugins.document.VersionGarbageCollector.gc(VersionGarbageCollector.java:105) at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreService 2.run(DocumentNodeStoreService.java:511) at org.apache.jackrabbit.oak.spi.state.RevisionGC 1.call(RevisionGC.java:68) at org.apache.jackrabbit.oak.spi.state.RevisionGC 1.call(RevisionGC.java:64) at java.util.concurrent.FutureTask.run(FutureTask.java:262) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745) {code}  I've narrowed the issue down to the disconnect(NodeDocument) method of the [SplitDocumentCleanUp class|https://svn.apache.org/repos/asf/jackrabbit/oak/trunk/oak-core/src/main/java/org/apache/jackrabbit/oak/plugins/document/SplitDocumentCleanUp.java]. The method always tries to extract the path of the node from its ID, but this won't work for documents whose path is very long because those documents will have the hash of their path in the ID.  I believe this code should fix the issue, but I haven't had a chance to actually try it: {code}     private void disconnect(NodeDocument splitDoc) {         String mainId = Utils.getIdFromPath(splitDoc.getMainPath());         NodeDocument doc = store.find(NODES, mainId);         if (doc == null) {             LOG.warn("Main document {} already removed. Split document is {}",                     mainId, splitId);             return;         }         String path = splitDoc.getPath();         int slashIdx = path.lastIndexOf('/');         int height = Integer.parseInt(path.substring(slashIdx + 1));         Revision rev = Revision.fromString(                 path.substring(path.lastIndexOf('/', slashIdx - 1) + 1, slashIdx));         doc = doc.findPrevReferencingDoc(rev, height);         if (doc == null) {             LOG.warn("Split document {} not referenced anymore. Main document is {}",                     splitId, mainId);             return;         }         // remove reference         if (doc.getSplitDocType() == INTERMEDIATE) {             disconnectFromIntermediate(doc, rev);         } else {             markStaleOnMain(doc, rev, height);         }     } {code} By using getPath(), the code should automatically use either the ID or the _path property, whichever is right for the document.$$patch1-oak-25850476_Developer_PatchNaturalnessYe$$added missing splitId info. $$1
oak-7552a10b$$Locking issues seen with CopyOnWrite mode enabled$$When CopyOnWrite mode is enabled and incremental mode is enabled i.e. {{indexPath}} property set then failure in any indexing cycle would prevent further indexing from progressing. For e.g. if any indexing cycle fails then subsequent indexing cycle would fail with Lucene locking related exception  {noformat} Caused by: org.apache.lucene.store.LockObtainFailedException: Lock obtain timed out: NativeFSLock@/tmp/junit8067118705344013640/2c26b46b68ffc68ff99b453c1d30413413422d706483bfa0f98a5e886266e7ae/1/write.lock 	at org.apache.lucene.store.Lock.obtain(Lock.java:89) 	at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:707) 	at org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexEditorContext.getWriter(LuceneIndexEditorContext.java:169) 	at org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexEditor.addOrUpdate(LuceneIndexEditor.java:293) 	... 37 more {noformat}  Any further indexing would continue to fail with this exception$$patch1-oak-7552a10b_Developer_PatchNaturalnessYe$$Remove unused imports. added missing import. add note on indexing in Oak is single threaded. $$1
oak-57bd2dc5$$DocumentNS may expose branch commit on earlier revision$$The DocumentNodeStore may expose the changes of a branch on a revision earlier than it's commit revision. This only happens when the read revision equals the revision of the not yet merged changes on the branch.$$patch1-oak-57bd2dc5_Developer_PatchNaturalnessYe$$remove ignore branch flag from NodeDocument. $$1
oak-0c3b3306$$Guard against invalid/missing checkpoints$$Playing with the backup revealed a case where a checkpoint can become invalid after a manual restore of the repository. [0] The NodeStore#retrieve apis already specify that this can return null in the case the checkpoint doesn't exist anymore, but it looks like the storage bits aren't yet prepared for that scenario.    [0] {noformat} org.apache.sling.commons.scheduler.impl.QuartzScheduler Exception during job execution of org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate@3a6d47 : Failed to load segment 8a8b281c-1a02-4950-aad5-aad8e436a0d8 java.lang.IllegalStateException: Failed to load segment 8a8b281c-1a02-4950-aad5-aad8e436a0d8 	at org.apache.jackrabbit.oak.plugins.segment.AbstractStore.readSegment(AbstractStore.java:109) ~[na:na] 	at org.apache.jackrabbit.oak.plugins.segment.Segment.getSegment(Segment.java:189) ~[na:na] 	at org.apache.jackrabbit.oak.plugins.segment.Record.getSegment(Record.java:97) ~[na:na] 	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.getTemplate(SegmentNodeState.java:56) ~[na:na] 	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.getChildNode(SegmentNodeState.java:209) ~[na:na] 	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStore.retrieve(SegmentNodeStore.java:175) ~[na:na] 	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService.retrieve(SegmentNodeStoreService.java:198) ~[na:na] 	at org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate.run(AsyncIndexUpdate.java:97) ~[na:na] 	at org.apache.sling.commons.scheduler.impl.QuartzJobExecutor.execute(QuartzJobExecutor.java:105) ~[org.apache.sling.commons.scheduler-2.4.2.jar:na] 	at org.quartz.core.JobRunShell.run(JobRunShell.java:207) [org.apache.sling.commons.scheduler-2.4.2.jar:na] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_40] 	at java.util.concurrent.ThreadPoolExecutor Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_40] 	at java.lang.Thread.run(Thread.java:724) [na:1.7.0_40] Caused by: java.lang.IllegalStateException: Segment 8a8b281c-1a02-4950-aad5-aad8e436a0d8 not found 	at org.apache.jackrabbit.oak.plugins.segment.file.FileStore.loadSegment(FileStore.java:184) ~[na:na] {noformat}$$patch1-oak-0c3b3306_Developer_PatchNaturalnessYe$$Improved JDK5 backwards compatibility with JDK7. $$1
oak-c7669f31$$Reindex removes all nodes under index definition node$$Reindex logic in {{IndexUpdate}} removes all child node from index definition node thus removing valid nodes which might be part of index defintion. It should only remove hidden nodes$$patch1-oak-c7669f31_Developer_PatchNaturalnessYe$$added missing import. don ' t remove hidden nodes in index update. $$1
oak-5f863af6$$Evaluation with restriction is not consistent with parent ACLs$$consider the following ACL setup:  {noformat} testuser allow rep:read,rep:write      /testroot testuser deny  jcr:removeNode /testroot/a  glob=*/c testuser allow jcr:removeNode /testroot/a  glob=*/b {noformat}  now: {{hasPermission(/tesroot/a/b/c, jcr:removeNode) == false}} but the user is still able to delete the node.  * if we change the order of the ACEs with the restriction, it works (i.e. the user can't delete) * if we use direct ACLs on the respective nodes, it works  I think this is a bug...but I'm not sure if {{hasPermission}} is wrong, or the check during node deletion.$$patch1-oak-5f863af6_Developer_PatchNaturalnessYe$$Fix 145. fix compile - time refactoring. make predicate implementation compatible with Hudson. Fixed allow bit bit structure. "Revert "" update to previous commit """. Fix isGranted ( ) method. Add toString ( ) to PermissionEntry. "Revert "" update predicate to respect parent """. "Fixed issue where "" parentPath "" in permission predicate is set to null". "Apply "" apply "" to permission entries by default". $$1
oak-6d8146f8$$Item names with trailing spaces should not be allowed$$the following should fail:  {code}         Node hello = session.getRootNode().addNode("hello");         session.save();          Node illegal = hello.addNode("test "); <-- here         session.save();          assertEquals("/hello/test ", illegal.getPath()); <-- and here          Node other = session.getNode("/hello/test "); <-- and here         assertTrue(other.isSame(illegal));         assertTrue(session.nodeExists("/hello/test ")); <-- and here {code}$$patch1-oak-6d8146f8_Developer_PatchNaturalnessYe$$improve namespace check. $$1
oak-2e20589f$$CacheLIRS implementation incomplete$$The current CacheLIRS implementation is not complete and e.g. does not provide a write through ConcurrentMap view on {{asMap()}}. For OAK-1088 it would be good to have this implementation as it allows conditional and atomic updates of cache entries.$$patch1-oak-2e20589f_Developer_PatchNaturalnessYe$$Fix NPE in CacheLIRS. $$1
oak-6feacf6b$$AssertionError in MemoryNodeBuilder$${code}     NodeBuilder root = ...     NodeBuilder child = root.child("new");      root.removeNode("new");     child.getChildNodeCount(); {code}  The last line throws an {{AssertionError}} when no node named "new" existed initially. It throws an {{IllegalStateException}} as expected otherwise.$$patch1-oak-6feacf6b_Developer_PatchNaturalnessYe$$Improved method name. Fix a check for node existence. Fixed # 242. added revision support for memory node builders. $$1
oak-b939aa6e$$Sysview import of single valued mv property creates sv property$$See test in filevault [0].  it imports a multivalue property that only has 1 value, via [1]. the same test succeeds in jackrabbit 2.0, but fails in oak 1.3.14  [0] https://github.com/apache/jackrabbit-filevault/blob/jackrabbit-filevault-3.1.26/vault-core/src/test/java/org/apache/jackrabbit/vault/packaging/integration/TestUserContentPackage.java#L297-L326 [1] https://github.com/apache/jackrabbit-filevault/blob/jackrabbit-filevault-3.1.26/vault-core/src/main/java/org/apache/jackrabbit/vault/fs/impl/io/JcrSysViewTransformer.java#L146-L148$$patch1-oak-b939aa6e_Developer_PatchNaturalnessYe$$don ' t delete multiple status fields when importing property. $$1
oak-d10362c0$$AsyncIndexer fails due to FileNotFoundException thrown by CopyOnWrite logic$$At times the CopyOnWrite reports following exception  {noformat} 15.07.2015 14:20:35.930 *WARN* [pool-58-thread-1] org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate The async index update failed org.apache.jackrabbit.oak.api.CommitFailedException: OakLucene0004: Failed to close the Lucene index 	at org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexEditor.leave(LuceneIndexEditor.java:204) 	at org.apache.jackrabbit.oak.plugins.index.IndexUpdate.leave(IndexUpdate.java:219) 	at org.apache.jackrabbit.oak.spi.commit.VisibleEditor.leave(VisibleEditor.java:63) 	at org.apache.jackrabbit.oak.spi.commit.EditorDiff.process(EditorDiff.java:56) 	at org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate.updateIndex(AsyncIndexUpdate.java:366) 	at org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate.run(AsyncIndexUpdate.java:311) 	at org.apache.sling.commons.scheduler.impl.QuartzJobExecutor.execute(QuartzJobExecutor.java:105) 	at org.quartz.core.JobRunShell.run(JobRunShell.java:207) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) 	at java.util.concurrent.ThreadPoolExecutor Worker.run(ThreadPoolExecutor.java:615) 	at java.lang.Thread.run(Thread.java:745) Caused by: java.io.FileNotFoundException: _2s7.fdt 	at org.apache.lucene.store.FSDirectory.fileLength(FSDirectory.java:261) 	at org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopier CopyOnWriteDirectory COWLocalFileReference.fileLength(IndexCopier.java:837) 	at org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopier CopyOnWriteDirectory.fileLength(IndexCopier.java:607) 	at org.apache.lucene.index.SegmentCommitInfo.sizeInBytes(SegmentCommitInfo.java:141) 	at org.apache.lucene.index.DocumentsWriterPerThread.sealFlushedSegment(DocumentsWriterPerThread.java:529) 	at org.apache.lucene.index.DocumentsWriterPerThread.flush(DocumentsWriterPerThread.java:502) 	at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:508) 	at org.apache.lucene.index.DocumentsWriter.flushAllThreads(DocumentsWriter.java:618) 	at org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:3147) 	at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:3123) 	at org.apache.lucene.index.IndexWriter.closeInternal(IndexWriter.java:988) 	at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:932) 	at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:894) 	at org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexEditorContext.closeWriter(LuceneIndexEditorContext.java:192) 	at org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexEditor.leave(LuceneIndexEditor.java:202) 	... 10 common frames omitted {noformat}$$patch1-oak-d10362c0_Developer_PatchNaturalnessYe$$added missing import. added shared working set map. added getIndexPathForLogging ( ) to IndexCopier wrapper. added missing synchronization. Add COW / JDK 1 . 6 constructor to CopyOnReadDirectory. add shared working set. Add shared working set to IndexCopier constructor. added shared working set to indexCopier. added clear on read. added error message. $$1
oak-df9e6913$$Calling addNode on a node that has orderable child nodes violates specification$$it seems to me that the current behavior of Node.addNode for a node that  has orderable child nodes violates the specification (section 23.3):  {quote} 23.3 Adding a New Child Node When a child node is added to a node that has orderable child nodes it is added to the end of the list. {quote}  however, the following test will fail:  {code} @Test     public void testAddNode() throws Exception {         new TestContentLoader().loadTestContent(getAdminSession());          Session session = getAdminSession();         Node test = session.getRootNode().addNode("test", "test:orderableFolder");         assertTrue(test.getPrimaryNodeType().hasOrderableChildNodes());          Node n1 = test.addNode("a");         Node n2 = test.addNode("b");         session.save();          NodeIterator it = test.getNodes();         assertEquals("a", it.nextNode().getName());         assertEquals("b", it.nextNode().getName());     } {code}$$patch1-oak-df9e6913_Developer_PatchNaturalnessYe$$added ordering to NodeImpls. fixing bug # 1435 added orderBefore ( ). $$1
oak-7c62bd81$$PhraseQuery fails due to missing posiion info in indexed fields$$Following OAK-1487 I've introduced a regression in the indexing of fields on the Lucene index. There are some types of queries (the ones that use property restrictions) that cannot run anymore.  bq. /jcr:root/content/dam//*[jcr:contains(jcr:content/metadata/@dc:format, 'application/pdf')]   bq. Caused by: java.lang.IllegalStateException: field "dc:format" was indexed without position data; cannot run PhraseQuery (term=text)  I could not reproduce this in an unit test so far.$$patch1-oak-7c62bd81_Developer_PatchNaturalnessYe$$Fixed a bug in LuceneIndex. Fixed LuceneIndex . extractMatchingTokens ( ) where the parameter is not used. Fixed bug in IndexingFulltextQueryIndex. $$1
oak-db19e70f$$Tree.getStatus() and Tree.getPropertyStatus() fail for items whose parent has been removed$$None$$patch1-oak-db19e70f_Developer_PatchNaturalnessYe$$Improved javadoc in TreeImpl . parent. Fix bug # 257. Added getOwnPropertyStatus ( ) to TreeImpl # internalGetPropertyStatus ( ). can ' t build the path of a removed tree. $$1
oak-0fa892b3$$issues with JsopBuilder.encode and .escape$$1) escape() escapes many characters that do not need to be escaped (>127)  2) encode() does not encode many control characters that would need to be escaped when read through a JSON parser.$$patch1-oak-0fa892b3_Developer_PatchNaturalnessYe$$Allow json escaping of backslash - escaped characters. removed unused local variable. Fix JsopBuilder to ignore unicode characters. Fix JsopBuilder use of int instead of string . format. $$1
oak-2ac1dccd$$NodeDocument.getNewestRevision() incorrect when there are previous documents$$The method may incorrectly return null when there are previous documents and the base revision is lower than all local changes.  This is most likely caused by changes done for OAK-3388.$$patch1-oak-2ac1dccd_Developer_PatchNaturalnessYe$$add clusterIds of local changes as well. getRevisionNewer ( context , lower , r ) =. $$1
oak-e115fd90$$Trailing slash not removed for simple path in JCR to Oak path conversion$$While converting from JCR path to Oak path the trailing slashes are not removed for simple paths. They are removed for complex path  {code} assertEquals("/oak-foo:bar/oak-quu:qux",npMapper.getOakPath("/foo:bar/quu:qux/")); assertEquals("/a/b/c",npMapper.getOakPath("/a/b/c/"));     } {code}  Of the two cases above the first one passes while the second one fails$$patch1-oak-e115fd90_Developer_PatchNaturalnessYe$$remove unused variable. Add back hasTrailingSlash. "Revert "" update to have . . . """. "Revert "" removed last char of jcrPath """. added parse errors. $$1
oak-e1ae968c$$MongoMK: split documents when they are too large$$Currently, the MongoMK stores all revisions of a node in the same document. Once there are many revisions, the document gets very large.  The plan is to split the document when it gets big.  It looks like this isn't just a "nice to have", but also a problem for some use cases. Example stack trace:  {code} 21.07.2013 12:35:47.554 *ERROR* ... Caused by: java.lang.IllegalArgumentException: 'ok' should never be null... 	at com.mongodb.CommandResult.ok(CommandResult.java:48) 	at com.mongodb.DBCollection.findAndModify(DBCollection.java:375) 	at org.apache.jackrabbit.oak.plugins.mongomk.MongoDocumentStore.findAndModify(MongoDocumentStore.java:302) 	... 32 more {code}  at the same time in the MongoDB log:  {code} Sun Jul 21 12:35:47.334 [conn7] warning: log line attempted (159k) over max size(10k),  printing beginning and end ...  Assertion: 10334:BSONObj size: 16795219 (0x53460001) is invalid.  Size must be between 0 and 16793600(16MB)  First element: :childOrder: { r1400279f22d-0-1: "[]", ... {code}$$patch1-oak-e1ae968c_Developer_PatchNaturalnessYe$$added long used for testing build break. workflow improvements - add workflow improvements. Fixing small issue in Controller implementation. Fixing # 7751. $$1
oak-01f5a26f$$Troublesome ExternalIdentityRef.equals(Object) implementation$$in the light of OAK-3508 i looked at the {{ExternalIdentifyRef}} class and found the following implementation of {{Object.equals(Object)}}:  {code} public boolean equals(Object o) {         try {             // assuming that we never compare other types of classes             return this == o || string.equals(((ExternalIdentityRef) o).string);         } catch (Exception e) {             return false;         }     } {code}  since this class is public and exported as part of a public API, i don't think the assumption made in the code is justified. also i would argue that catching {{Exception}} is bad style as is exception driven development. in this particular case it was IMHO perfectly trivial to just get rid of the catch clause altogether.$$patch1-oak-01f5a26f_Developer_PatchNaturalnessYe$$Fixed issue with invalid external authentication provider name parameter. Allow null string input. Allow null values for literals. Fixed a bug in Controller . equals. $$1
oak-024e5d37$$Repository upgrade does not copy default values of property definitions$$The {{RepositoryUpgrade}} class needs to copy also the default values of property definitions in the node types being upgraded. See the TODO in https://github.com/apache/jackrabbit-oak/blob/jackrabbit-oak-0.20.0/oak-upgrade/src/main/java/org/apache/jackrabbit/oak/upgrade/RepositoryUpgrade.java#L485.$$patch1-oak-024e5d37_Developer_PatchNaturalnessYe$$added missing imports. added missing import. added missing import. removed unused imports. added inverse hash to prevent adding nodes to the upgrade map. added missing method. added missing throw. added missing prefixToUri for childNodeDefinition. added missing throws. Fix defaultValues for UpgradeDef. Fix oak upgrade error. $$1
oak-dcadb0e1$$UUID collision check is not does not work in transient space$$I think OAK-1037 broke the system view import.  test case: 1. create a new node with a uuid (referenceable, or new user) 2. import systemview with IMPORT_UUID_COLLISION_REPLACE_EXISTING 3. save()  result: {noformat} javax.jcr.nodetype.ConstraintViolationException: OakConstraint0030: Uniqueness constraint violated at path [/] for one of the property in [jcr:uuid] having value e358efa4-89f5-3062-b10d-d7316b65649e {noformat}  expected: * imported content should replace the existing node - even in transient space.  note: * if you perform a save() after step 1, everything works.$$patch1-oak-dcadb0e1_Developer_PatchNaturalnessYe$$removed unused imports. Added missing mandatory imports. Improve import script. CAMEL - 6618 Add missing import step. IDEA - 77215 avoid adding empty uuids for imports. Fixing performance optimisation on import - > import state. $$1
oak-24f7f60a$$Node.addNode(String, String) may check nt-mgt-permission against the wrong node$$While I was troubleshooting an issue we're having in AEM 6.1, I've noticed an "impossible" access denied exception in the logs: the user had permission to add nodes under the node in question but still got an error.  Some testing narrowed the issue down to a difference in behavior between the following two invocations: {{someNode.getNode("child").addNode("grandchild", "nt:unstructured");}} {{someNode.addNode("child/grandchild", "nt:unstructured");}}  As far as I can tell, both should behave identically per the JCR spec, but the second one fails if the user doesn't have node type management permission to someNode, even if they have that permission to someNode/child.  I believe the issue is in line 283 of [NodeImpl|https://svn.apache.org/repos/asf/jackrabbit/oak/trunk/oak-jcr/src/main/java/org/apache/jackrabbit/oak/jcr/session/NodeImpl.java]: it is checking permissions against dlg.getTree(), but it should really check against parent.getTree(), or if possible, the path of the node that's about to be created (so glob restrictions can be evaluated).$$patch1-oak-24f7f60a_Developer_PatchNaturalnessYe$$Fixed permission check for node types , parent is not used .. $$1
oak-90ea7aa5$$NPE in DocumentNodeStore#retrieve for non existing checkpoint$$Said method throws a NPE when passing it a valid revision identifier from a non existing checkpoint.$$patch1-oak-90ea7aa5_Developer_PatchNaturalnessYe$$added missing import. added missing import. avoid NPE. $$1
oak-00df38d2$$Adding a node with the name of a removed node can lead to an inconsistent hierarchy of node builders$$None$$patch1-oak-00df38d2_Developer_PatchNaturalnessYe$$fix merge bug. $$1
oak-a5ff019e$$Sometimes hierarchy conflict between concurrent add/delete isn't detected$$I'm not sure of exact set of event that led to an incident on one of our test clusters. The cluster is running 3 AEM instances based on oak build at 1.3.10.r1713699 backed by a single mongo 3 instance.  Unfortunately, we found the issue too late and logs had rolled over. Here's the exception that showed over and over as workflow jobs were (trying to) being processed: {noformat} ....         at java.lang.Thread.run(Thread.java:745) Caused by: javax.jcr.InvalidItemStateException: OakMerge0004: OakMerge0004: The node 8:/oak:index/event.job.topic/:index/com%2Fadobe%2Fgranite%2Fworkflow%2Ftransient%2Fjob%2Fetc%2Fworkflow%2Fmodels%2Fdam-xmp-writeback%2Fjcr_content%2Fmodel/var/eventing/jobs/assigned was already added in revision r151233e54e1-0-4, before r15166378b6a-0-2 (retries 5, 6830 ms)         at org.apache.jackrabbit.oak.api.CommitFailedException.asRepositoryException(CommitFailedException.java:239)         at org.apache.jackrabbit.oak.api.CommitFailedException.asRepositoryException(CommitFailedException.java:212)         at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.newRepositoryException(SessionDelegate.java:669)         at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.save(SessionDelegate.java:495)         at org.apache.jackrabbit.oak.jcr.session.SessionImpl 8.performVoid(SessionImpl.java:419)         at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.performVoid(SessionDelegate.java:273)         at org.apache.jackrabbit.oak.jcr.session.SessionImpl.save(SessionImpl.java:416)         at org.apache.sling.jcr.resource.internal.helper.jcr.JcrResourceProvider.commit(JcrResourceProvider.java:634)         ... 16 common frames omitted Caused by: org.apache.jackrabbit.oak.api.CommitFailedException: OakMerge0004: OakMerge0004: The node 8:/oak:index/event.job.topic/:index/com%2Fadobe%2Fgranite%2Fworkflow%2Ftransient%2Fjob%2Fetc%2Fworkflow%2Fmodels%2Fdam-xmp-writeback%2Fjcr_content%2Fmodel/var/eventing/jobs/assigned was already added in revision r151233e54e1-0-4, before r15166378b6a-0-2 (retries 5, 6830 ms)         at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch.merge0(DocumentNodeStoreBranch.java:200)         at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch.merge(DocumentNodeStoreBranch.java:123)         at org.apache.jackrabbit.oak.plugins.document.DocumentRootBuilder.merge(DocumentRootBuilder.java:158)         at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.merge(DocumentNodeStore.java:1497)         at org.apache.jackrabbit.oak.core.MutableRoot.commit(MutableRoot.java:247)         at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.commit(SessionDelegate.java:346)         at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.save(SessionDelegate.java:493)         ... 20 common frames omitted Caused by: org.apache.jackrabbit.oak.plugins.document.ConflictException: The node 8:/oak:index/event.job.topic/:index/com%2Fadobe%2Fgranite%2Fworkflow%2Ftransient%2Fjob%2Fetc%2Fworkflow%2Fmodels%2Fdam-xmp-writeback%2Fjcr_content%2Fmodel/var/eventing/jobs/assigned was already added in revision r151233e54e1-0-4, before r15166378b6a-0-2         at org.apache.jackrabbit.oak.plugins.document.Commit.checkConflicts(Commit.java:582)         at org.apache.jackrabbit.oak.plugins.document.Commit.createOrUpdateNode(Commit.java:487)         at org.apache.jackrabbit.oak.plugins.document.Commit.applyToDocumentStore(Commit.java:371)         at org.apache.jackrabbit.oak.plugins.document.Commit.applyToDocumentStore(Commit.java:265)         at org.apache.jackrabbit.oak.plugins.document.Commit.applyInternal(Commit.java:234)         at org.apache.jackrabbit.oak.plugins.document.Commit.apply(Commit.java:219)         at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch.persist(DocumentNodeStoreBranch.java:290)         at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch.persist(DocumentNodeStoreBranch.java:260)         at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch.access 300(DocumentNodeStoreBranch.java:54)         at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch InMemory.merge(DocumentNodeStoreBranch.java:498)         at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch.merge0(DocumentNodeStoreBranch.java:180)         ... 26 common frames omitted .... {noformat}  Doing following removed repo corruption and restored w/f processing: {noformat} oak.removeDescendantsAndSelf("/oak:index/event.job.topic/:index/com%2Fadobe%2Fgranite%2Fworkflow%2Ftransient%2Fjob%2Fetc%2Fworkflow%2Fmodels%2Fdam-xmp-writeback%2Fjcr_content%2Fmodel/var/eventing/jobs/assigned") {noformat}  Attaching [mongoexport output|^mongoexport.zip] for {{/oak:index/event.job.topic/:index/com%2Fadobe%2Fgranite%2Fworkflow%2Ftransient%2Fjob%2Fetc%2Fworkflow%2Fmodels%2Fdam-xmp-writeback%2Fjcr_content%2Fmodel/var/eventing/jobs/assigned/6a389a6a-a8bf-4038-b57b-cb441c6ac557/com.adobe.granite.workflow.transient.job.etc.workflow.models.dam-xmp-writeback.jcr_content.model/2015/11/19/23/54/6a389a6a-a8bf-4038-b57b-cb441c6ac557_10}} (the hierarchy created at {{r151233e54e1-0-4}}). I've renamed a few path elements to make it more reable though (e.g. {{:index/com%2Fadobe%2Fgranite%2Fworkflow%2Ftransient%2Fjob%2Fetc%2Fworkflow%2Fmodels%2Fdam-xmp-writeback%2Fjcr_content%2Fmodel}} -> {{enc_value}}).  [~mreutegg], I'm assigning it to myself for now, but I think this would require your expertise all the way :).$$patch1-oak-a5ff019e_Developer_PatchNaturalnessYe$$"avoid "" multiple adds "" in one more commit .". "Revert "" update conflict message """. "CPP - 5108 Add "" allowConcurrentAddRemove "" method is used by the executor. $$1
oak-0c3e3d70$$Range check fails with IllegalArgumentException$${{Range.includes()}} fails with IllegalArgumentException when provided revision is from another cluster node:  {noformat} java.lang.IllegalArgumentException: Trying to compare revisions of different cluster ids: r142f43d2f0f-0-2 and r142f43d46fb-0-1 	at org.apache.jackrabbit.oak.plugins.mongomk.Revision.compareRevisionTime(Revision.java:84) 	at org.apache.jackrabbit.oak.plugins.mongomk.Range.includes(Range.java:55) {noformat}  The IllegalArgumentException was introduced with OAK-1274.$$patch1-oak-0c3e3d70_Developer_PatchNaturalnessYe$$"Add @ Nonnull to "" includes "" method". $$1
oak-e6c31270$$Inconsistency in Node#setProperty in case of null value$$Setting a null value to a single valued property will result in 'null' being returned while executing the same on a multivalued property will return the removed property.  jr2 returned the removed property in both cases as far as i  remember and i would suggest that we don't change that behavior. in particular since the specification IMO doesn't allow to return null-values for these methods.$$patch1-oak-e6c31270_Developer_PatchNaturalnessYe$$Fix failing tests. Fix a crash in node - remove ( ). $$1
oak-35562cce$$PropertyIndex only considers the cost of a single indexed property$$The existing PropertyIndex loops through the PropertyRestriction objects in the Filter and essentially only calculates the cost of the first indexed property. This isn't actually the first property in the query and Filter.propertyRestrictions is a HashMap.  More confusingly, the plan for a query with multiple indexed properties outputs *all* indexed properties, even though only the first one is used.  For queries with multiple indexed properties, the cheapest property index should be used in all three relevant places: when calculating the cost, when executing the query, and when producing the plan.$$patch1-oak-35562cce_Developer_PatchNaturalnessYe$$added missing import. added log level for PropertyIndex. "Revert "" property cost is not null """. Cheapest property cost is not loged. "Revert "" update to property index """. "Revert "" break workflow workflow workflow workflow workflow workflow """. added missing break. "avoid "" cheapest property "" restriction". "Revert "" update to higher cost property restriction """. $$1
oak-002c5845$$Lucene index / compatVersion 2: search for 'a=b=c' does not work$$Similar to OAK-3879, we need to escape '=' (althoug lucene [escape()|https://github.com/apache/lucene-solr/blob/releases/lucene-solr/4.7.1/lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParserBase.java#L988] apparently doesn't escape it).  Due to this searching for {{a=b=c}} throws parse exception from lucene query parser. Also, searching for {{a=b}} gives incorrect result.$$patch1-oak-002c5845_Developer_PatchNaturalnessYe$$added '=' to lucene query operators. $$1
oak-89317b28$$Hierarchy conflict detection broken$$Hierarchy conflict detection is broken in 1.0.14. It may happen that a child document is created even though the parent is considered deleted.$$patch1-oak-89317b28_Developer_PatchNaturalnessYe$$Fixing # 383. Fix UpdateOp constructor. add conditions. avoid condition copy. add "" conditions "" if it is not null """. "Revert "" update op """. "Revert "" updateCommitRoot with collision checks """. $$1
oak-b2ca8baa$$Property Index: cost calculation is wrong (zero) when searching for many values$$Currently, for queries of the form   {code} select * from [nt:unstructured] where type = 'xyz' {code}  the node type index is used in some cases, even if there is an index on the property "type". The reason is that the cost for the node type index is 0 due to a bug. The node type index internally uses the property index on the property "jcr:primaryType", and asks for the cost using all possible children node types of "nt:unstructured". The returned cost is 0 because of this bug. The cost estimation is an extrapolation of the number of entries for the first 3 values. It is currently coded as:  {code} count = count / size / i; {code}  when in fact it should be written as:  {code} count = count * size / i; {code}$$patch1-oak-b2ca8baa_Developer_PatchNaturalnessYe$$extrapolate the count as well. $$1
oak-438e31a7$$Better support for RangeIterators$$Currently all RangeIterators returned from the JCR API don't implement the {{getSize()}} method but rather return {{-1}}. We should return the size of the iterator if and where feasible.$$patch1-oak-438e31a7_Developer_PatchNaturalnessYe$$removed unused import. added static import. added getChildCount ( ) to NodeImpl. added propertyCount method to NodeImpl. $$1
oak-9772f5b2$$Initial read of _lastRev creates incorrect RevisionComparator$$The logic in backgroundRead(false) orders the local lastRev  before external lastRev. This the last change done by the local cluster node will look as if it happend before a potentially older external change.$$patch1-oak-9772f5b2_Developer_PatchNaturalnessYe$$added missing import. getHeadRevision ( ) must return a null value. remove false to pull in changes from other cluster nodes. remove unused parameter. Keep old behaviour unchanged for debugging .. added missing line. Fix an issue where the diff cache could become corrupt after a merge. log external revision timestamps in workflow. remove falsey flag. $$1
oak-4bfbfcdd$$ContentMirrorStoreStrategy should utilize path restriction when available$$Currently {{ContentStoreMirrorStrategy}} has a mirror of content path under {{:index}}. Yet, while {{query}} (and {{count}}) methods doesn't jump directly into restricted path.  This would be very useful for {{PropertyIndex}} where the queries can be optimized by supplying a path restriction along with an indexed property restriction (I don't know if queries with references would use paths so much though)$$patch1-oak-4bfbfcdd_Developer_PatchNaturalnessYe$$add filter to PropertyIndexLookup # count. "removed unused "" TYPE "" index from OrderedPropertyIndex". "Revert "" update to OrderedPropertyIndex with more custom logic """. added missing path prefix. added pathPrefix for OrderedPropertyIndex. added missing closing bracket. "remove "" prefix "" cursor from "" paths "" query". added missing closing backslash. add @ opt for optimized query execution. Added missing javadoc. added filter to property index plan. added filter to property index plan. Fixed static inner class definition. Added missing Override annotation. "Revert "" update to latest opengl """. Remove erroneous legacy comment. Improved cursors that use relative paths together with other cursors. added query ( ) method. added path prefix to query ( ). added pathPrefix to query ( ). Fix path prefix added by using the + 1 as the < - 1 would be the case if. added pathPrefix parameter to query results. Fixed opengling problem. added pathPrefix parameter to QueryResultsWrapper. added missing closing parenthesis. added missing @ Override. adding filterRootPath in last commit. don ' t count nodes if filterRootPath is null. added filterPath and pathPrefix to ContentMirrorStoreStrategy. "Revert "" update iterator to use "" include root """. added relative path to filter path. added pathPrefix + currentPath to result of ContentMirrorStoreStrategy. added default implementation of count. $$1
oak-147515ae$$Async Update fails after IllegalArgumentException$$The async index update can fail due to a mismatch between an index definition and the actual content. If that is the case, it seems that it can no longer make any progress. Instead it re-indexes the latest changes over and over again until it hits the problematic property.  Discussion at http://markmail.org/thread/42bixzkrkwv4s6tq  Stacktrace attached.$$patch1-oak-147515ae_Developer_PatchNaturalnessYe$$Log warning if property multivalued is not supported. Fixed warning for later release notes. $$1
oak-668f08f2$$Incomplete journal when move and copy operations are involved$$Given a node at /source:  {code} head = mk.commit("/",     ">\"source\" : \"moved\"" +     "*\"moved\" : \"copy\"",     head, ""); {code}  results in the following journal:  {code} >"/source":"/copy" {code}  where the freshly created node at /moved is missing.  Similarly   {code} head = mk.commit("/",     "*\"source\" : \"copy\"" +     ">\"copy\" : \"moved\"",     head, ""); {code}  results in  {code} +"/moved":{} {code}  where moved away node at /source is missing.$$patch1-oak-668f08f2_Developer_PatchNaturalnessYe$$update message and changes. added missing fields in AbstractCommit. Added getChanges ( ) method to AbstractCommit. added serialize ( ) method to Commit object. removed unused imports. added missing import. "Revert "" update to latest opengl """. Remove old comment. "Revert "" update to latest cache """. Improved javadoc comment .. parse node recursively. Fixed bug in JsopTokenizer. add changes ( copied from old commit ). Add method that allows setting of changes in a commit message. Add changes parameter. Increasing size for method addNode ( ). apply changes on staged change log in order to avoid concurrent modifications. added changeLog to CommitBuilder. added NodeTree to json. added asDiff method. added missing asDiff method. added missing throw. added asDiff. $$1
oak-a7f0e808$$NPE in the TypeValidator when using the Lucene Index$$None$$patch1-oak-a7f0e808_Developer_PatchNaturalnessYe$$Avoid NPE in ValidatingHook. Avoid NPE in ValidatingHook. $$1
oak-004db804$$XPath queries with certain combinations of "or" conditions don't use an index$$XPath queries with the following conditions are not converted to "union" SQL-2 queries and therefore don't use an index:  {noformat} /jcr:root/content//*[((@i = '1' or @i = '2') or (@s = 'x')) and (@t = 'a' or @t = 'b')] {noformat}$$patch1-oak-004db804_Developer_PatchNaturalnessYe$$removed unused import. added Expression . optimize ( ). Fixed small issue with Expression . getLeft ( ). Fixed small issue with Expression # getCommonLeftPart ( ). added optimize ( ) method in Expression. fixed in condition class. always use optimize ( ) method of AndCondition. Add an override method to Expression # containsFullTextCondition. override isName ( ) in Expression. Add full text condition implementation. added Expression. "Revert "" update to latest opengl """. Remove unused private fields. optimize query xpath. fixed full text search. Clarify a use case for contains in xpath query. fixed # 77. $$1
oak-7d72e6ed$$Query: for joins, sometimes no or the wrong index is used$$Currently, no index is used for the join condition. For example, the query:  {code} select * from [nodeTypeA] as a  inner join [nodeTypeB] as b on isdescendantnode(b, a)  where lower(a.x) = 'y' and b.[property] is not null {code}  currently doesn't take into account that the path of the selector 'a' is known at the time selector 'b' is accessed (given that selector 'a' is executed first). So in this case, the query would use an index on the property b.[property], even if this index has a very bad selectivity (many nodes with this problem), or the query would use the node type index on [nodeTypeB], even if there are many nodes of this type.  Instead, most likely the query should do a traversal, using the isdescendantnode(b, a) join condition.$$patch1-oak-7d72e6ed_Developer_PatchNaturalnessYe$$improve log. Adding a private modifier to FilterImpl # preparing. Add a method to set preparing flag on FilterImpl .. Fix an issue with the join condition ' s restrictPath method. Added missing Javadoc. Remove source . getPlan ( rootState ) ; unify with Query . java. update log. Added private method to Query . getPlan ( ). update join impl. added hint for selector2 path being null. added a hack to make sure that selector is properly prepared. Fixed a bug in EquiJoinConditionImpl # restrictProperty. update internals. added isPrepared method. don ' t index query by default. Add a missing javadoc. fix a merge bug in DescendantNodeJoinConditionImpl. $$1
oak-33c18762$$LastRevRecoveryAgent can update _lastRev of children but not the root$$As mentioned in [OAK-2131|https://issues.apache.org/jira/browse/OAK-2131?focusedCommentId=14616391&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14616391] there can be a situation wherein the LastRevRecoveryAgent updates some nodes in the tree but not the root. This seems to happen due to OAK-2131's change in the Commit.applyToCache (where paths to update are collected via tracker.track): in that code, paths which are non-root and for which no content has changed (and mind you, a content change includes adding _deleted, which happens by default for nodes with children) are not 'tracked', ie for those the _lastRev is not update by subsequent backgroundUpdate operations - leaving them 'old/out-of-date'. This seems correct as per description/intention of OAK-2131 where the last revision can be determined via the commitRoot of the parent. But it has the effect that the LastRevRecoveryAgent then finds those intermittent nodes to be updated while as the root has already been updated (which is at first glance non-intuitive).  I'll attach a test case to reproduce this.  Perhaps this is a bug, perhaps it's ok. [~mreutegg] wdyt?$$patch1-oak-33c18762_Developer_PatchNaturalnessYe$$"Revert "" update last rev recovery agent to ignore deleted properties """. added missing import. "avoid "" invalid state "" assertion ( according to Duden it ' s intentional ). "Revert "" update last modified revision of "" doc """. $$1
oak-29e5b734$$Redundent entries in effective policies per principal-set$$when retrieving the effective policies for a given set of principals the resulting array of policies contains redundant entries if a given policy contains multiple ACEs for the given set of principals.$$patch1-oak-29e5b734_Developer_PatchNaturalnessYe$$"Revert "" update to latest ace access control api """. Add missing imports. Fix null pointer check in AccessControlManagerImpl .. Fixed bug where path in effective set of rules was being added twice. $$1
oak-7ba9dd66$$DocumentNodeStore revision GC removes intermediate docs$$The revision garbage collection in DocumentNodeStore removes intermediate documents of the revision history of a node even if it is still in use.$$patch1-oak-7ba9dd66_Developer_PatchNaturalnessYe$$added missing import. "Revert "" update split doc """. Improved method to validate only deleted local fields after the split. "remove "" INTERMEDIATE "" from GC_TYPES". $$1
oak-e403e003$$Path parsing must support SNS indexes, irrespective of SNS support$${code} Session.getNode("/foo/bar[2]"); {code}  throws {{javax.jcr.RepositoryException: Invalid name or path: /foo/bar\[2]}}  This should be an ItemNotFoundException (if the item does not exist), irrespective if the repository supports SNS or not.$$patch1-oak-e403e003_Developer_PatchNaturalnessYe$$added check for SNS path existence ( added check for 1 ). $$1
oak-3535afe2$$Session.nodeExists("/foo/bar[2]") must not throw PathNotFoundException$$similar to OAK-1216, Session.nodeExists() of an SNS path with indexes > 1 should return false.$$patch1-oak-3535afe2_Developer_PatchNaturalnessYe$$Handle PathNotFoundException in readOperation ( ). Handle case when absPath is null. $$1
oak-c215b267$$TarMK compaction can create mixed segments$$As described in http://markmail.org/message/ujkqdlthudaortxf, commits that occur while the compaction operation is running can make the compacted segments contain references to older data segments, which prevents old data from being reclaimed during cleanup.$$patch1-oak-c215b267_Developer_PatchNaturalnessYe$$"Revert "" update to latest compactor changes """. update Compactor. redundant constructor. $$1
oak-08f0b280$$Possible overflow in checkpoint creation$$Creating a checkpoint with {{Long.MAX_VALUE}} lifetime will overflow the value, allowing the store to immediately release the checkpoint.$$patch1-oak-08f0b280_Developer_PatchNaturalnessYe$$Fixed bug with segment node history. $$1
oak-83427028$$Property value converion ignores reisdual property definition$$Assume following node type which a property defined with type and a residual unnamed property also defined  {noformat} [oak:foo]  - stringProp (String)  - * (undefined) {noformat}  For such node type if a property {{stringProp}} is being set with a binary value then Oak converts it into a String property thereby causing binary stream to change. In JR2 conversion would not happen as conversion logic treats setting (stringProp,BINARY) as a residual property$$patch1-oak-83427028_Developer_PatchNaturalnessYe$$Fixing # 690 by adding a few more types and adding node delegates .. workflow improvements - improve implementation of NodeDelegate. Fixed fuzzy match in NodeDelegate. $$1
oak-94110f21$$Provide Simple Exception Name in Credentials Attribute for PW Expiry$$currently upon encountering a pw history exception while changing the password of a user, the credential attribute is set with the FQ class name, instead of the simple name. this requires consumers (e.g. sling) to use oak package names instead of a simple class name to react to the situation.$$patch1-oak-94110f21_Developer_PatchNaturalnessYe$$"Revert "" update to "" use username "" instead of class name """. $$1
oak-621a5101$$Query test failures on buildbot$$Since revision 1398915 various query tests fail on [buildbot|http://ci.apache.org/builders/oak-trunk/builds/784]:  {code} sql1(org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexQueryTest): No LoginModules configured for jackrabbit.oak sql2(org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexQueryTest): No LoginModules configured for jackrabbit.oak xpath(org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexQueryTest): No LoginModules configured for jackrabbit.oak bindVariableTest(org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexQueryTest): No LoginModules configured for jackrabbit.oak sql1(org.apache.jackrabbit.oak.plugins.index.property.PropertyIndexQueryTest): No LoginModules configured for jackrabbit.oak sql2(org.apache.jackrabbit.oak.plugins.index.property.PropertyIndexQueryTest): No LoginModules configured for jackrabbit.oak xpath(org.apache.jackrabbit.oak.plugins.index.property.PropertyIndexQueryTest): No LoginModules configured for jackrabbit.oak bindVariableTest(org.apache.jackrabbit.oak.plugins.index.property.PropertyIndexQueryTest): No LoginModules configured for jackrabbit.oak sql2Explain(org.apache.jackrabbit.oak.plugins.index.old.QueryTest): No LoginModules configured for jackrabbit.oak sql1(org.apache.jackrabbit.oak.plugins.index.old.QueryTest): No LoginModules configured for jackrabbit.oak xpath(org.apache.jackrabbit.oak.plugins.index.old.QueryTest): No LoginModules configured for jackrabbit.oak bindVariableTest(org.apache.jackrabbit.oak.plugins.index.old.QueryTest): No LoginModules configured for jackrabbit.oak sql1(org.apache.jackrabbit.oak.query.index.TraversingIndexQueryTest): No LoginModules configured for jackrabbit.oak sql2(org.apache.jackrabbit.oak.query.index.TraversingIndexQueryTest): No LoginModules configured for jackrabbit.oak xpath(org.apache.jackrabbit.oak.query.index.TraversingIndexQueryTest): No LoginModules configured for jackrabbit.oak bindVariableTest(org.apache.jackrabbit.oak.query.index.TraversingIndexQueryTest): No LoginModules configured for jackrabbit.oak {code}$$patch1-oak-621a5101_Developer_PatchNaturalnessYe$$Warn about failed logins. $$1
oak-78c37386$$NPE in MarkSweepGarbageCollector.saveBatchToFile during Datastore GC with FileDataStore$$During running a datastore garbage collection on a Jackrabbit 2 FileDataStore (org.apache.jackrabbit.oak.plugins.blob.datastore.FileDataStore, see http://jackrabbit.apache.org/oak/docs/osgi_config.html) an NPE is thrown {code} 13.05.2014 17:50:16.944 *ERROR* [qtp1416657193-147] org.apache.jackrabbit.oak.management.ManagementOperation Blob garbage collection failed java.lang.RuntimeException: Error in retrieving references 	at org.apache.jackrabbit.oak.plugins.blob.MarkSweepGarbageCollector 1.addReference(MarkSweepGarbageCollector.java:395) 	at org.apache.jackrabbit.oak.plugins.segment.Segment.collectBlobReferences(Segment.java:248) 	at org.apache.jackrabbit.oak.plugins.segment.SegmentTracker.collectBlobReferences(SegmentTracker.java:178) 	at org.apache.jackrabbit.oak.plugins.segment.SegmentBlobReferenceRetriever.collectReferences(SegmentBlobReferenceRetriever.java:38) 	at org.apache.jackrabbit.oak.plugins.blob.MarkSweepGarbageCollector.iterateNodeTree(MarkSweepGarbageCollector.java:361) 	at org.apache.jackrabbit.oak.plugins.blob.MarkSweepGarbageCollector.mark(MarkSweepGarbageCollector.java:201) 	at org.apache.jackrabbit.oak.plugins.blob.MarkSweepGarbageCollector.markAndSweep(MarkSweepGarbageCollector.java:173) 	at org.apache.jackrabbit.oak.plugins.blob.MarkSweepGarbageCollector.collectGarbage(MarkSweepGarbageCollector.java:149) 	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService 2.collectGarbage(SegmentNodeStoreService.java:185) 	at org.apache.jackrabbit.oak.plugins.blob.BlobGC 1.call(BlobGC.java:68) 	at org.apache.jackrabbit.oak.plugins.blob.BlobGC 1.call(BlobGC.java:64) 	at java.util.concurrent.FutureTask.run(FutureTask.java:262) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) 	at java.util.concurrent.ThreadPoolExecutor Worker.run(ThreadPoolExecutor.java:615) 	at java.lang.Thread.run(Thread.java:745) Caused by: java.lang.NullPointerException: null 	at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:192) 	at com.google.common.base.Joiner.toString(Joiner.java:436) 	at com.google.common.base.Joiner.appendTo(Joiner.java:108) 	at com.google.common.base.Joiner.appendTo(Joiner.java:152) 	at com.google.common.base.Joiner.join(Joiner.java:193) 	at com.google.common.base.Joiner.join(Joiner.java:183) 	at org.apache.jackrabbit.oak.plugins.blob.MarkSweepGarbageCollector.saveBatchToFile(MarkSweepGarbageCollector.java:317) 	at org.apache.jackrabbit.oak.plugins.blob.MarkSweepGarbageCollector 1.addReference(MarkSweepGarbageCollector.java:391) 	... 14 common frames omitted {code}  Attached you find the OSGi config for both the nodestore and the datastore.$$patch1-oak-78c37386_Developer_PatchNaturalnessYe$$Fix bug in SegmentWriter. $$1
oak-c91bfa54$$DataStoreBlobStore does not take into maxLastModifiedTime when fetching all chunks$$Currently the {{DataStoreBlobStore}} has a pending TODO  {code}  @Override     public Iterator<String> getAllChunkIds(long maxLastModifiedTime) throws Exception {         //TODO Ignores the maxLastModifiedTime currently.         return Iterators.transform(delegate.getAllIdentifiers(), new Function<DataIdentifier, String>() {             @Nullable             @Override             public String apply(@Nullable DataIdentifier input) {                 return input.toString();             }         });     } {code}  Due to this it currently returns all blobId. This would issue when new binary gets created while a blob gc is running as such binaries might be considered orphan and deleted$$patch1-oak-c91bfa54_Developer_PatchNaturalnessYe$$added maxLastModifiedTime condition. $$1
oak-f2bb1a17$$MutableTree#isNew: replace implementation by NodeBuilder#isNew$$Similar to the issue described in OAK-1177 we may consider replacing the implementation of MutableTree#isNew by the corresponding call on the NodeBuilder.  See also OAK-947.$$patch1-oak-f2bb1a17_Developer_PatchNaturalnessYe$$made rootBase private. added reset ( ) to the kernel root builder. added missing imports. "Revert "" update to 2005 06 07 """. Improved javadoc in MutableTree. Improved javadoc in MutableTree. Fix javadoc for MutableTree . isVisible ( ). added missing import. fixing mongomk issue. added reset ( ) to MongoRootBuilder. Improve memory node builder. $$1
oak-7441a3d5$$Index path property should be considered optional for copy on read logic$$As part of changes done for OAK-4347 logic assumes that indexPath is always non null. This works fine for fresh setup where the indexPath would have been set by the initial indexing. However for upgraded setup this assumption would break as it might happen that index does not get updated with new approach and before that a read is performed.  Currently with updated code on upgraded setup following exception is seen   {noformat} Caused by: javax.security.auth.login.LoginException: java.lang.NullPointerException: Index path property [:indexPath] not found         at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:236)         at org.apache.jackrabbit.oak.plugins.index.lucene.IndexDefinition.getIndexPathFromConfig(IndexDefinition.java:664)         at org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopier.getSharedWorkingSet(IndexCopier.java:242)         at org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopier.wrapForRead(IndexCopier.java:140)         at org.apache.jackrabbit.oak.plugins.index.lucene.IndexNode.open(IndexNode.java:53)         at org.apache.jackrabbit.oak.plugins.index.lucene.IndexTracker.findIndexNode(IndexTracker.java:179)         at org.apache.jackrabbit.oak.plugins.index.lucene.IndexTracker.acquireIndexNode(IndexTracker.java:154)         at org.apache.jackrabbit.oak.plugins.index.lucene.LucenePropertyIndex.getPlans(LucenePropertyIndex.java:250) {noformat}  For this specific flow the indexPath can be passed in and not looked up from IndexDefinition$$patch1-oak-7441a3d5_Developer_PatchNaturalnessYe$$Fix log indexing for read ( ). remove unused variable. $$1
oak-786b3d76$$Lucene suggestions index definition can't be restricted to a specific type of node$$While performing a suggestor query like   {code} SELECT [rep:suggest()] as suggestion  FROM [nt:unstructured] WHERE suggest('foo') {code}  Suggestor does not provide any result. In current implementation, [suggestions|http://jackrabbit.apache.org/oak/docs/query/lucene.html#Suggestions] in Oak work only for index definitions for {{nt:base}} nodetype. So, an index definition like: {code:xml}     <lucene-suggest         jcr:primaryType="oak:QueryIndexDefinition"         async="async"         compatVersion="{Long}2"         type="lucene">         <indexRules jcr:primaryType="nt:unstructured">             <nt:base jcr:primaryType="nt:unstructured">                 <properties jcr:primaryType="nt:unstructured">                     <description                         jcr:primaryType="nt:unstructured"                         analyzed="{Boolean}true"                         name="description"                         propertyIndex="{Boolean}true"                         useInSuggest="{Boolean}true"/>                 </properties>             </nt:base>         </indexRules>     </lucene-suggest> {code} works, but if we change nodetype to {{nt:unstructured}} like: {code:xml}     <lucene-suggest         jcr:primaryType="oak:QueryIndexDefinition"         async="async"         compatVersion="{Long}2"         type="lucene">         <indexRules jcr:primaryType="nt:unstructured">             <nt:unstructured jcr:primaryType="nt:unstructured">                 <properties jcr:primaryType="nt:unstructured">                     <description                         jcr:primaryType="nt:unstructured"                         analyzed="{Boolean}true"                         name="description"                         propertyIndex="{Boolean}true"                         useInSuggest="{Boolean}true"/>                 </properties>             </nt:base>         </indexRules>     </lucene-suggest> {code} , it won't work.  The issue is that suggestor implementation essentially is passing a pseudo row with path=/.: {code:title=LucenePropertyIndex.java}     private boolean loadDocs() { ...                         queue.add(new LuceneResultRow(suggestedWords)); ... {code} and {code:title=LucenePropertyIndex.java}         LuceneResultRow(Iterable<String> suggestWords) {             this.path = "/";             this.score = 1.0d;             this.suggestWords = suggestWords;         } {code} Due to path being set to "/", {{SelectorImpl}} later filters out the result as {{rep:root}} (primary type of "/") isn't a {{nt:unstructured}}.$$patch1-oak-786b3d76_Developer_PatchNaturalnessYe$$Indexing LuceneResultRow constructor returns a unique index value. Fixed isVirtualRow ( ) checkstyle. Indexing LuceneResultRow constructor private. Fixed small issue in IndexWriter. $$1
oak-17032c50$$Intermittent IllegalMonitorStateException seen while releaseing IndexNode$$At times following exception seen. On this system the index got corrupted because backing index files got deleted from the system and hence index is not accessible.   {noformat} 21.09.2015 09:26:36.764 *ERROR* [FelixStartLevel] com.adobe.granite.repository.impl.SlingRepositoryManager start: Uncaught Throwable trying to access Repository, calling stopRepository() java.lang.IllegalMonitorStateException: attempt to unlock read lock, not locked by current thread         at java.util.concurrent.locks.ReentrantReadWriteLock Sync.unmatchedUnlockException(ReentrantReadWriteLock.java:444)         at java.util.concurrent.locks.ReentrantReadWriteLock Sync.tryReleaseShared(ReentrantReadWriteLock.java:428)         at java.util.concurrent.locks.AbstractQueuedSynchronizer.releaseShared(AbstractQueuedSynchronizer.java:1341)         at java.util.concurrent.locks.ReentrantReadWriteLock ReadLock.unlock(ReentrantReadWriteLock.java:881)         at org.apache.jackrabbit.oak.plugins.index.lucene.IndexNode.release(IndexNode.java:121)         at org.apache.jackrabbit.oak.plugins.index.lucene.LucenePropertyIndex.getPlans(LucenePropertyIndex.java:212)         at org.apache.jackrabbit.oak.query.QueryImpl.getBestSelectorExecutionPlan(QueryImpl.java:847)         at org.apache.jackrabbit.oak.query.QueryImpl.getBestSelectorExecutionPlan(QueryImpl.java:793)         at org.apache.jackrabbit.oak.query.ast.SelectorImpl.prepare(SelectorImpl.java:283)         at org.apache.jackrabbit.oak.query.QueryImpl.prepare(QueryImpl.java:568)         at org.apache.jackrabbit.oak.query.QueryEngineImpl.executeQuery(QueryEngineImpl.java:183)         at org.apache.jackrabbit.oak.security.user.UserProvider.getAuthorizableByPrincipal(UserProvider.java:234)         at org.apache.jackrabbit.oak.security.user.UserManagerImpl.getAuthorizable(UserManagerImpl.java:116)         at org.apache.jackrabbit.oak.security.principal.PrincipalProviderImpl.getAuthorizable(PrincipalProviderImpl.java:140)         at org.apache.jackrabbit.oak.security.principal.PrincipalProviderImpl.getPrincipal(PrincipalProviderImpl.java:69)         at org.apache.jackrabbit.oak.spi.security.principal.CompositePrincipalProvider.getPrincipal(CompositePrincipalProvider.java:50)         at org.apache.jackrabbit.oak.spi.security.principal.PrincipalManagerImpl.getPrincipal(PrincipalManagerImpl.java:47)         at com.adobe.granite.repository.impl.SlingRepositoryManager.setupPermissions(SlingRepositoryManager.java:997)         at com.adobe.granite.repository.impl.SlingRepositoryManager.createRepository(SlingRepositoryManager.java:420)         at com.adobe.granite.repository.impl.SlingRepositoryManager.acquireRepository(SlingRepositoryManager.java:290)         at org.apache.sling.jcr.base.AbstractSlingRepositoryManager.start(AbstractSlingRepositoryManager.java:304)         at com.adobe.granite.repository.impl.SlingRepositoryManager.activate(SlingRepositoryManager.java:267)         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)         at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)         at java.lang.reflect.Method.invoke(Method.java:483)         at org.apache.felix.scr.impl.helper.BaseMethod.invokeMethod(BaseMethod.java:222)         at org.apache.felix.scr.impl.helper.BaseMethod.access 500(BaseMethod.java:37)         at org.apache.felix.scr.impl.helper.BaseMethod Resolved.invoke(BaseMethod.java:615)         at org.apache.felix.scr.impl.helper.BaseMethod.invoke(BaseMethod.java:499)         at org.apache.felix.scr.impl.helper.ActivateMethod.invoke(ActivateMethod.java:295)         at org.apache.felix.scr.impl.manager.SingleComponentManager.createImplementationObject(SingleComponentManager.java:302)         at org.apache.felix.scr.impl.manager.SingleComponentManager.createComponent(SingleComponentManager.java:113)         at org.apache.felix.scr.impl.manager.SingleComponentManager.getService(SingleComponentManager.java:832)         at org.apache.felix.scr.impl.manager.SingleComponentManager.getServiceInternal(SingleComponentManager.java:799)         at org.apache.felix.scr.impl.manager.AbstractComponentManager.activateInternal(AbstractComponentManager.java:724)         at org.apache.felix.scr.impl.manager.DependencyManager SingleStaticCustomizer.addedService(DependencyManager.java:927)         at org.apache.felix.scr.impl.manager.DependencyManager SingleStaticCustomizer.addedService(DependencyManager.java:891)         at org.apache.felix.scr.impl.manager.ServiceTracker Tracked.customizerAdded(ServiceTracker.java:1492)         at org.apache.felix.scr.impl.manager.ServiceTracker Tracked.customizerAdded(ServiceTracker.java:1413)         at org.apache.felix.scr.impl.manager.ServiceTracker AbstractTracked.trackAdding(ServiceTracker.java:1222)         at org.apache.felix.scr.impl.manager.ServiceTracker AbstractTracked.track(ServiceTracker.java:1158)         at org.apache.felix.scr.impl.manager.ServiceTracker Tracked.serviceChanged(ServiceTracker.java:1444)         at org.apache.felix.framework.util.EventDispatcher.invokeServiceListenerCallback(EventDispatcher.java:987)         at org.apache.felix.framework.util.EventDispatcher.fireEventImmediately(EventDispatcher.java:838)         at org.apache.felix.framework.util.EventDispatcher.fireServiceEvent(EventDispatcher.java:545)         at org.apache.felix.framework.Felix.fireServiceEvent(Felix.java:4547)         at org.apache.felix.framework.Felix.registerService(Felix.java:3521)         at org.apache.felix.framework.BundleContextImpl.registerService(BundleContextImpl.java:348)         at org.apache.sling.commons.threads.impl.Activator.start(Activator.java:55)         at org.apache.felix.framework.util.SecureAction.startActivator(SecureAction.java:697)         at org.apache.felix.framework.Felix.activateBundle(Felix.java:2223)         at org.apache.felix.framework.Felix.startBundle(Felix.java:2141)         at org.apache.felix.framework.Felix.setActiveStartLevel(Felix.java:1368)         at org.apache.felix.framework.FrameworkStartLevelImpl.run(FrameworkStartLevelImpl.java:308)         at java.lang.Thread.run(Thread.java:745) {noformat}  Above exception happens at  {code} for (String path : indexPaths) {             try {                 indexNode = tracker.acquireIndexNode(path);                  if (indexNode != null) {                     IndexPlan plan = new IndexPlanner(indexNode, path, filter, sortOrder).getPlan();                     if (plan != null) {                         plans.add(plan);                     }                 }             } finally {                 if (indexNode != null) {                     indexNode.release();                 }             }         } {code}  It has been ensured that if indexNode is initialized then it has been acquired. So only way for such an exception to happen is that in a loop of say 2 paths {{indexNode}} got initialized for Loop 1 and then while acquiring in Loop 2 the indexNode still refers to old released value and that would cause the exception. The fix should be simply to null the variable once released$$patch1-oak-17032c50_Developer_PatchNaturalnessYe$$Fixed error in LucenePropertyIndex . getPlans ( ). $$1
oak-7ae92779$$SegmentNodeStore rebase operation assumes wrong child node order$$This popped up during the async merge process. The merge first does a rebase which can fail, making some index files look like they disappeared [0], wrapping the actual root cause.  The problem is that the rebase failed and removed the missing file. This can be seen by analyzing the ':conflict' marker info: bq. addExistingNode {_b_Lucene41_0.doc, _b.fdx, _b.fdt, _b_4.del, } so it points to something trying to add some index related files twice, almost like a concurrent commit exception.  Digging even deeper I found that the rebase operation during the state comparison phase assumes a certain order of child nodes [1], and based on that tries to read the mentioned nodes again, thinking that they are new ones, when if fact they are already present in the list [2]. This causes a conflict which fails the entire async update process, but also any lucene search, as the index files are now gone and the index is in a corrupted state.   [0]  {noformat} *WARN* [pool-5-thread-2] org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate Index update async failed org.apache.jackrabbit.oak.api.CommitFailedException: OakLucene0004: Failed to close the Lucene index 	at org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexEditor.leave(LuceneIndexEditor.java:122) 	at org.apache.jackrabbit.oak.spi.commit.VisibleEditor.leave(VisibleEditor.java:64) 	at org.apache.jackrabbit.oak.spi.commit.VisibleEditor.leave(VisibleEditor.java:64) 	at org.apache.jackrabbit.oak.plugins.index.IndexUpdate.leave(IndexUpdate.java:129) 	at org.apache.jackrabbit.oak.spi.commit.EditorDiff.process(EditorDiff.java:56) 	at org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate.run(AsyncIndexUpdate.java:100) 	at org.apache.sling.commons.scheduler.impl.QuartzJobExecutor.execute(QuartzJobExecutor.java:105) 	at org.quartz.core.JobRunShell.run(JobRunShell.java:207) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) 	at java.util.concurrent.ThreadPoolExecutor Worker.run(ThreadPoolExecutor.java:615) 	at java.lang.Thread.run(Thread.java:724) Caused by: java.io.FileNotFoundException: _b_Lucene41_0.doc at org.apache.jackrabbit.oak.plugins.index.lucene.OakDirectory.openInput(OakDirectory.java:145) {noformat}  [1] http://svn.apache.org/viewvc/jackrabbit/oak/trunk/oak-core/src/main/java/org/apache/jackrabbit/oak/plugins/segment/MapRecord.java?view=markup#l329  [2] before child list {noformat} [_b_Lucene41_0.doc, _b.fdx, _b.fdt, segments_34, _b_4.del, _b_Lucene41_0.pos, _b.nvm, _b.nvd, _b.fnm, _3n.si, _b_Lucene41_0.tip, _b_Lucene41_0.tim, _3n.cfe, segments.gen, _3n.cfs, _b.si] {noformat}  after list {noformat} _b_Lucene41_0.pos, _3k.cfs, _3j_1.del, _b.nvm, _b.nvd, _3d.cfe, _3d.cfs, _b.fnm, _3j.si, _3h.si, _3i.cfe, _3i.cfs, _3e_2.del, _3f.si, _b_Lucene41_0.tip, _b_Lucene41_0.tim, segments.gen, _3e.cfe, _3e.cfs, _b.si,_3g.si, _3l.si, _3i_1.del, _3d_3.del, _3e.si, _3d.si, _b_Lucene41_0.doc, _3h_2.del, _3i.si, _3k_1.del, _3j.cfe, _3j.cfs, _b.fdx, _b.fdt, _3g_1.del, _3k.si, _3l.cfe, _3l.cfs, segments_33, _3f_1.del, _3h.cfe, _3h.cfs, _b_4.del, _3f.cfe, _3f.cfs, _3g.cfe, _3g.cfs {noformat}$$patch1-oak-7ae92779_Developer_PatchNaturalnessYe$$update MapRecord with better hash handling. Fixed read only. Fixed error in MapRecord . compareTo. Fixed hash compare logic. Add HASH_MASK to MapEntry. Fixed accidentely removed hash code for MapEntry. $$1
oak-cdb34ffc$$FileStore.flush prone to races leading to corruption$$There is a small window in {{FileStore.flush}} that could lead to data corruption: if we crash right after setting the persisted head but before any delay-flushed {{SegmentBufferWriter}} instance flushes (see {{SegmentBufferWriterPool.returnWriter()}}) then that data is lost although it might already be referenced from the persisted head.  We need to come up with a test case for this.   A possible fix would be to return a future from {{SegmentWriter.flush}} and rely on a completion callback. Such a change would most likely also be useful for OAK-3690.$$patch1-oak-cdb34ffc_Developer_PatchNaturalnessYe$$Add missing import. Adding a monitor for segment buffer write safety. Fix potential deadlocks with readers .. "removed the "" wait "" keyword from the WriteOperationHandler interface". $$1
oak-daf9a4ef$$RootImplFuzzIT test failures$$As seen in the CI build, {{RootImplFuzzIT}} fails every now and then. This might be because of OAK-174, but there's been quite a bit of other work on the same area, so this could be caused also by something else.  The troublesome seeds as seen in failing CI builds are 1437930918, 206057576, 1638075186, 1705736349, -1856261793 and 569172885.$$patch1-oak-daf9a4ef_Developer_PatchNaturalnessYe$$don ' t commit if currentRoot is not the new root node. "Revert "" update to latest root node state after purge - > notifyListeners """. $$1
oak-99996c25$$OakDirectory not usable in readOnly mode with a readOnly builder$$When using {{OakDirectory}} with a read only builder say in LuceneCommand in oak-console following error is seen  {noformat} lc info /oak:index/users ERROR java.lang.UnsupportedOperationException: This builder is read-only.        at org.apache.jackrabbit.oak.spi.state.ReadOnlyBuilder.unsupported (ReadOnlyBuilder.java:45)        at org.apache.jackrabbit.oak.spi.state.ReadOnlyBuilder.child (ReadOnlyBuilder.java:190)        at org.apache.jackrabbit.oak.spi.state.ReadOnlyBuilder.child (ReadOnlyBuilder.java:35)        at org.apache.jackrabbit.oak.plugins.index.lucene.OakDirectory.<init> (OakDirectory.java:93)        at org.apache.jackrabbit.oak.plugins.index.lucene.OakDirectory.<init> (OakDirectory.java:87)        at org.apache.jackrabbit.oak.console.commands.LuceneCommand.getDirectory (LuceneCommand.groovy:128)        at org.apache.jackrabbit.oak.console.commands.LuceneCommand.this 4 getDirectory (LuceneCommand.groovy)        at org.apache.jackrabbit.oak.console.commands.LuceneCommand _closure1.doCall (LuceneCommand.groovy:55) {noformat}$$patch1-oak-99996c25_Developer_PatchNaturalnessYe$$added missing ChildNode. $$1
oak-f4349a96$$ClusterNodeInfo does not pick an existing entry on startup$$When the {{DocumentNodeStore}} starts up, it attempts to find an entry that matches the current instance (which is defined by something based on network interface address and the current working directory).  However, an additional check is done when the cluster lease end time hasn't been reached, in which case the entry is skipped (assuming it belongs to a different instance), and the scan continues. When no other entry is found, a new one is created.  So why would we *ever* consider instances with matching instance information to be different? As far as I can tell the answer is: for unit testing.  But...  With the current assignment very weird things can happen, and I believe I see exactly this happening in a customer problem I'm investigating. The sequence is:  1) First system startup, cluster node id 1 is assigned  2) System crashes or was crashed  3) System restarts within the lease time (120s?), a new cluster node id is assigned  4) System shuts down, and gets restarted after a longer interval: cluster id 1 is used again, and system starts {{MissingLastRevRecovery}}, despite the previous shutdown having been clean  So what we see is that the system starts up with varying cluster node ids, and recovery processes may run with no correlation to what happened before.  Proposal:  a) Make {{ClusterNodeInfo.createInstance()}} much more verbose, so that the default system log contains sufficient information to understand why a certain cluster node id was picked.  b) Drop the logic that skips entries with non-expired leases, so that we get a one-to-one relation between instance ids and cluster node ids. For the unit tests that currently rely on this logic, switch to APIs where the test setup picks the cluster node id.$$patch1-oak-f4349a96_Developer_PatchNaturalnessYe$$added missing boolean flag to createInstance. added missing @ Before @ After @ Deprecated. "avoid "" forever "" loop". Fixed log in. $$1
oak-df59fb45$$Ordered index fails with old index content$$With the latest changes, the ordered index no longer works with old index data. When running the latest Oak 1.0.2 snapshot run against an Oak 1.0.0 repository with an existing ordered index, the index fails with the exception below.  As a workaround, the ordered index can be manually re-built. Either the index re-build needs to be automatic, or the ordered index needs to work with the old index content.  {noformat} java.lang.IndexOutOfBoundsException: index (3) must be less than size (1)     at com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:306)     at com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:285)     at org.apache.jackrabbit.oak.plugins.segment.SegmentPropertyState.getValue(SegmentPropertyState.java:157)     at org.apache.jackrabbit.oak.plugins.index.property.strategy.OrderedContentMirrorStoreStrategy.getPropertyNext(OrderedContentMirrorStoreStrategy.java:1024) {noformat}$$patch1-oak-df59fb45_Developer_PatchNaturalnessYe$$added missing import. added missing import. added log for removed properties. added logging for walkedLanes. added logging. $$1
oak-a1556c30$$[Ordered Index] Indexing on large content is slow$$Indexing large number of ordered properties is quite slow.  Explore ways of making it faster. The current skip list implementation uses 4 lanes with a probability of 10%. It should be made configurable and the defaults changed.$$patch1-oak-a1556c30_Developer_PatchNaturalnessYe$$added missing import. removed unused imports. Fix NPE in OrderedContentMirrorStoreStrategy. reset currentNode to null. added support for empty string properties. added missing closing Javadoc. removed unnecessary check for array length. Fixed default probability of the indexer , but it ' s still in the code , and it. $$1
oak-79467350$$XPath query failures for mvps$$Adding some cases related to mvps that are not currently covered by the existing (jackrabbit) tests.$$patch1-oak-79467350_Developer_PatchNaturalnessYe$$Fixed a bug in property value comparison. Fixed bug in PropertyValues . contains. Fix notMatch implementation. Fix bug in FilterImpl # setAlwaysFalse. CAMEL - 2216 Handle case with null values in IndexFilter. $$1
oak-84fb6b29$$MutableTree#isNew: replace implementation by NodeBuilder#isNew$$Similar to the issue described in OAK-1177 we may consider replacing the implementation of MutableTree#isNew by the corresponding call on the NodeBuilder.  See also OAK-947.$$patch1-oak-84fb6b29_Developer_PatchNaturalnessYe$$Improved javadoc in MemoryNodeBuilder. $$1
oak-24cb1908$$DocumentNodeStore revision GC may lead to NPE$$The DocumentNodeStore revision GC may cause a NPE in a reader thread when the GC deletes documents currently accessed by the reader. The {{docChildrenCache}} is invalidated in {{VersionGarbageCollector.collectDeletedDocuments()}} after documents are removed in the DocumentStore. The NPE may occur if removed documents are access in between.$$patch1-oak-24cb1908_Developer_PatchNaturalnessYe$$added missing import. added missing import. "remove "" import "" statement from opengl". added private modifier to readChildDocs. "Remove "" final "" modifier from StringValue ( path ) ;". "Revert "" update to 0 """. always invalidate children cache when we are done with the last document. $$1
oak-3270e761$$Adding a node to a node that doesn't accept children doesn't fail with ConstraintViolationException$$More node type fun!  I ran into this via the tck test {{org.apache.jackrabbit.test.api.query.SaveTest#testConstraintViolationException}}.  It seems adding a node to a node that doesn't accept children (like for example {{nt:query}}) fails with a {{RepositoryException}} that wraps a {{CommitFailedException}} with a message along the lines of: {{Cannot add node 'q2' at /q1}}, further wrapping a weird-looking {{RepositoryException: No matching node definition found for org.apache.jackrabbit.oak.plugins.nodetype.ValidatingNodeTypeManager@257f1b}}  While this seems ok enough, the tck test expects a {{ConstraintViolationException}}, so that's why I created this bug.   I'll attach a test case shortly.  Trace  {code} javax.jcr.RepositoryException 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39) 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27) 	at java.lang.reflect.Constructor.newInstance(Constructor.java:513) 	at org.apache.jackrabbit.oak.api.CommitFailedException.throwRepositoryException(CommitFailedException.java:57) 	at org.apache.jackrabbit.oak.jcr.SessionDelegate.save(SessionDelegate.java:244) 	at org.apache.jackrabbit.oak.jcr.SessionImpl.save(SessionImpl.java:283) 	at org.apache.jackrabbit.oak.jcr.nodetype.NodeTypeTest.illegalAddNode(NodeTypeTest.java:39) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) 	at java.lang.reflect.Method.invoke(Method.java:597) 	at org.junit.runners.model.FrameworkMethod 1.runReflectiveCall(FrameworkMethod.java:45) 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42) 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20) 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28) 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30) 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263) 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68) 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47) 	at org.junit.runners.ParentRunner 3.run(ParentRunner.java:231) 	at org.junit.runners.ParentRunner 1.schedule(ParentRunner.java:60) 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229) 	at org.junit.runners.ParentRunner.access 000(ParentRunner.java:50) 	at org.junit.runners.ParentRunner 2.evaluate(ParentRunner.java:222) 	at org.junit.runners.ParentRunner.run(ParentRunner.java:300) 	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:50) 	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197) Caused by: org.apache.jackrabbit.oak.api.CommitFailedException: Cannot add node 'q2' at /q1 	at org.apache.jackrabbit.oak.plugins.nodetype.TypeValidator.childNodeAdded(TypeValidator.java:134) 	at org.apache.jackrabbit.oak.spi.commit.CompositeValidator.childNodeAdded(CompositeValidator.java:68) 	at org.apache.jackrabbit.oak.spi.commit.ValidatingHook ValidatorDiff.childNodeAdded(ValidatingHook.java:155) 	at org.apache.jackrabbit.oak.spi.state.AbstractNodeState.compareAgainstBaseState(AbstractNodeState.java:157) 	at org.apache.jackrabbit.oak.kernel.KernelNodeState.compareAgainstBaseState(KernelNodeState.java:243) 	at org.apache.jackrabbit.oak.spi.commit.ValidatingHook ValidatorDiff.validate(ValidatingHook.java:110) 	at org.apache.jackrabbit.oak.spi.commit.ValidatingHook ValidatorDiff.validate(ValidatingHook.java:101) 	at org.apache.jackrabbit.oak.spi.commit.ValidatingHook ValidatorDiff.childNodeAdded(ValidatingHook.java:157) 	at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState 3.childNodeAdded(ModifiedNodeState.java:292) 	at org.apache.jackrabbit.oak.spi.state.AbstractNodeState.compareAgainstBaseState(AbstractNodeState.java:157) 	at org.apache.jackrabbit.oak.kernel.KernelNodeState.compareAgainstBaseState(KernelNodeState.java:243) 	at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:269) 	at org.apache.jackrabbit.oak.spi.commit.ValidatingHook ValidatorDiff.validate(ValidatingHook.java:110) 	at org.apache.jackrabbit.oak.spi.commit.ValidatingHook ValidatorDiff.validate(ValidatingHook.java:101) 	at org.apache.jackrabbit.oak.spi.commit.ValidatingHook.processCommit(ValidatingHook.java:73) 	at org.apache.jackrabbit.oak.spi.commit.CompositeHook.processCommit(CompositeHook.java:59) 	at org.apache.jackrabbit.oak.kernel.KernelNodeStoreBranch.merge(KernelNodeStoreBranch.java:127) 	at org.apache.jackrabbit.oak.core.RootImpl 2.run(RootImpl.java:239) 	at org.apache.jackrabbit.oak.core.RootImpl 2.run(RootImpl.java:1) 	at java.security.AccessController.doPrivileged(Native Method) 	at javax.security.auth.Subject.doAs(Subject.java:337) 	at org.apache.jackrabbit.oak.core.RootImpl.commit(RootImpl.java:234) 	at org.apache.jackrabbit.oak.jcr.SessionDelegate.save(SessionDelegate.java:241) 	... 27 more Caused by: javax.jcr.RepositoryException: No matching node definition found for org.apache.jackrabbit.oak.plugins.nodetype.ValidatingNodeTypeManager@257f1b 	at org.apache.jackrabbit.oak.plugins.nodetype.ReadOnlyNodeTypeManager.getDefinition(ReadOnlyNodeTypeManager.java:406) 	at org.apache.jackrabbit.oak.plugins.nodetype.TypeValidator EffectiveNodeType.getDefinition(TypeValidator.java:302) 	at org.apache.jackrabbit.oak.plugins.nodetype.TypeValidator EffectiveNodeType.checkAddChildNode(TypeValidator.java:249) 	at org.apache.jackrabbit.oak.plugins.nodetype.TypeValidator.childNodeAdded(TypeValidator.java:127) 	... 49 more {code}$$patch1-oak-3270e761_Developer_PatchNaturalnessYe$$added catch block for checkProtected. $$1
oak-9a109aa3$$Suggestion dictionary don't update after suggestUpdateFrequencyMinutes unless something else causes index update$$Currently, suggestions building is tied at the end of indexing cycle. Along with that we check if diff between currTime and lastSugguestionBuildTime is more than {{suggestUpdateFrequencyMinutes}} before deciding to build suggestions or not.  This allows for suggestions not getting updated if: * At T1 suggestions are built * At T2 an index update takes place but suggestions aren't rebuilt because not enough time has passed since T1 * Now at T3 (after sufficient time), changes at T2 won't show up for suggestions until some other index change happens.  We should probably see track about last changes in index (at T2) and use that too while running indexing cycle at T3.$$patch1-oak-9a109aa3_Developer_PatchNaturalnessYe$$log debug statement. always update suggestions if updateSuggestions is enabled. updated ISO8601 value for status property. Fixed small issue in code snippet. $$1
oak-b3071839$$read is inefficient when there are many split documents$$As reported in OAK-2358 there is a potential problem with revisionGC not cleaning up split documents properly (in 1.0.8.r1644758 at least).   As a side-effect, having many garbage-revisions renders the diffImpl algorithm to become very slow - normally it would take only a few millis, but with nodes that have many split-documents I can see diffImpl take hundres of millis, sometimes up to a few seconds. Which causes the observation dequeuing to be slower than the rate in which observation events are enqueued, which results in observation queue never being cleaned up and event handling being delayed more and more.  Adding some logging showed that diffImpl would often read many split-documents, which supports the assumption that the revisionGC not cleaning up revisions has the diffImpl-slowness as a side-effect. Having said that - diffImpl should probably still be able to run fast, since all the revisions it should look at should be in the main document, not in split documents.  I dont have a test case handy for this at the moment unfortunately - if more is coming up, I'll add more details here.$$patch1-oak-b3071839_Developer_PatchNaturalnessYe$$Fix broken backwards compatibility issue. Fix broken delete / update relationship. "avoid the confusion of "" invalid revision "" assertion ( # 1435 )". @ Nonnull @ method return the value of the last modified revision. Fixed possible NPE in NodeDocument. added value map presence code. $$1
oak-c05cec12$$Invalid JCR paths not caught$${{NamePathMapper.getOakPath}} should return {{null}} when called with an invalid JCR path like {{foo:bar]baz}}, but it doesn't.$$patch1-oak-c05cec12_Developer_PatchNaturalnessYe$$Improved javadoc comment. $$1
oak-86edbffb$$Oak Analyzer can't tokenize chinese phrases$$It looks like the _WhitespaceTokenizer_ cannot properly split Chinese phrases, for example ''. I could not find a reference to this issue other than LUCENE-5096.  The fix is to switch to the _ClassicTokenizer_ which seems better equipped for this kind of task.$$patch1-oak-86edbffb_Developer_PatchNaturalnessYe$$Bump LuceneIndexConstants version to match main pom. added missing import. Removed use of deprecated API. $$1
oak-77d2d3b0$$Failure in one of the batch in VersionGC might lead to orphaned nodes$$VersionGC logic currently performs deletion of nodes in batches. For GC to work properly NodeDocument should always be removed in bottom-up mode i.e. parent node should be removed *after* child has been removed  Currently the GC logic deletes the NodeDocument in undefined order. In such mode if one of the batch fails then its possible that parent might have got deleted but the child was not deleted.   Now in next run the child node would not be recognized as a deleted node because the commit root would not be found.$$patch1-oak-77d2d3b0_Developer_PatchNaturalnessYe$$removed unused import. added sort to docIdsToDelete. $$1
oak-a2950285$$Parent of unseen children must not be removable$$With OAK-2673, it's now possible to have hidden intermediate nodes created concurrently. So, a scenario like: {noformat} start -> /:hidden N1 creates /:hiddent/parent/node1 N2 creates /:hidden/parent/node2 {noformat} is allowed.  But, if N2's creation of {{parent}} got persisted later than that on N1, then N2 is currently able to delete {{parent}} even though there's {{node1}}.$$patch1-oak-a2950285_Developer_PatchNaturalnessYe$$"remove unused import "" import "" statement". added missing import. Build number 12. $$1
oak-b8fe2ded$$NodeType index doesn't respect the declaringNodeTypes setting$$Following the OAK-1150 discussion, I've noticed that the node type index doesn't respect the declaringNodeTypes setting. Setting a restriction on the node type index definition breaks the index - there are 0 query hits.$$patch1-oak-b8fe2ded_Developer_PatchNaturalnessYe$$added filter to isIndexed ( ) method. Fix nodetype indexing. added cost function to NodeTypeIndex , that now works with nodetype filters. added 58. $$1
oak-6dde8e9d$$XPath: Query with mixed full-text, "and", "or" conditions fails$$When performing a query like   {noformat}         //element(*, test:Asset)[             (                 jcr:contains(., 'summer')                 or                 jcr:content/metadata/@tags = 'namespace:season/summer'             ) and                 jcr:contains(jcr:content/metadata/@format, 'image')             ]  {noformat}  The Lucene/Aggregate returns as well nodes that does not match all the criterias.$$patch1-oak-6dde8e9d_Developer_PatchNaturalnessYe$$Fixed bug 439. Fix fullText constraint. "Revert "" update to new selectorName in FullTextSearchImpl implements """. added relativePath to propertyName for fulltext search. fixed union list format. $$1
oak-f63d745a$$Multivalued properties with array size 0 forget their type$$thought i remember that i have seen a related TODO or issue before, i couldn't find it any more... sorry for that.  while cleaning up the node type code i found that one FIXME in the  ReadOnlyNodeTypeManager related to definition generation was only needed because the TypeValidator failed upon validation of an empty jcr:supertypes definition. not storing the super types if none has be declared solved the problem for the time being.  however, it seems to me that the underlying problem is in a completely different area: namely that mv properties with an empty value array forget their type.  this can be verified with the following test: {code}     @Test     public void addEmptyMultiValueName() throws RepositoryException {         Node parentNode = getNode(TEST_PATH);         Value[] values = new Value[0];          parentNode.setProperty("multi name", values);         parentNode.getSession().save();          Session session2 = createAnonymousSession();         try {             Property property = session2.getProperty(TEST_PATH + "/multi name");             assertTrue(property.isMultiple());             assertEquals(PropertyType.NAME, property.getType());             Value[] values2 = property.getValues();             assertEquals(values.length, values2.length);             assertEquals(values[0], values2[0]);             assertEquals(values[1], values2[1]);         } finally {             session2.logout();         }     } {code}$$patch1-oak-f63d745a_Developer_PatchNaturalnessYe$$Fix undefined value property association warnings. $$1
oak-b4a93c81$$Always create new UUID on ImportBehavior.IMPORT_UUID_CREATE_NEW$$The implementation should create a new UUID for each referenceable node even if there is no existing node with that UUID. This spec says:  bq.  Incoming nodes are assigned newly created identifiers upon addition to the workspace. As a result, identifier collisions never occur.  This will break backward compatibility, but is IMO the correct behavior and the only way to guarantee import of referenceable nodes does not fail in a concurrent import scenario. See OAK-1186 for more details.$$patch1-oak-b4a93c81_Developer_PatchNaturalnessYe$$added missing author note. $$1
oak-978c77ff$$Inconsistent read on DocumentNodeStore startup$$This is a regression introduced with OAK-2929. On DocumentNodeStore startup the RevisionComparator of the local instance is initialized with the current _lastRev entries from the other cluster nodes. The external _lastRev entries are 'seenAt' the same revision, which means for those revisions the RevisionComparator will use the clusterId to compare them. This is also described in OAK-3388.  OAK-2929 changed the sequence of revisions to check for conflicts from StableRevisionComparator to RevisionComparator. This makes the conflict check susceptible to the RevisionComparison behaviour described in OAK-3388. Commits may be rejected with a conflict, when there isn't really a conflict.$$patch1-oak-978c77ff_Developer_PatchNaturalnessYe$$added missing import. Remove unused import. Improved javadoc comment .. $$1
oak-6fc5ea9d$$TreeImpl#*Location: unable retrieve child location if access to parent is denied$$as a consequence of OAK-709 we now have an issue with the way SessionDelegate and Root#getLocation access a node in the hierarchy which has an ancestor which is not accessible.  specifically RootImpl#getLocation will be served a NullLocation for the first ancestor which is not accessible and consequently any accessible child node cannot be accessed.  in order to reproduce the issue you may:  - change AccessControlConfigurationImpl to use PermissionProviderImpl instead   of the tmp solution - and run o.a.j.oak.jcr.security.authorization.ReadTest#testReadDenied$$patch1-oak-6fc5ea9d_Developer_PatchNaturalnessYe$$Fix bug 45195 .. getChildNode ( ) is not a child of the tree. Improved method return type for TreeImpl # getFirstChild ( ). Fixed a small bug. "Revert "" return null """. $$1
oak-bc7b7e8c$$ACE merging not behaving correctly if not using managed principals$${{org.apache.jackrabbit.api.security.JackrabbitAccessControlList#addEntry()}} does not work correctly, if the given principal is not retrieved from the PrincipalManager.  Exception: {noformat} Caused by: org.apache.jackrabbit.oak.api.CommitFailedException: OakAccessControl0013: Duplicate ACE found in policy 	at org.apache.jackrabbit.oak.security.authorization.accesscontrol.AccessControlValidator.accessViolation(AccessControlValidator.java:278) 	at org.apache.jackrabbit.oak.security.authorization.accesscontrol.AccessControlValidator.checkValidPolicy(AccessControlValidator.java:188) {noformat}  this used to work in jackrabbit 2.x.  the problem is probably in {{org.apache.jackrabbit.oak.security.authorization.accesscontrol.ACL#internalAddEntry}} where the principals are "equalled" instead of comparing their names.  note, that adding an ACE with such a principal works, just the merging/overwriting detection doesn't.  test: {code}   Principal p1 = new Principal() { getName(){return "foo"}};   Principal p2 = new Principal() { getName(){return "foo"}};   acl.addEntry(p1, privileges, true);   acl.addEntry(p2, privileges, false);   ...   save(); // throws {code}$$patch1-oak-bc7b7e8c_Developer_PatchNaturalnessYe$$Fixed filterByPrincipalName. $$1
oak-093b9128$$Default sync handler property mapping does not allow constant properties$$it would be useful, if the default sync handler user (and group) mapping could also handle constant properties and use given primary type and mixin type information. eg:  {noformat} profile/nt:primaryType="sling:Folder" profile/sling:resourceType="sling/security/profile" {noformat}$$patch1-oak-093b9128_Developer_PatchNaturalnessYe$$Fixed whitespace in SyncHandler. $$1
oak-7a84b3a8$$NPE trying to add a node to an nt:folder node$$The following code throws a NPE:  {code} Session s = getAdminSession(); s.getRootNode().addNode("a", "nt:folder").addNode("b"); s.save();         {code}  Stack trace: {code} java.lang.NullPointerException at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:191) at org.apache.jackrabbit.oak.namepath.LocalNameMapper.getOakNameOrNull(LocalNameMapper.java:82) at org.apache.jackrabbit.oak.namepath.GlobalNameMapper.getOakName(GlobalNameMapper.java:64) at org.apache.jackrabbit.oak.namepath.NamePathMapperImpl.getOakName(NamePathMapperImpl.java:62) at org.apache.jackrabbit.oak.plugins.nodetype.ReadOnlyNodeTypeManager.getOakName(ReadOnlyNodeTypeManager.java:92) at org.apache.jackrabbit.oak.plugins.nodetype.ReadOnlyNodeTypeManager.getNodeType(ReadOnlyNodeTypeManager.java:186) at org.apache.jackrabbit.oak.jcr.NodeImpl 5.perform(NodeImpl.java:265) at org.apache.jackrabbit.oak.jcr.NodeImpl 5.perform(NodeImpl.java:1) at org.apache.jackrabbit.oak.jcr.SessionDelegate.perform(SessionDelegate.java:136) at org.apache.jackrabbit.oak.jcr.NodeImpl.addNode(NodeImpl.java:219) at org.apache.jackrabbit.oak.jcr.NodeImpl.addNode(NodeImpl.java:210) at org.apache.jackrabbit.oak.jcr.CRUDTest.nodeType(CRUDTest.java:122) {code}$$patch1-oak-7a84b3a8_Developer_PatchNaturalnessYe$$better handle case. $$1
oak-08ba79d4$$Upgraded version history has UUIDs as jcr:frozenUuid of non-referenceable nodes$$In Jackrabbit Classic each node, even non-referenceable ones, has a UUID as its identifier, and thus the {{jcr:frozenUuid}} properties of frozen nodes are always UUIDs. In contrast Oak uses path identifiers for non-referenceable frozen nodes (see OAK-1009), which presents a problem when dealing with version histories migrated from Jackrabbit Classic.  To avoid this mismatch, the upgrade code should check each frozen node for referenceability and replace the frozen UUID with a path identifier if needed.$$patch1-oak-08ba79d4_Developer_PatchNaturalnessYe$$added missing imports. addedFrozenNode to JackrabbitNodeState. added isFrozenNode in JackrabbitNodeState contructor. added frozen node predicate. Make method private. convert frozenUuid to path identifier. $$1
oak-d645112f$$RepositorySidegrade: oak-segment to oak-segment-tar should migrate checkpoint info$$The sidegrade from {{oak-segment}} to {{oak-segment-tar}} should also take care of moving the checkpoint data and meta. This will save a very expensive full-reindex.$$patch1-oak-d645112f_Developer_PatchNaturalnessYe$$added missing import. added missing imports. Have the SegmentTarFactory return a better implementation. added missing import. added missing imports. do not remove checkpoint references in upgrade workflow. added get checkpoint paths method. added missing import. Have the SegmentFactory return a better implementation. Add back missing constructor .. $$1
oak-2a02a138$$Hidden properties (one prefixed with ':') in lucene's analyzer configuration fail to construct analyzers$$This is similar to OAK-2524 in the sense that lucene doesn't like extra arguments sent its way while constructing analyzers. In some cases (like node move adds {{:source-path}}) we have hidden properties added to index definition nodes and they get passed along to lucene analyzer factories which complaint and fail.$$patch1-oak-2a02a138_Developer_PatchNaturalnessYe$$added missing import. added ISO - 8859 - 2 workflow improvements. $$1
oak-5c4589bd$$Range queries and relative properties resultset should be consistent with JR2$$When running a range query like {{/jcr:root/content/nodes//*[(*/*/*/@prop >= 9)]}} the resultset is not consistent for the same use-case when running in jacrabbit 2.$$patch1-oak-5c4589bd_Developer_PatchNaturalnessYe$$added missing import. added missing log. Add logging for currentOakProperty ( ). Add logging for readOakProperties ( ). added log trace for added property. $$1
oak-e33516d5$$DefaultSyncContext.syncMembership may sync group of a foreign IDP$$With the following scenario the {{DefaultSyncContext.syncMembership}} may end up synchronizing (i.e. updating) a group defined by an foreign IDP and even add the user to be synchronized as a new member:  - configuration with different IDPs - foreign IDP synchronizes a given external group 'groupA' => rep:externalID points to foreign-IDP (e.g. rep:externalId = 'groupA;foreignIDP') - my-IDP contains a group with the same ID (but obviously with a different rep:externalID) and user that has declared group membership pointing to 'groupA' from my IDP  if synchronizing my user first the groupA will be created with a rep:externalId = 'groupA;myIDP'. however, if the group has been synced before by the foreignIDP the code fails to verify that an existing group 'groupA' really belongs to the same IDP and thus may end up synchronizing the group and updating it's members.  IMHO that's a critical issue as it violates the IDP boundaries. the fix is pretty trivial as it only requires testing for the IDP of the existing group as we do it in other places (even in the same method).$$patch1-oak-e33516d5_Developer_PatchNaturalnessYe$$Fixed IDP name in SyncContext .. add missing "" remove from "" loop". $$1
oak-f51ea2a2$$XPath backwards compatibility issue with false() and true()$$In JR2 (actually CRX 2) both of the following queries for nodes with a boolean property can be parsed, however only query (a) returns search results. {noformat}     (a) /jcr:root/test//*[@foo = true()]     (b) /jcr:root/test//*[@foo = true] {noformat}  On Oak 1.2, query (a) results in an exception\[0\] and query (b) returns search results.  See discussion at http://markmail.org/thread/kpews55jpdwm62ds$$patch1-oak-f51ea2a2_Developer_PatchNaturalnessYe$$Fixed read ( ). $$1
oak-487de751$$Possibility of overflow in file length calculation$$In OakDirectory the length of a file is calculated in following way  {code:title=OakDirectory|linenumbers=true}         public OakIndexFile(String name, NodeBuilder file) {             ...             this.blobSize = determineBlobSize(file);             this.blob = new byte[blobSize];              PropertyState property = file.getProperty(JCR_DATA);             if (property != null && property.getType() == BINARIES) {                 this.data = newArrayList(property.getValue(BINARIES));             } else {                 this.data = newArrayList();             }              this.length = data.size() * blobSize;             if (!data.isEmpty()) {                 Blob last = data.get(data.size() - 1);                 this.length -= blobSize - last.length();             } {code}  In above calculation its possible to have an overflow in  bq. this.length = data.size() * blobSize;  As multiplication of two integers result in an integer [1]  [1] http://stackoverflow.com/questions/12861893/casting-result-of-multiplication-two-positive-integers-to-long-is-negative-value$$patch1-oak-487de751_Developer_PatchNaturalnessYe$$data to long. Add missing message. $$1
oak-a28098fd$$Session.getItem violates JCR Spec$$Session.getItem(path) is supposed to first return a node for the given path, and if no node is found return a property.  The oak implementation returns this in the opposite order.  see attached patch for a possible fix.$$patch1-oak-a28098fd_Developer_PatchNaturalnessYe$$added NodeDelegate for ( @@ ). $$1
oak-70564c7c$$Revisit full-text queries in case of multiple tokens$$There's still an issue with tokenizing the search terms when trying for example to search for a fulltext term that will split into 2 actual terms because of the analyzer.  Taking 'hello-world*' this will break into 2 tokens 'hello' and 'world*' which when treated as a PhraseQuery will not work, so I want to change this into a MutiPhraseQuery based on the simple tokens provided and all the existing tokens that match the wildchar character.$$patch1-oak-70564c7c_Developer_PatchNaturalnessYe$$added missing import. added missing imports. Fixed a small issue with missing reader option. added LuceneIndex . getFullTextQuery ( ) that uses the index reader for the. Fixed a small bug in Indexing. Fixed a small bug in Indexing. added missing reader dependency. added missing IndexReader dependency. Fixed a bug in the Lucene index that was failing too often. Fixed bug in lucene index. $$1
oak-45b110e1$$MemoryNodeBuilder.setNode() loses property values$${code} builder.setNode("a", nodeA); builder.child("a").setProperty(...); {code}  After the 2nd line executed, properties initially present on {{nodeA}} are gone on {{builder.getNodeState()}}.$$patch1-oak-45b110e1_Developer_PatchNaturalnessYe$$added missing copy of base state from getWriteState ( ). $$1
oak-c9765c21$$Ignore files in the root directory of the FileDataStore in #getAllIdentifiers$$The call to OakFileDataStore#getAllIdentifiers should ignore the the files directly at the root of the DataStore (These files are used for SharedDataStore etc). This does not cause any functional problems but leads to logging warning in the logs.  There is already a check but it fails when the data store root is specified as a relative path.$$patch1-oak-c9765c21_Developer_PatchNaturalnessYe$$remove unused imports. added normalizeNoEndSeparator. don ' t normalize path separators ; fix. $$1
oak-c02ecef8$$MemoryPropertyBuilder.assignFrom leads to ClassCastException on getPropertyState with date properties$$None$$patch1-oak-c02ecef8_Developer_PatchNaturalnessYe$$improve memory property builder. $$1
oak-24ce6788$$ValueFactory: Missing identifier validation when creating (weak)reference value from String$$the JCR specification mandates validation of the identifier during value conversion from STRING to REFERENCE or WEAK_REFERENCE:  <quote from 3.6.4.1 From STRING To)> REFERENCE or WEAKREFERENCE: If the string is a syntactically valid  identifier, according to the implementation, it is converted directly, otherwise a  ValueFormatException is thrown. The identifier is not required to be that of an  existing node in the current workspace.  <end_quote>  the current ValueFactory implementation in oak-jcr lacks that validation: creating a REFERENCE or WEAKREFERENCE value using ValueFactory#createValue(String, int) succeeds even if the specified string isn't a valid referenceable node identifier.$$patch1-oak-24ce6788_Developer_PatchNaturalnessYe$$added missing import. allow value to be string ( or null ). $$1
oak-a0a495f0$$Missing privileges after repository upgrade$$After upgrading from Jackrabbit classic all Oak specific privileges are missing (rep:userManagement, rep:readNodes, rep:readProperties, rep:addProperties, rep:alterProperties, rep:removeProperties, rep:indexDefinitionManagement).  The reason seems to be that the {{PrivilegeInitializer}} is not run during upgrade.$$patch1-oak-a0a495f0_Developer_PatchNaturalnessYe$$added missing import. Fixing the build .. Fixing the build .. ignore built in privileges. static resolvePrivilegeBits ( ). $$1
oak-16225d51$$MongoMK GC removes documents with data still in use$$The version garbage collector may delete previous documents that contain commit root information still in use by the main document.$$patch1-oak-16225d51_Developer_PatchNaturalnessYe$$removed unused imports. remove OAK - 1794 from GC_TYPES. $$1
oak-ca63fdf3$$Missing privileges after repository upgrade$$After upgrading from Jackrabbit classic all Oak specific privileges are missing (rep:userManagement, rep:readNodes, rep:readProperties, rep:addProperties, rep:alterProperties, rep:removeProperties, rep:indexDefinitionManagement).  The reason seems to be that the {{PrivilegeInitializer}} is not run during upgrade.$$patch1-oak-ca63fdf3_Developer_PatchNaturalnessYe$$added workspace initializer to each security configuration. $$1
oak-6d82cb64$$PathUtils#getDepth returns 1 for empty path$$PathUtils#getDepths that the root path / has depth 0. however, passing in a empty string is accepted and returns 1.  according to the API contract getDepth is counting the number of elements in the path which for "" should IMO be zero.$$patch1-oak-6d82cb64_Developer_PatchNaturalnessYe$$avoid NPE on initial check for empty path Strings. $$1
oak-117b0a3d$$Node.hasNode("foo[2]") must not throw PathNotFoundException$$similar to OAK-1225, Node.hasNode("foo[2]") should return false$$patch1-oak-117b0a3d_Developer_PatchNaturalnessYe$$added better handling for hasNode ( ). $$1
oak-279bb3ce$$Slow event listeners do not scale as expected$${{org.apache.jackrabbit.oak.jcr.LargeOperationIT#slowListener}} does not scale to {{O n log n}} on the document node store.$$patch1-oak-279bb3ce_Developer_PatchNaturalnessYe$$getCommitRootDepth from local to getCommitRootDepth from repository. Remove a for - loop that is not needed .. getCommitRootDepth ( ) is used by the repository only for the current commit root. $$1
oak-74cbba24$$Stale cluster ids can potentially lead to lots of previous docs traversal in NodeDocument.getNewestRevision$$When some (actual test case and conditions still being investigated) of the following conditions are met: * There are property value changes from different cluster id * There are very old and stale cluster id (probably older incarnations of current node itself) * A parallel background split removes all _commitRoot, _revision entries such that the latest one (which is less that baseRev) is very old  , finding newest revision traverses a lot of previous docs. Since root document gets split a lot and is a very common commitRoot (thus participating during checkConflicts in lot of commits), the issue can slow down commits by a lot$$patch1-oak-74cbba24_Developer_PatchNaturalnessYe$$add a static helper method for aborting iterables. added missing import. added missing import. Remove unused imports. removed abortingIterable from interface method. Fix merge bug. Fixed an issue with workflow workflow workflow workflow. Allow revision predicate to be applied to previous revisions. $$1
oak-705ce1d1$$NodeStoreKernel doesn't handle array properties correctly$${{NodeStoreKernel}} currently only supports array properties of type long. For other types it will fail with an {{IllegalStateException}}. See also the FIXME in the code.$$patch1-oak-705ce1d1_Developer_PatchNaturalnessYe$$Fixing possible NPE in SegmentNodeState . getProperty ( ). $$1
oak-7e250001$$UpdateOp.Key.equals() incorrect$$As reported on the dev list [0], the equals implementation of UpdateOp.Key is incorrect.  [0] http://markmail.org/message/acpg2mhbxjn4lglu$$patch1-oak-7e250001_Developer_PatchNaturalnessYe$$Fixed updateOp. $$1
oak-ca36450e$$IllegalStateException when using "lowerCase"/"lower" on a array property$$if query contain lowerCase on array property then QueryResult.getRows() throwing  IllegalStateException.  Query which causing issue   select [selector_1].* from [nt:unstructured] AS [selector_1] where (([selector_1].[lcc:className] = 'com.adobe.icc.dbforms.obj.ConditionalDataModule')) AND (LOWER([selector_1].[dataDictionaryRefs]) = 'employeedd')  If we remove LOWER function then it is working    select [selector_1].* from [nt:unstructured] AS [selector_1] where (([selector_1].[lcc:className] = 'com.adobe.icc.dbforms.obj.ConditionalDataModule')) AND ([selector_1].[dataDictionaryRefs] = 'EmployeeDD')$$patch1-oak-ca36450e_Developer_PatchNaturalnessYe$$"add missing import of "" lower case "" to query parser". fixed failing test. $$1
oak-3bf07779$$Index updation fails on updating multivalued property$$On emptying a multivalued property, fulltext index updation fails and one can search on old values. Following test demonstrates the issue. Added below test in [LuceneIndexQueryTest.java|https://github.com/apache/jackrabbit-oak/blob/trunk/oak-lucene/src/test/java/org/apache/jackrabbit/oak/plugins/index/lucene/LuceneIndexQueryTest.java] which should pass -  {code}     @Test     public void testMultiValuedPropUpdate() throws Exception {         Tree test = root.getTree("/").addChild("test");         String child = "child";         String mulValuedProp = "prop";         test.addChild(child).setProperty(mulValuedProp, of("foo","bar"), Type.STRINGS);         root.commit();         assertQuery(                 "/jcr:root//*[jcr:contains(@" + mulValuedProp + ", 'foo')]",                 "xpath", ImmutableList.of("/test/" + child));         test.getChild(child).setProperty(mulValuedProp, new ArrayList<String>(), Type.STRINGS);         root.commit();         assertQuery(                 "/jcr:root//*[jcr:contains(@" + mulValuedProp + ", 'foo')]",                 "xpath", new ArrayList<String>());          test.getChild(child).setProperty(mulValuedProp, of("bar"), Type.STRINGS);         root.commit();         assertQuery(                 "/jcr:root//*[jcr:contains(@" + mulValuedProp + ", 'foo')]",                 "xpath", new ArrayList<String>());      } {code}$$patch1-oak-3bf07779_Developer_PatchNaturalnessYe$$added missing field. added propertyChanged ( ) , checkAggregates ( ). index if a node having a single property was removed. Fixed javadoc error. $$1
oak-64045631$$Inconsistent state in Mongo/KernelRootBuilder$$The state of Kernel- and MongoRootBuilder may turn inconsistent when a NodeStoreBranch.merge() performs a rebase followed by a failed merge on the underlying storage. The head and base are not properly updated to reflect the successful rebase.$$patch1-oak-64045631_Developer_PatchNaturalnessYe$$Improve merge method. fix merge call. $$1
oak-8ed779dc$$Assertion error when adding node with expanded name$${code} node.addNode("{http://foo}new"); {code}  results in an assertion error$$patch1-oak-8ed779dc_Developer_PatchNaturalnessYe$$Fix NodeImpl ' s @@. $$1
oak-26041fe7$$Cross foreign cluster revision comparison may be wrong$$Running one of the access control related benchmarks concurrently on a MongoDB may result in strange conflicts even when DocumentNodeStore retries the commit. The root cause may be a wrong revision comparison when both revision to compare are from a foreign cluster node and one of them is not withing the known seen-at revision ranges.$$patch1-oak-26041fe7_Developer_PatchNaturalnessYe$$Fix null pointer check in revision range. $$1
oak-ff81ef72$$NodeDocument.getNodeAtRevision can go into property history traversal when latest rev on current doc isn't committed$${{NodeDocument.getNodeAtRevision}} tried to look at latest revisions entries for each property in current document. But it just looks at the *last* entry for a given property. In case this last entry isn't committed, the code would go into previous documents to look for a committed value.  (cc [~mreutegg])$$patch1-oak-ff81ef72_Developer_PatchNaturalnessYe$$workflow improvements - remove check for isRevisionNewer ( ) and isNotMostRecentCommitted (. Improved method to compare revisions with valueMap. $$1
oak-4e245a76$$missing support for relative path consisting of parent-element$$could not reopen OAK-95 -> cloning. during testing of user-mgt api found that relpath consisting of a single parent element doesn't work (but used to):  {code} @Test     public void getNode3() throws RepositoryException {         Node node = getNode("/foo");         Node root = node.getNode("..");         assertNotNull(root);         assertEquals("", root.getName());         assertTrue("/".equals(root.getPath()));     } :  {code}$$patch1-oak-4e245a76_Developer_PatchNaturalnessYe$$added missing import. Fix error in NodeDelegate . getProperty ( ). Fix error in NodeDelegate # getChild. Fix error in NodeDelegate # getChildLocation. $$1
oak-35a7f014$$Malformed solr delete query$$Following OAK-734 the solr query tests are failing because of a parsing error on the wildcard delete query.  The exact query is 'path_exact:/test*', which apparently upsets the lucene parser somehow.  Full trace:  {code} SEVERE: org.apache.solr.common.SolrException: org.apache.lucene.queryparser.classic.ParseException: Cannot parse 'path_exact:/test*': Lexical error at line 1, column 18.  Encountered: <EOF> after : "/test*" 	at org.apache.solr.update.DirectUpdateHandler2.getQuery(DirectUpdateHandler2.java:328) 	at org.apache.solr.update.DirectUpdateHandler2.deleteByQuery(DirectUpdateHandler2.java:340) 	at org.apache.solr.update.processor.RunUpdateProcessor.processDelete(RunUpdateProcessorFactory.java:72) 	at org.apache.solr.update.processor.UpdateRequestProcessor.processDelete(UpdateRequestProcessor.java:55) 	at org.apache.solr.update.processor.DistributedUpdateProcessor.doLocalDelete(DistributedUpdateProcessor.java:437) 	at org.apache.solr.update.processor.DistributedUpdateProcessor.doDeleteByQuery(DistributedUpdateProcessor.java:835) 	at org.apache.solr.update.processor.DistributedUpdateProcessor.processDelete(DistributedUpdateProcessor.java:657) 	at org.apache.solr.update.processor.LogUpdateProcessor.processDelete(LogUpdateProcessorFactory.java:121) 	at org.apache.solr.handler.loader.XMLLoader.processDelete(XMLLoader.java:330) 	at org.apache.solr.handler.loader.XMLLoader.processUpdate(XMLLoader.java:261) 	at org.apache.solr.handler.loader.XMLLoader.load(XMLLoader.java:157) 	at org.apache.solr.handler.UpdateRequestHandler 1.load(UpdateRequestHandler.java:92) 	at org.apache.solr.handler.ContentStreamHandlerBase.handleRequestBody(ContentStreamHandlerBase.java:74) 	at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:129) 	at org.apache.solr.core.SolrCore.execute(SolrCore.java:1699) 	at org.apache.solr.client.solrj.embedded.EmbeddedSolrServer.request(EmbeddedSolrServer.java:150) 	at org.apache.solr.client.solrj.request.AbstractUpdateRequest.process(AbstractUpdateRequest.java:117) 	at org.apache.solr.client.solrj.SolrServer.deleteByQuery(SolrServer.java:285) 	at org.apache.solr.client.solrj.SolrServer.deleteByQuery(SolrServer.java:271) 	at org.apache.jackrabbit.oak.plugins.index.solr.index.SolrIndexUpdate.deleteSubtreeWriter(SolrIndexUpdate.java:161) 	at org.apache.jackrabbit.oak.plugins.index.solr.index.SolrIndexUpdate.apply(SolrIndexUpdate.java:98) 	at org.apache.jackrabbit.oak.plugins.index.solr.index.SolrIndexDiff.leave(SolrIndexDiff.java:202) 	at org.apache.jackrabbit.oak.spi.commit.CompositeEditor.leave(CompositeEditor.java:74) 	at org.apache.jackrabbit.oak.plugins.index.IndexHookManagerDiff.leave(IndexHookManagerDiff.java:117) 	at org.apache.jackrabbit.oak.spi.commit.EditorHook EditorDiff.process(EditorHook.java:115) 	at org.apache.jackrabbit.oak.spi.commit.EditorHook.process(EditorHook.java:80) 	at org.apache.jackrabbit.oak.spi.commit.EditorHook.processCommit(EditorHook.java:54) 	at org.apache.jackrabbit.oak.kernel.KernelNodeStoreBranch.merge(KernelNodeStoreBranch.java:144) 	at org.apache.jackrabbit.oak.core.RootImpl 2.run(RootImpl.java:266) 	at org.apache.jackrabbit.oak.core.RootImpl 2.run(RootImpl.java:1) 	at java.security.AccessController.doPrivileged(Native Method) 	at javax.security.auth.Subject.doAs(Subject.java:337) 	at org.apache.jackrabbit.oak.core.RootImpl.commit(RootImpl.java:261) 	at org.apache.jackrabbit.oak.query.AbstractQueryTest.test(AbstractQueryTest.java:236) 	at org.apache.jackrabbit.oak.plugins.index.solr.query.SolrIndexQueryTest.sql2(SolrIndexQueryTest.java:79) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) 	at java.lang.reflect.Method.invoke(Method.java:597) 	at org.junit.runners.model.FrameworkMethod 1.runReflectiveCall(FrameworkMethod.java:44) 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41) 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20) 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28) 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31) 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76) 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50) 	at org.junit.runners.ParentRunner 3.run(ParentRunner.java:193) 	at org.junit.runners.ParentRunner 1.schedule(ParentRunner.java:52) 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191) 	at org.junit.runners.ParentRunner.access 000(ParentRunner.java:42) 	at org.junit.runners.ParentRunner 2.evaluate(ParentRunner.java:184) 	at org.junit.runners.ParentRunner.run(ParentRunner.java:236) 	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:50) 	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197) Caused by: org.apache.lucene.queryparser.classic.ParseException: Cannot parse 'path_exact:/test*': Lexical error at line 1, column 18.  Encountered: <EOF> after : "/test*" 	at org.apache.lucene.queryparser.classic.QueryParserBase.parse(QueryParserBase.java:130) 	at org.apache.solr.search.LuceneQParser.parse(LuceneQParserPlugin.java:72) 	at org.apache.solr.search.QParser.getQuery(QParser.java:143) 	at org.apache.solr.update.DirectUpdateHandler2.getQuery(DirectUpdateHandler2.java:310) 	... 58 more Caused by: org.apache.lucene.queryparser.classic.TokenMgrError: Lexical error at line 1, column 18.  Encountered: <EOF> after : "/test*" 	at org.apache.lucene.queryparser.classic.QueryParserTokenManager.getNextToken(QueryParserTokenManager.java:1048) 	at org.apache.lucene.queryparser.classic.QueryParser.jj_ntk(QueryParser.java:638) 	at org.apache.lucene.queryparser.classic.QueryParser.Clause(QueryParser.java:246) 	at org.apache.lucene.queryparser.classic.QueryParser.Query(QueryParser.java:181) 	at org.apache.lucene.queryparser.classic.QueryParser.TopLevelQuery(QueryParser.java:170) 	at org.apache.lucene.queryparser.classic.QueryParserBase.parse(QueryParserBase.java:120) 	... 61 more {code}$$patch1-oak-35a7f014_Developer_PatchNaturalnessYe$$Fixed path separator in SolrIndexUpdate. $$1
oak-a0dc4c89$$Diff reads too many nodes$$DocumentNodeStore.diffManyChildren() may read too many nodes when there is an inactive cluster node with an old _lastRev on the root document. This is a regression introduced with the fix for OAK-2232.  The fix assumes an inactive cluster node does not have a revision range with an old revision seen at a current timestamp. The DocumentNodeStore will in fact purge revisions from the range in the RevisionComparator after an hour. But on startup the first background read may populate the RevisionComparator with a revision, which is potentially very old (e.g. if the clusterId is not used anymore).$$patch1-oak-a0dc4c89_Developer_PatchNaturalnessYe$$"OAK - 2345"" consider as external change if the revision was never deleted """. add static modifier to revisionComparator. $$1
oak-2565d74a$$BackgroundLeaseUpdate not scheduled when asyncDelay=0$$The BackgroundLeaseUpdate extends from NodeStoreTask, which returns from the run() method when asyncDelay is 0. This is fine for the background read and update tasks. However, the lease update task must run even when asyncDelay is set to zero.$$patch1-oak-2565d74a_Developer_PatchNaturalnessYe$$added missing import. give NodeStoreTask a chance to cache node delay if it is not used. Fix DocumentNodeStore ' s async delay value. Add back missing constructor .. $$1
oak-b896c926$$Item names starting with '{X}' cause RepositoryException$$The exception is RepositoryException: Invalid name or path: {0} foo  E.g. for an item named '{0} foo'.  I guess oak-jcr tries to interpret it as a name in expanded form but does not find a namespace uri for '0'. IIRC these names are valid in Jackrabbit 2.x.$$patch1-oak-b896c926_Developer_PatchNaturalnessYe$$Fixed check for expanded name. $$1
oak-8159fc21$$Lucene AND query with a complex OR phrase returns incorrect result$$Queries like this {noformat}/jcr:root/content//element(*, test:Asset)[(jcr:contains(., 'cube')) and (jcr:contains(jcr:content/@foo, '"a" OR "b"'))] {noformat} returns wrong results.  This get converted to {noformat}+:fulltext:cube full:jcr:content/foo:"a" full:jcr:content/foo:"b" {noformat}$$patch1-oak-8159fc21_Developer_PatchNaturalnessYe$$Remove must_not ( ) in BooleanQuery. Remove must_not ( x ) from must_not ( x ) since it ' s not. $$1
oak-f4324736$$LuceneIndexProviderService may miss on registering PreExtractedTextProvider$${{LuceneIndexProviderService}} has an optional dependency on {{PreExtractedTextProvider}}. In such a case it can happen that bind for the provided is invoked before the activate is called. In such a case the provider would not be registered.$$patch1-oak-f4324736_Developer_PatchNaturalnessYe$$Fixed NPE in javadoc. added missing register. $$1
oak-a8493efc$$The Property2Index eagerly and unnecessarily fetches all data$$Currently, the Property2Index (as well as the PropertyIndex and the NodeTypeIndex) loads all paths into a hash set. This is even the case for the getCost operation, which should be fast and therefore not load too much data.  This strategy can cause ouf-of-memory if the result is too big. Also, loading all data is not necessary unless the user reads all rows.  Instead, the index should only load data on demand. Also, the getCost operation should only estimate the number of read nodes, and not actually read the data.$$patch1-oak-a8493efc_Developer_PatchNaturalnessYe$$added missing import. "Revert "" update to latest property2 index support """. added static modifier to Property2IndexLookup. Fix an issue with property value being null or undefined. added static modifier to Index2IndexLookup. $$1
oak-2b5d3afb$$Full-text search on the traversing index fails if the condition contains a slash$$A full-text search on the traversing index falls back to a sort of manual evaluation of results.  This is handled by the _FullTextTerm_ class, and it appears that it passes the constraint text through a cleanup process where it strips most of the characters that are neither _Character.isLetterOrDigit(c)_ not in the list _+-:&_  I'm not exactly sure where this list comes from, but I see the '/' character is missing which causes a certain type of query to fail.  Example: {code} //*[jcr:contains(., 'text/plain')] {code}$$patch1-oak-2b5d3afb_Developer_PatchNaturalnessYe$$"fix false positive in "" + - > "" ( according to Duden it ' s Wo. $$1
oak-037dea72$$XPath: queries starting with "//" are not always converted correctly$$XPath queries starting with "//" are not always converted to the expected SQL-2 query. Examples:  {noformat} //element(*, oak:QueryIndexDefinition)/* select [jcr:path], [jcr:score], * from [oak:QueryIndexDefinition] as a  //element(*, oak:QueryIndexDefinition)//* select [jcr:path], [jcr:score], * from [oak:QueryIndexDefinition] as a {noformat}  This is wrong. Instead, a join should be used.$$patch1-oak-037dea72_Developer_PatchNaturalnessYe$$added more token. added missing closing parenthesis in XPathToSQL2Converter. $$1
oak-3e83a4c1$$NodeDocument _modified may go back in time$$In a cluster with multiple DocumentMK instances the _modified field of a NodeDocument may go back in time. This will result in incorrect diff calculations when the DocumentNodeStore uses the _modified field to find changed nodes for a given revision range.$$patch1-oak-3e83a4c1_Developer_PatchNaturalnessYe$$added missing import. added version check. "Revert "" check version """. Fixed MongoDB operation type. added maxUpdates = new BasicDBObject ( ) ;. Fixed case. added max / max updates to update - > min. add UpdateUtils . applyChanges ( ). Added MAX op to UpdateUtils. Fixing # 12508. Fix update update op. "Remove map entry operation with type = "" REMOVE_MAP_ENTRY "" ; don '. update update op with max ( String , Object ). Fix error in updateOp. "remove unnecessary "" type = "" in UpdateOp constructor". Added update operation for Long value update .. Add constructor to UpdateOp. Fix update op implementation. $$1
oak-cb3ac20d$$Lucene Index should ignore property existence checks$$Some optimizations on the query engine transform certain clauses in property existence checks. ie (p = 'somevalue' turns into 'p is not null').  This doesn't play well with lucene as it can not  effectively build a 'not null' query, even worse the query doesn't return any results.  As a fix I'll just skip the existence constraints from the generated lucene query.$$patch1-oak-cb3ac20d_Developer_PatchNaturalnessYe$$add OAK - 1208 - LuceneIndex does not handle child - level property restrictions. $$1
oak-01a8b283$$DataStoreBlobStore does not take into maxLastModifiedTime when fetching all chunks$$Currently the {{DataStoreBlobStore}} has a pending TODO  {code}  @Override     public Iterator<String> getAllChunkIds(long maxLastModifiedTime) throws Exception {         //TODO Ignores the maxLastModifiedTime currently.         return Iterators.transform(delegate.getAllIdentifiers(), new Function<DataIdentifier, String>() {             @Nullable             @Override             public String apply(@Nullable DataIdentifier input) {                 return input.toString();             }         });     } {code}  Due to this it currently returns all blobId. This would issue when new binary gets created while a blob gc is running as such binaries might be considered orphan and deleted$$patch1-oak-01a8b283_Developer_PatchNaturalnessYe$$added missing import. add missing imports. added log output. $$1
oak-073b814c$$Node isNew() is false in case the node is removed and added in same commit$$When you remove a Node /path/a transiently and add one add /path/a again. The transiently added Node isNew() check will be false. {code} root.getNode(name).remove(); Node newNode = root.addNode(name); nowNode.isNew() => false {code}  The API says {quote} Returns true if this is a new item, meaning that it exists only in transient storage on the Session and has not yet been saved. Within a transaction, isNew on an Item may return false (because the item has been saved) even if that Item is not in persistent storage (because the transaction has not yet been committed).... {quote}$$patch1-oak-073b814c_Developer_PatchNaturalnessYe$$Improved JDK7 build workflow. $$1
oak-0adf3a6e$$Folder containing an admin user should not be removed$$The action of removing a folder that contains the admin user should fail.  This is already the case if it is tried to remove the admin node .  Attaching unit test$$patch1-oak-0adf3a6e_Developer_PatchNaturalnessYe$$avoid NPE. added simple constraint exception for removed users .. $$1
oak-06c367af$$Cost per entry for Lucene index of type v1 should be higher than that of v2$$Currently default cost per entry for Lucene index of type # v1 - which uses query time aggregation # v2 - which uses index time aggregation  Are same. However given that query time aggregation would require more effort it should result in a higher cost per entry.  This fact impacts the result in cases like OAK-2081 (see last few comments) where with usage of limits both index are currently considered equals$$patch1-oak-06c367af_Developer_PatchNaturalnessYe$$Fixed missing default value. Should fix # 184. $$1
oak-beaca1a4$$IndexPlanner returning plan for queries involving jcr:score$$Consider a query like   {noformat} /jcr:root//element(*, cq:Taggable)[ (@cq:tags = 'geometrixx-outdoors:activity/biking' or @cq:tags = '/etc/tags/geometrixx-outdoors/activity/biking') ] order by @jcr:score descending  {noformat}  And a seemingly non related index like  {noformat} /oak:index/assetType   ...   - type = "lucene"   + indexRules     + nt:base       + properties         + assetType           - propertyIndex = true           - name = "assetType" {noformat}  Then currently {{IndexPlanner}} would return a plan because even when it cannot evaluate any of property restrictions because it thinks it can sort on {{jcr:score}}. This later results in an exception like  {noformat} 14.01.2015 16:16:35.866 *ERROR* [0:0:0:0:0:0:0:1 [1421248595863] POST /bin/tagcommand HTTP/1.1] org.apache.sling.engine.impl.SlingRequestProcessorImpl service: Uncaught Throwable java.lang.IllegalStateException: No query created for filter Filter(query=select [jcr:path], [jcr:score], * from [cq:Taggable] as a where [cq:tags] in('geometrixx-outdoors:activity/swimming', '/etc/tags/geometrixx-outdoors/activity/swimming') and isdescendantnode(a, '/') order by [jcr:score] desc /* xpath: /jcr:root//element(*, cq:Taggable)[ (@cq:tags = 'geometrixx-outdoors:activity/swimming' or @cq:tags = '/etc/tags/geometrixx-outdoors/activity/swimming') ] order by @jcr:score descending */, path=//*, property=[cq:tags=in(geometrixx-outdoors:activity/swimming, /etc/tags/geometrixx-outdoors/activity/swimming)]) 	at org.apache.jackrabbit.oak.plugins.index.lucene.LucenePropertyIndex.getQuery(LucenePropertyIndex.java:505) 	at org.apache.jackrabbit.oak.plugins.index.lucene.LucenePropertyIndex.access 200(LucenePropertyIndex.java:158) 	at org.apache.jackrabbit.oak.plugins.index.lucene.LucenePropertyIndex 1.loadDocs(LucenePropertyIndex.java:303) 	at org.apache.jackrabbit.oak.plugins.index.lucene.LucenePropertyIndex 1.computeNext(LucenePropertyIndex.java:261) 	at org.apache.jackrabbit.oak.plugins.index.lucene.LucenePropertyIndex 1.computeNext(LucenePropertyIndex.java:253) 	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143) {noformat}$$patch1-oak-beaca1a4_Developer_PatchNaturalnessYe$$added missing import. added missing import. Fixed small API check. Fixed a bug in canHandleSorting. $$1
oak-b6f89048$$Ordered index fails with old index content$$With the latest changes, the ordered index no longer works with old index data. When running the latest Oak 1.0.2 snapshot run against an Oak 1.0.0 repository with an existing ordered index, the index fails with the exception below.  As a workaround, the ordered index can be manually re-built. Either the index re-build needs to be automatic, or the ordered index needs to work with the old index content.  {noformat} java.lang.IndexOutOfBoundsException: index (3) must be less than size (1)     at com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:306)     at com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:285)     at org.apache.jackrabbit.oak.plugins.segment.SegmentPropertyState.getValue(SegmentPropertyState.java:157)     at org.apache.jackrabbit.oak.plugins.index.property.strategy.OrderedContentMirrorStoreStrategy.getPropertyNext(OrderedContentMirrorStoreStrategy.java:1024) {noformat}$$patch1-oak-b6f89048_Developer_PatchNaturalnessYe$$OrderedContentMirrorStoreStrategy now uses string[] or array of strings. $$1
oak-3f51fb09$$PropertyStates#createProperty ignores namespace mappings when creating states of type NAME and PATH$$as far as i saw we use PropertyStates#createProperty to create and set an OAK property from a given JCR value or a list of JCR values.  this works well for all types of values except for NAME, PATH which  may contain values with remapped namespaces which will not be converted back to oak-values during the state creation:  {code}      List<String> vals = Lists.newArrayList();      for (Value value : values) {          vals.add(value.getString());      }      return new MultiGenericPropertyState(name, vals, Type.fromTag(type, true)); {code}  if am not mistaken {code}value.getString(){code} will return the JCR representation of the value instead of the oak representation as it would be needed here.  possible solutions include: - passing namepathmapper to the create method - only accept oak Value implementation that allows to retrieve the   internal representation, which is present in the ValueImpl afaik.$$patch1-oak-3f51fb09_Developer_PatchNaturalnessYe$$Added missing import. Fixing failing tests. Fix error in PropertyStates # getString. Fixing failing tests. Allow string conversion from any class .. Improved javadoc in ValueImpl # getOakString. Improved JCR code. "Revert "" getOakString ( ) "" from ValueImpl where it is not defined. Fix hash code for string properties. $$1
oak-494da6de$$UserValidator and AccessControlValidator must not process hidden nodes$$This is similar to OAK-3019 but for {{UserValidator}} and {{AccessControlValidator}}.$$patch1-oak-494da6de_Developer_PatchNaturalnessYe$$"Revert "" update to latest commit """. AccessControlValidator should probably use newValidator. AccessControlValidator should probably use newValidator. Add a static constructor to AccessControlValidator. Fixing a TODO. added default constructor. $$1
oak-73cc2442$$MoveDetector does not detect moved nodes that have been moved in an earlier commit already$$None$$patch1-oak-73cc2442_Developer_PatchNaturalnessYe$$"Revert "" update memory node builder to point to the same node """. $$1
oak-913c2f53$$TarMK compaction can create mixed segments$$As described in http://markmail.org/message/ujkqdlthudaortxf, commits that occur while the compaction operation is running can make the compacted segments contain references to older data segments, which prevents old data from being reclaimed during cleanup.$$patch1-oak-913c2f53_Developer_PatchNaturalnessYe$$Added protected method to retrieve the segment tracker from the record .. Improved method to validate compaction status of segments. Fix Compactor constructor. added flush ( ) to FileStore. Drop the SegmentWriter caches before compacting any data .. Oops , now expect the writer to be passed through to constructor. do not compact if data is taken from cache. added uncompaction to SegmentWriter. $$1
oak-9120fd1b$$segment's compareAgainstBaseState wont call childNodeDeleted when deleting last and adding n nodes$${{SegmentNodeState.compareAgainstBaseState}} fails to call {{NodeStateDiff.childNodeDeleted}} when for the same parent the only child is deleted and at the same time multiple new, different children are added.  Reason is that the [current code|https://github.com/apache/jackrabbit-oak/blob/a9ce70b61567ffe27529dad8eb5d38ced77cf8ad/oak-segment/src/main/java/org/apache/jackrabbit/oak/plugins/segment/SegmentNodeState.java#L558] for '{{afterChildName == MANY_CHILD_NODES}}' *and* '{{beforeChildName == ONE_CHILD_NODE}}' does not handle all cases: it assumes that 'after' contains the 'before' child and doesn't handle the situation where the 'before' child has gone.$$patch1-oak-9120fd1b_Developer_PatchNaturalnessYe$$added beforeChildStillExists. added missing childNode delete. $$1
oak-ffa818f3$$Wrong compareTo in micro-kernel Id class$$CompareTo method in Id class fails in some cases.  {code}  // this works final Id id1 = Id.fromString( "0000000000000007" ); final Id id2 = Id.fromString( "000000000000000c" );  assertTrue( id1 + " should be less than " + id2, id1.compareTo( id2 ) < 0 );  // but this doesn't final Id id1 = Id.fromString( "0000000000000070" ); final Id id2 = Id.fromString( "00000000000000c0" );  assertTrue( id1 + " should be less than " + id2, id1.compareTo( id2 ) < 0 ); {code}$$patch1-oak-ffa818f3_Developer_PatchNaturalnessYe$$Fix the for - loop of Id . compareTo ( ). $$1
oak-f2a2edec$$NamePathMapper should fail on absolute paths escaping root$$The name path mapper should no accept invalid paths of type  {code} /.. {code}  I.e. paths which escape beyond the root of the hierarchy.$$patch1-oak-f2a2edec_Developer_PatchNaturalnessYe$$added missing var. Fixed parser with more than one root node. Increased the depth of the parser for relative names so it parses. "Revert "" update to latest oak """. Add missing final modifier to JcrPath. added relative path escape. "Revert "" getOakPath ( final String , final boolean ) """. Added parse errors for parent ( ). $$1
oak-591e4d4a$$AsyncIndexUpdate may resurrect nodes$$There is a race condition in the AsyncIndexUpdate.run() method. The implementation creates a checkpoint used as the after node state for the comparison with the previous checkpoint. In a next step a builder is created from the current root state of the node store. Node removed between the checkpoint call and retrieving the root state may get resurrected by the AsyncIndexUpdate.$$patch1-oak-591e4d4a_Developer_PatchNaturalnessYe$$Fix typo. $$1
oak-3efb5cbf$$Node not accessible after document split$$In a cluster setup it may happen that a node becomes inaccessible when all remaining local revision entries after a split are not yet visible to a cluster node.$$patch1-oak-3efb5cbf_Developer_PatchNaturalnessYe$$added missing import. added stable type to Commit. workflow workflow should validate revisions against changeRev. "avoid the "" null """. $$1
oak-077efee5$$ConstraintViolationException seen with multiple Oak/Mongo with ConcurrentCreateNodesTest$$While running ConcurrentCreateNodesTest with 5 instances writing to same Mongo instance following exception is seen  {noformat} Exception in thread "Background job org.apache.jackrabbit.oak.benchmark.ConcurrentCreateNodesTest Writer@3f56e5ed" java.lang.RuntimeException: javax.jcr.nodetype.ConstraintViolationException: OakConstraint0001: /: The primary type rep:root does not exist     at org.apache.jackrabbit.oak.benchmark.ConcurrentCreateNodesTest Writer.run(ConcurrentCreateNodesTest.java:111)     at org.apache.jackrabbit.oak.benchmark.AbstractTest 1.run(AbstractTest.java:481) Caused by: javax.jcr.nodetype.ConstraintViolationException: OakConstraint0001: /: The primary type rep:root does not exist     at org.apache.jackrabbit.oak.api.CommitFailedException.asRepositoryException(CommitFailedException.java:225)     at org.apache.jackrabbit.oak.api.CommitFailedException.asRepositoryException(CommitFailedException.java:212)     at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.newRepositoryException(SessionDelegate.java:679)     at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.save(SessionDelegate.java:553)     at org.apache.jackrabbit.oak.jcr.session.SessionImpl 8.perform(SessionImpl.java:417)     at org.apache.jackrabbit.oak.jcr.session.SessionImpl 8.perform(SessionImpl.java:414)     at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.perform(SessionDelegate.java:308)     at org.apache.jackrabbit.oak.jcr.session.SessionImpl.perform(SessionImpl.java:127)     at org.apache.jackrabbit.oak.jcr.session.SessionImpl.save(SessionImpl.java:414)     at org.apache.jackrabbit.oak.benchmark.ConcurrentCreateNodesTest Writer.run(ConcurrentCreateNodesTest.java:100)     ... 1 more Caused by: org.apache.jackrabbit.oak.api.CommitFailedException: OakConstraint0001: /: The primary type rep:root does not exist     at org.apache.jackrabbit.oak.plugins.nodetype.TypeEditor.constraintViolation(TypeEditor.java:150)     at org.apache.jackrabbit.oak.plugins.nodetype.TypeEditor.getEffectiveType(TypeEditor.java:286)     at org.apache.jackrabbit.oak.plugins.nodetype.TypeEditor.<init>(TypeEditor.java:101)     at org.apache.jackrabbit.oak.plugins.nodetype.TypeEditorProvider.getRootEditor(TypeEditorProvider.java:85)     at org.apache.jackrabbit.oak.spi.commit.CompositeEditorProvider.getRootEditor(CompositeEditorProvider.java:80)     at org.apache.jackrabbit.oak.spi.commit.EditorHook.processCommit(EditorHook.java:53)     at org.apache.jackrabbit.oak.spi.commit.CompositeHook.processCommit(CompositeHook.java:60)     at org.apache.jackrabbit.oak.spi.commit.CompositeHook.processCommit(CompositeHook.java:60)     at org.apache.jackrabbit.oak.spi.state.AbstractNodeStoreBranch InMemory.merge(AbstractNodeStoreBranch.java:498)     at org.apache.jackrabbit.oak.spi.state.AbstractNodeStoreBranch.merge(AbstractNodeStoreBranch.java:300)     at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch.merge(DocumentNodeStoreBranch.java:129)     at org.apache.jackrabbit.oak.plugins.document.DocumentRootBuilder.merge(DocumentRootBuilder.java:159)     at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.merge(DocumentNodeStore.java:1275)     at org.apache.jackrabbit.oak.core.MutableRoot.commit(MutableRoot.java:247)     at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.commit(SessionDelegate.java:405)     at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.save(SessionDelegate.java:551)     ... 7 more {noformat}  This has been reported by [~rogoz]$$patch1-oak-077efee5_Developer_PatchNaturalnessYe$$update Revision object in read only state. $$1
oak-2e16a983$$Query: UnsupportedOperationException for some combinations of "or" and "and" conditions$$The following query throws an UnsupportedOperationException:  {noformat} select * from [nt:base]    where [a] = 1 and [b] = 2 and [b] = 3 or [c] = 4 {noformat}$$patch1-oak-2e16a983_Developer_PatchNaturalnessYe$$removed OAK - 1933. "Revert "" update to latest query index """. $$1
oak-1552be04$$Unresolved conflicts in TokenProviderImpl#createToken()$$In certain situations (e.g. heavy load) {{TokenProviderImpl#createToken()}} might create some unresolved conflicts.  e.g.   {code} org.apache.jackrabbit.oak.api.CommitFailedException: OakState0001: Unresolved conflicts in /home/users/..../..../.tokens/2014-04-07T11.55.58.167+02.00 {code}  and  {code} 01.04.2014 17:52:41.216 *WARN* [qtp218544742-286] org.apache.jackrabbit.oak.security.authentication.token.TokenProviderImpl Failed to create login token. 01.04.2014 17:52:41.218 *WARN* [qtp218544742-300] org.eclipse.jetty.servlet.ServletHandler /projects.html java.lang.IllegalArgumentException: Invalid token ''     at org.apache.jackrabbit.api.security.authentication.token.TokenCredentials.<init>(TokenCredentials.java:42) {code}$$patch1-oak-1552be04_Developer_PatchNaturalnessYe$$added missing import. added ISO8601 format to JCR token names. added simple method to create token node below parent node. $$1
oak-05c89637$$CacheLIRS concurrency issue$$Some of the methods of the cache can throw a NullPointerException when the cache is used concurrently. Example stack trace:  {code} java.lang.NullPointerException: null org.apache.jackrabbit.oak.cache.CacheLIRS.values(CacheLIRS.java:470)  org.apache.jackrabbit.oak.cache.CacheLIRS 1.values(CacheLIRS.java:1432) org.apache.jackrabbit.oak.plugins.segment.file.FileStore.flush(FileStore.java:205) {code}$$patch1-oak-05c89637_Developer_PatchNaturalnessYe$$"Revert "" update to latest cache "" report". "Revert "" update CacheLIRS to latest cache "" issue". Improved cache implementation. Improved cache implementation. Improved cache presence code. Add null check in CacheLIRS. Fix possible NPE in CacheLIRS . get ( K ). $$1
oak-f37ce716$$DocumentNodeStore does not make use of References while serializing Blob$$The BlobSerializer in DocumentNodeStore does not make use of Blob references which results in copying the blobs by value hence significantly slowing down any migration$$patch1-oak-f37ce716_Developer_PatchNaturalnessYe$$added blob reference check. $$1
oak-64712735$$Some version copy settings conflicts with the earlyShutdown$$The {{RepositoryUpgrade#earlyShutdown}} property causes the source CRX2 repository to shutdown right after copying the content, before the first commit hook is launched. However, the {{VersionableEditor}} hook sometimes needs access to the source repository, to read the version histories that hasn't been copied yet (as the version histories are copied in two stages). As a result, the {{earlyShutdown}} may cause the upgrade process to fail.  {{earlyShutdown}} should be overriden for all cases in which the source repository is still needed in the commit hook phase. In particular, it should be set to {{false}} if:  * orphaned version histories are not copied, * orphaned version histories are copied, but the copyOrphanedVersion date is set after the copyVersion date.$$patch1-oak-64712735_Developer_PatchNaturalnessYe$$Fixed issue with merging of old commit hooks. Fix overrideEarlyShutdown because of new configuration .. $$1
oak-f72dd8d1$$Uploading large number of files to single folder fails.$$Repository: OAK with TarPM  Upload is successful till 254 files and it started failing afterwards with exception in server logs.  [1]  {code} 14.11.2013 12:36:34.608 *ERROR* [10.40.146.206 [1384412794576] POST /content/dam/cq9032/./Coconut-5mb-110.jpg HTTP/1.1] org.apache.sling.servlets.post.impl.operations.ModifyOperation Exception during response processing. java.lang.IllegalStateException: null 	at com.google.common.base.Preconditions.checkState(Preconditions.java:133) ~[na:na] 	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeRecordId(SegmentWriter.java:259) ~[na:na] 	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeListBucket(SegmentWriter.java:346) ~[na:na] 	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeList(SegmentWriter.java:508) ~[na:na] 	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeProperty(SegmentWriter.java:669) ~[na:na] 	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:847) ~[na:na] 	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter 3.childNodeChanged(SegmentWriter.java:806) ~[na:na] 	at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:387) ~[na:na] 	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:797) ~[na:na] 	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter 3.childNodeChanged(SegmentWriter.java:806) ~[na:na] 	at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:387) ~[na:na] 	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:797) ~[na:na] 	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter 3.childNodeChanged(SegmentWriter.java:806) ~[na:na] 	at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:387) ~[na:na] 	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:797) ~[na:na] 	at org.apache.jackrabbit.oak.plugins.segment.SegmentRootBuilder.getNodeState(SegmentRootBuilder.java:53) ~[na:na] 	at org.apache.jackrabbit.oak.plugins.segment.SegmentRootBuilder.getNodeState(SegmentRootBuilder.java:21) ~[na:na] 	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStore.rebase(SegmentNodeStore.java:135) ~[na:na] 	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStore.merge(SegmentNodeStore.java:113) ~[na:na] 	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService.merge(SegmentNodeStoreService.java:174) ~[na:na] 	at org.apache.jackrabbit.oak.core.AbstractRoot.commit(AbstractRoot.java:260) ~[na:na] 	at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.commit(SessionDelegate.java:224) ~[na:na] 	at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.commit(SessionDelegate.java:219) ~[na:na] 	at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.commit(SessionDelegate.java:207) ~[na:na] 	at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.save(SessionDelegate.java:332) ~[na:na] 	at org.apache.jackrabbit.oak.jcr.session.SessionImpl 8.perform(SessionImpl.java:399) ~[na:na] 	at org.apache.jackrabbit.oak.jcr.session.SessionImpl 8.perform(SessionImpl.java:396) ~[na:na] 	at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.perform(SessionDelegate.java:128) ~[na:na] 	at org.apache.jackrabbit.oak.jcr.session.SessionImpl.perform(SessionImpl.java:117) ~[na:na] 	at org.apache.jackrabbit.oak.jcr.session.SessionImpl.save(SessionImpl.java:396) ~[na:na] 	at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source) ~[na:na] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) ~[na:1.6.0_26] 	at java.lang.reflect.Method.invoke(Unknown Source) ~[na:1.6.0_26] 	at org.apache.sling.jcr.base.SessionProxyHandler SessionProxyInvocationHandler.invoke(SessionProxyHandler.java:109) ~[na:na] 	at  Proxy9.save(Unknown Source) ~[na:na] {code}$$patch1-oak-f72dd8d1_Developer_PatchNaturalnessYe$$increase readme. $$1
oak-8079f7b5$$issues with JsopBuilder.encode and .escape$$1) escape() escapes many characters that do not need to be escaped (>127)  2) encode() does not encode many control characters that would need to be escaped when read through a JSON parser.$$patch1-oak-8079f7b5_Developer_PatchNaturalnessYe$$added int cast to improve JSON output. "Fixed issue where "" isSurrogatePair ( char ) "" was failing in JSON format". $$1
oak-52372042$$Parallel execution of ConcurrentReadAccessControlledTreeTest fails with MongoMK$$The is caused by concurrent creation of test content and the conflict it creates in the index. Every Oak test instance tries to create {{/oak:index/nodetype/:index/nt%3Afile}}, but only one will succeed. AFAICS there are two options how to handle this:  - Implement conflict annotation (OAK-1185), though I'm not sure this will really work. On commit, the rebase happens first, when changes from the other Oak instance may not be visible yet. Then, the commit hook runs and perform another branch commit with the changes, which works fine. Only the last step fails, when MongoMK tries to merge the branch. This is the point when the conflict may be detected.  - Implement a retry logic in MongoMK/NS$$patch1-oak-52372042_Developer_PatchNaturalnessYe$$add method to remove revision from node document. added method to remove commit root mapping. added method to remove deleted from update - removed - > updateMapEntry. added missing imports. added missing import. don ' t undo changes for the branch , because they are not marked as committed. Fixing return type for method ' done ' in MongoNodeStore. "Add @ Nonnull method to "" resetDiff "" that implements interface method". $$1
oak-d7f0f180$$IllegalArgumentException on Row.getValues()$$Calling {{row.getValues()}} is throwing an {{IllegalArgumentException}} when called on the {{QueryResult}} of the query {{SELECT properties FROM \[nt:base\] WHERE \[sling:resourceType\]="cq/personalization/components/contextstores/surferinfo"}}  {quote} java.lang.IllegalArgumentException 	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:76) 	at org.apache.jackrabbit.oak.plugins.value.ValueImpl.checkSingleValued(ValueImpl.java:85) 	at org.apache.jackrabbit.oak.plugins.value.ValueImpl.<init>(ValueImpl.java:72) 	at org.apache.jackrabbit.oak.plugins.value.ValueFactoryImpl.createValue(ValueFactoryImpl.java:95) 	at org.apache.jackrabbit.oak.jcr.query.QueryResultImpl.createValue(QueryResultImpl.java:266) 	at org.apache.jackrabbit.oak.jcr.query.RowImpl.getValues(RowImpl.java:99) 	at com.day.cq.analytics.sitecatalyst.impl.FrameworkComponentImpl.getListProperty(FrameworkComponentImpl.java:128) 	at com.day.cq.analytics.sitecatalyst.impl.FrameworkComponentImpl.<init>(FrameworkComponentImpl.java:91) {quote}$$patch1-oak-d7f0f180_Developer_PatchNaturalnessYe$$added missing import. Fixed mvpToString for RowImpl # hashCode ( ). $$1
oak-8188ef54$$Incorrect handling of multivalued comparisons in queries$$[Section 6.7.14|http://www.day.com/specs/jcr/2.0/6_Query.html#6.7.16 Comparison] of the JCR 2.0 spec says:  bq. ... operand1 may evaluate to an array of values (for example, the values of a multi-valued property), in which case the comparison is separately performed for each element of the array, and the Comparison constraint is satisfied as a whole if the comparison against any element of the array is satisfied.  This is currently not the case in Oak. Instead only the first value of the array is used in the comparison.$$patch1-oak-8188ef54_Developer_PatchNaturalnessYe$$"Revert "" update to latest property workflow """. fixed JCR 2 . 0 constraint. $$1
oak-be3a9114$$TARMK Cold Standby size increase due to checkpoints copy$$The current sync design gets confused by existing checkpoints and tries to copy them by value, bypassing the current storage optimization where there are a lot of references to existing content. this can result in a considerable size increase on the standby.$$patch1-oak-be3a9114_Developer_PatchNaturalnessYe$$"Revert "" update to latest standby plugin """. remove logonly option from StandbyApplyDiff constructor. don ' t log by default. don ' t log by default. don ' t remove logonly property when apply is called. remove childNodeAdded logonly for segment node states. added logonly condition for childNodeChanged. don ' t log childNodeDeleted if logOnly is set. $$1
oak-f3c9c818$$NPE in RecordIdMap$${{RecordIdMap}} is not properly guarded against NPEs when calling accessors on an empty map (which is represented by {{keys == null}}.   {noformat} testRecordIdMap(org.apache.jackrabbit.oak.plugins.segment.RecordIdMapTest)  Time elapsed: 0.019 sec  <<< ERROR! java.lang.NullPointerException at org.apache.jackrabbit.oak.plugins.segment.RecordIdMap.size(RecordIdMap.java:100) at org.apache.jackrabbit.oak.plugins.segment.RecordIdMapTest.testRecordIdMap(RecordIdMapTest.java:64) {noformat}$$patch1-oak-f3c9c818_Developer_PatchNaturalnessYe$$Fixed NPE in RecordIdMap. Fix possible NPE in RecordIdMap put. remove null check. Add throws notice for getKey. Add throws. $$1
oak-f2740ce1$$Ordered index does not return relative properties for un-restricted indexes$$Even if we specify an index without any restriction to node type; the ordered index does not return any result for relative properties$$patch1-oak-f2740ce1_Developer_PatchNaturalnessYe$$fixed opengl version. $$1
oak-503451c1$$ContentMirrorStoreStrategy #insert fails to enforce uniqueness and is slow$$Following OAK-734 I've noticed that the _ContentMirrorStoreStrategy_ fails to enforce the uniqueness constraints assumed on the #insert method.  It is also responsible for a slowdown on the #insert method because of the behavior change of the Property2Index (very frequent saves instead of a bulk one).$$patch1-oak-503451c1_Developer_PatchNaturalnessYe$$added TODO. "Revert "" throw exception on uniqueness constraint for key "" + key". "Revert "" update to index ( ) with unique = true """. $$1
oak-90c45a02$$NodeBuilder deleted child nodes can come back$$While working on OAK-520, I've noticed a problem with the NodeBuilder: when we delete an entire hierarchy of nodes and then recreate a part of it, some of the previously deleted nodes can come back.  This only happens when there are more than 3 levels of nodes.  So given a hierarchy of nodes: /x/y/z deleted 'x' and simply use the NodeBuilder to traverse down on the same path: .child('x').child('y'). At this point the 'z' child reappears even though it was deleted before.   I'll attach a test case shortly.$$patch1-oak-90c45a02_Developer_PatchNaturalnessYe$$Improved method to check removed state. Fix memory node builder. Improved method to return removed state from MemoryNodeBuilder. $$1
oak-08b25cb0$$Lucene Index property definition is ignored if its not in includePropertyNames config$$Lucene index property definition will not be used unless that property is in includePropertyNames config. This enforces including that property in includePropertyNames. includePropertyNames restricts all properties from getting indexed, so user is now enforced to include all properties in includePropertyNames to be indexed.$$patch1-oak-08b25cb0_Developer_PatchNaturalnessYe$$added missing comment. added property definition for property nodes. $$1
oak-63070cf9$$Lucene should not serve queries for what it doesn't index$$If a query is asked and Lucene is chosen as index for serving it, it will try to serve all the restrictions of the query, even the one that are not indexed.$$patch1-oak-63070cf9_Developer_PatchNaturalnessYe$$Add LuceneIndexConstants. added missing import. Fixed missing import. added getIndexDef to the plan. Fixed a bug in LuceneIndex. Fixed a bug in the LuceneIndex constructor. added index definition to nonFullTextConstraints. added node state constraint. don ' t check excluded properties and types in lucene ( they are not used ). Fixed lucene exclusion. $$1
oak-5449bf39$$PathUtils.isAncestor("/", "/") should return false but returns true$$None$$patch1-oak-5449bf39_Developer_PatchNaturalnessYe$$added parsePath method. "fix false alarm in "" denotes root "" of OSGi metadata". $$1
oak-00b4b8a0$$Moving or deleting tree instances with status NEW doesn't change its status to DISCONNECTED$$Further fall out from OAK-606:  {code}         Tree t = tree.addChild("new");          root.move("/x", "/y/x");         assertEquals(Status.DISCONNECTED, t.getStatus()); {code}  The assertion fails.$$patch1-oak-00b4b8a0_Developer_PatchNaturalnessYe$$added private field nodeBuilder. added constructor. Fix # 7751. use the already established node builder. Fixing a warning. Fix getChildCount ( ). Fix typo. added missing semicolon. added OAK - 621. Fixing a warning. added missing parent node builder property. Fix warning. Fixing a warning. Fixing a warning. Fix removeProperty ( ). "Remove "" nodeBuilder "" check for reparenting children of workflow". Fix # 444. Fix # 1796. Fixed issue with ensureChildOrderProperty. $$1
oak-7c320b1e$$issues with JsopBuilder.encode and .escape$$1) escape() escapes many characters that do not need to be escaped (>127)  2) encode() does not encode many control characters that would need to be escaped when read through a JSON parser.$$patch1-oak-7c320b1e_Developer_PatchNaturalnessYe$$removed erroneous legacy check. Add private escape method. Improved JsopBuilder . forDigit ( ). $$1
oak-3979fa8d$$OakIndexInput cloned instances are not closed$$Related to the inspections I was doing for OAK-2798 I also noticed that we don't fully comply with the {{IndexInput}}javadoc [1]as the cloned instances should throw the given exception if original is closed, but I also think that the original instance should close the cloned instances, see also [ByteBufferIndexInput#close|https://github.com/apache/lucene-solr/blob/lucene_solr_4_7_1/lucene/core/src/java/org/apache/lucene/store/ByteBufferIndexInput.java#L271].  [1]: {code} /** Abstract base class for input from a file in a {@link Directory}.  A  * random-access input stream.  Used for all Lucene index input operations.  *  * <p>{@code IndexInput} may only be used from one thread, because it is not  * thread safe (it keeps internal state like file position). To allow  * multithreaded use, every {@code IndexInput} instance must be cloned before  * used in another thread. Subclasses must therefore implement {@link #clone()},  * returning a new {@code IndexInput} which operates on the same underlying  * resource, but positioned independently. Lucene never closes cloned  * {@code IndexInput}s, it will only do this on the original one.  * The original instance must take care that cloned instances throw  * {@link AlreadyClosedException} when the original one is closed. {code}$$patch1-oak-3979fa8d_Developer_PatchNaturalnessYe$$added missing import. added missing import. added missing import. added readClone support. added read ( ) method to OakDirectory. added isClosed check. $$1
oak-531aca78$$IllegalArgumentException on Row.getValues()$$Calling {{row.getValues()}} is throwing an {{IllegalArgumentException}} when called on the {{QueryResult}} of the query {{SELECT properties FROM \[nt:base\] WHERE \[sling:resourceType\]="cq/personalization/components/contextstores/surferinfo"}}  {quote} java.lang.IllegalArgumentException 	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:76) 	at org.apache.jackrabbit.oak.plugins.value.ValueImpl.checkSingleValued(ValueImpl.java:85) 	at org.apache.jackrabbit.oak.plugins.value.ValueImpl.<init>(ValueImpl.java:72) 	at org.apache.jackrabbit.oak.plugins.value.ValueFactoryImpl.createValue(ValueFactoryImpl.java:95) 	at org.apache.jackrabbit.oak.jcr.query.QueryResultImpl.createValue(QueryResultImpl.java:266) 	at org.apache.jackrabbit.oak.jcr.query.RowImpl.getValues(RowImpl.java:99) 	at com.day.cq.analytics.sitecatalyst.impl.FrameworkComponentImpl.getListProperty(FrameworkComponentImpl.java:128) 	at com.day.cq.analytics.sitecatalyst.impl.FrameworkComponentImpl.<init>(FrameworkComponentImpl.java:91) {quote}$$patch1-oak-531aca78_Developer_PatchNaturalnessYe$$LPS - 50819 Also use mvpToString for array types. $$1
oak-25a70439$$Parallel execution of SimpleSearchTest fails with MongoMK$$At some point in the benchmark run one MongoMK instance will fail to read a node created by another instance. The exception is very similar to *E1* reported in OAK-1204.$$patch1-oak-25a70439_Developer_PatchNaturalnessYe$$workflow update - don ' t publish revisions of other cluster nodes. workflow update - remove an obsolete code. $$1
oak-90ad50da$$RDB/MongoDocumentStore may return stale documents$$It appears that the implementations of the {{update}} method sometimes populate the memory cache with documents that do not reflect any current or previous state in the persistence (that is, miss changes made by another node).  (will attach test)$$patch1-oak-90ad50da_Developer_PatchNaturalnessYe$$invalidate nodes cache on chunked IDs. Improved the invalidation of cached documents in RDBDocumentStore. removed unused imports. added missing import. Fixed issue with invalidateCache. update cache with nodes updated after last load. invalidate nodes cache on update ( because we think they ' re still in use ). Fixed error in getModCounts ( ). $$1
oak-9c2421ed$$Unnecessary invocations of LastRevRecovery when recovery already done.$$Even after _lastRev recovery executed on a cluster node, there are unnecessary  invocations of recovery happening on that cluster node, till that cluster node comes online again.$$patch1-oak-9c2421ed_Developer_PatchNaturalnessYe$$remove legacy metadata from MissingLastRevSeeker. $$1
oak-5135cf4b$$VersionablePathHook must not process hidden nodes$$The VersionablePathHook also processes hidden nodes, e.g. index data, which adds considerable overhead to the merge phase.$$patch1-oak-5135cf4b_Developer_PatchNaturalnessYe$$added missing import. Stop childNodeChanged ( ) for hidden nodes. avoid version history creation twice. $$1
oak-00b9bc52$$Two spaces in SQL2 fulltext search -> error$$Execute the following SQL2 query (eg, in crx/de's query tool) SELECT * FROM [nt:unstructured] AS c WHERE (CONTAINS(c.[jcr:title], 'a  b') AND ISDESCENDANTNODE(c, '/content')) (note there are 2 spaces between "a" and "b") Result: java.lang.IllegalArgumentException: Invalid expression: 'a b'  If there is only 1 space between a and b, there is no error.   Per jsr-283, fulltext expressions should be able to have strings of whitespace.$$patch1-oak-00b9bc52_Developer_PatchNaturalnessYe$$improve parser. $$1
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch1-oak-5286861d_Developer_PatchNaturalnessYe$$avoid NPE. $$1
oak-ce0b0955$$XPath queries: compatibility for missing @ in front of property names$$XPath queries with conditions of the form {noformat}[id='test']{noformat} are not problematic. Jackrabbit 2.x interpreted such conditions as {noformat}[@id='test']{noformat}, and Oak currently interprets them as {noformat}[@id/* = 'test']{noformat}, as this is the expected behavior for conditions of the form {noformat}[jcr:contains(id, 'test')]{noformat}.  I believe the condition {noformat}[id='test']{noformat} is illegal, and it would be better to throw an exception instead, saying a @ is missing.$$patch1-oak-ce0b0955_Developer_PatchNaturalnessYe$$"Revert "" Invalid @ in front of property name """. removed unnecessary &&. Permit Expression constructors to set thereWasNoAt. fixed error in XPathToSQL2Converter. $$1
oak-7acb091a$$Branch conflicts not detected by MongoMK$$MongoMK does not correctly detect conflicts when changes are committed into multiple branches concurrently and then merged back.  ConflictTest already covers conflict detection for non-branch commits and mixed branch/non-branch changes, but is missing tests for conflicting branches. I'll commit an ignored test to illustrate the problem.$$patch1-oak-7acb091a_Developer_PatchNaturalnessYe$$added missing @ param javadoc. Fix workflow error. formatDocument ( Map < String , Object > ). added missing import. added a few known collisions. Improved javadoc in Controller. Fix a typo in Controller # 12. added conflict message for merge of old commit , see # 772. remove false TODO. Fix default implementation of CollisionHandler. $$1
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1-oak-3ce758b7_Developer_PatchNaturalnessYe$$added PutTokenImpl constructor. $$1
oak-0ac7ff20$$TarMK Cold Standby can corrupt bulk segments$$There's a race condition on the segment transfer code that may introduce corrupted binary segments on the secondary instance. What can happen during the head sync phase is that the master may send the head segment twice which will make the client receive&store the second segment thinking it's a different one.$$patch1-oak-0ac7ff20_Developer_PatchNaturalnessYe$$Fix semantic versioning issue. Improved method to return the segment with the id. $$1
oak-0598498e$$DocumentNodeStore.dispatch() may pass null to NodeStateDiff$$This is a regression introduced by OAK-2562. The dispatch method passes a null state if the node does not exist at a given revision.$$patch1-oak-0598498e_Developer_PatchNaturalnessYe$$added missing closing parenthesis. added missing bracket. added getChildNode ( ). added getChildNode ( ) method that returns a node state that is not null. $$1
oak-3ae276c1$$Indexes: re-index automatically when adding an index$$When adding an index via import of content, the index is not automatically re-built. This is problematic, because subsequent queries will return no data because of that. Currently, the only way to re-index is to set the "reindex" property to "true".  I suggest that indexes are automatically re-indexes if the hidden child node (":data" I believe) is missing. This is in addition to the "reindex" property.$$patch1-oak-3ae276c1_Developer_PatchNaturalnessYe$$added missing import. add index definition editors before updating index update. don ' t reindex if definition has no children. $$1
oak-9b268da0$$Microkernel.diff returns empty diff when there are differences$${code} String rev1 = mk.commit("/", "+\"node1\":{\"node2\":{\"prop1\":\"val1\",\"prop2\":\"val2\"}}", null, null); String rev2 = mk.commit("/", "^\"node1/node2/prop1\":\"val1 new\" ^\"node1/node2/prop2\":null", null, null); String diff = mk.diff(rev1, rev2, "/node1/node2", 0); {code}  Here {{diff}} is empty although there are clearly differences between {{rev1}} and {{rev2}} at depth 0.$$patch1-oak-9b268da0_Developer_PatchNaturalnessYe$$added missing import. added children to copy. $$1
oak-14849e22$$java.lang.IllegalArgumentException when running FlatTreeWithAceForSamePrincipalTest$$Running  {code} java -jar oak-run*.jar benchmark FlatTreeWithAceForSamePrincipalTest Oak-Tar {code} will end with {code} java.lang.IllegalArgumentException 	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:77) 	at org.apache.jackrabbit.oak.plugins.segment.ListRecord.<init>(ListRecord.java:37) 	at org.apache.jackrabbit.oak.plugins.segment.ListRecord.getEntries(ListRecord.java:80) 	at org.apache.jackrabbit.oak.plugins.segment.SegmentPropertyState.getValue(SegmentPropertyState.java:130) 	at org.apache.jackrabbit.oak.util.PropertyBuilder.assignFrom(PropertyBuilder.java:225) 	at org.apache.jackrabbit.oak.util.PropertyBuilder.copy(PropertyBuilder.java:136) 	at org.apache.jackrabbit.oak.core.MutableTree.addChild(MutableTree.java:216) 	at org.apache.jackrabbit.oak.util.TreeUtil.addChild(TreeUtil.java:190) 	at org.apache.jackrabbit.oak.jcr.delegate.NodeDelegate.internalAddChild(NodeDelegate.java:841) 	at org.apache.jackrabbit.oak.jcr.delegate.NodeDelegate.addChild(NodeDelegate.java:684) 	at org.apache.jackrabbit.oak.jcr.session.NodeImpl 5.perform(NodeImpl.java:288) 	at org.apache.jackrabbit.oak.jcr.session.NodeImpl 5.perform(NodeImpl.java:253) 	at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.perform(SessionDelegate.java:125) 	at org.apache.jackrabbit.oak.jcr.session.ItemImpl.perform(ItemImpl.java:111) 	at org.apache.jackrabbit.oak.jcr.session.NodeImpl.addNode(NodeImpl.java:253) 	at org.apache.jackrabbit.oak.jcr.session.NodeImpl.addNode(NodeImpl.java:238) 	at org.apache.jackrabbit.oak.benchmark.FlatTreeWithAceForSamePrincipalTest.beforeSuite(FlatTreeWithAceForSamePrincipalTest.java:56) 	at org.apache.jackrabbit.oak.benchmark.AbstractTest.setUp(AbstractTest.java:113) 	at org.apache.jackrabbit.oak.benchmark.FlatTreeWithAceForSamePrincipalTest.setUp(FlatTreeWithAceForSamePrincipalTest.java:31) 	at org.apache.jackrabbit.oak.benchmark.AbstractTest.runTest(AbstractTest.java:151) 	at org.apache.jackrabbit.oak.benchmark.AbstractTest.run(AbstractTest.java:138) 	at org.apache.jackrabbit.oak.benchmark.FlatTreeWithAceForSamePrincipalTest.run(FlatTreeWithAceForSamePrincipalTest.java:31) 	at org.apache.jackrabbit.oak.benchmark.BenchmarkRunner.main(BenchmarkRunner.java:195) 	at org.apache.jackrabbit.oak.run.Main.main(Main.java:81)  {code}$$patch1-oak-14849e22_Developer_PatchNaturalnessYe$$Add an checkArgument to writeListBucket ( ). Fixed bug in SegmentWriter. added missing increment of bucket size. added missing i in last commit. $$1
oak-4ce4e3c9$$Node.getNodes throwing exception if user does not have access to any child node$$When trying to obtain child iterator via Node.getNodes {{InvalidItemStateException}} is thrown if user does not have access to its content  {code:java}     @Test     public void testGetChildren() throws Exception {         deny(path, privilegesFromName(PrivilegeConstants.JCR_ADD_CHILD_NODES));         NodeIterator it1 = testSession.getNode(path).getNodes();         while(it1.hasNext()){             Node n = it1.nextNode();             NodeIterator it2 = n.getNodes();         }     } {code}  Executing above code leads to following exception  {noformat} javax.jcr.InvalidItemStateException: Item is stale 	at org.apache.jackrabbit.oak.jcr.delegate.NodeDelegate.getTree(NodeDelegate.java:827) 	at org.apache.jackrabbit.oak.jcr.delegate.NodeDelegate.getChildren(NodeDelegate.java:336) 	at org.apache.jackrabbit.oak.jcr.session.NodeImpl 8.perform(NodeImpl.java:546) 	at org.apache.jackrabbit.oak.jcr.session.NodeImpl 8.perform(NodeImpl.java:543) 	at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.perform(SessionDelegate.java:125) 	at org.apache.jackrabbit.oak.jcr.session.ItemImpl.perform(ItemImpl.java:113) 	at org.apache.jackrabbit.oak.jcr.session.NodeImpl.getNodes(NodeImpl.java:543) 	at org.apache.jackrabbit.oak.jcr.security.authorization.ReadPropertyTest.testGetChildren(ReadPropertyTest.java:135) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) 	at org.apache.jackrabbit.test.AbstractJCRTest.run(AbstractJCRTest.java:464) 	at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:83) 	at org.junit.runner.JUnitCore.run(JUnitCore.java:157) 	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:77) 	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:195) 	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:63) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) 	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:120) {noformat}  The exception is thrown for path {{/testroot/node1/rep:policy}}.   The issue occurs because the {{NodeIterator}} {{it1}} includes {{rep:policy}} and later when its child are accessed security check leads to exception. Probably the {{it1}} should not include {{rep:policy}} as part of child list and filter it out$$patch1-oak-4ce4e3c9_Developer_PatchNaturalnessYe$$Fixed issue with NodeDelegate # filter ( ). added jira issue. added path to exception message. $$1
oak-a9efe3c4$$Wildcards in relative property paths don't work in search expressions$$A search XPath of the form: {code} /jcr:root/etc/commerce/products//*[@size='M' or */@size='M'] {code} returns: {code} Invalid path: * {code} (This works fine in Jackrabbit.)$$patch1-oak-a9efe3c4_Developer_PatchNaturalnessYe$$Fix modifier order in SourceImpl .. Improve javadoc in SimpleExcerptProvider. "Revert "" update selectorImpl """. added missing import. added currentProperty ( ) method that normalizes the property name. Fix SelectorImpl # currentOakProperty ( String , Integer ). "Revert "" update to latest oak property """. Fixed a bug in SelectorImpl # readOakProperties. added oak - specific javadoc. Fixed bug in query . getOakPath ( relativePath ). "Revert "" update to 2005 """. "Revert "" update to latest property value """. "Revert "" fix an issue with "" selector . currentProperty ( ) "" that would conflict. $$1
oak-c65b07c3$$Global fulltext index returning plan for pure NodeType queries$$On a system having  # Global fulltext index enabled with version V2 and {{evaluatePathRestriction}} enabled # NodeType index having indexing enabled for specific nodetype like cq:ClientLibraryFolder  A query like  {noformat} /jcr:root//element(*, cq:ClientLibraryFolder) {noformat}  Ends up getting evaluated by fulltext index as it return plan with include all query  *Expected* For such query global fulltext index should not return any plan if the path restriction is on root path with include all children$$patch1-oak-c65b07c3_Developer_PatchNaturalnessYe$$Optimize optimize_query ( ). $$1
oak-56accddf$$AssertionError thrown for Lucene index with empty suggest disctionary$$Create an index where one field is enabled for suggestion but no content is indexed for that index i.e. no matching content. Then while performing any query following exception is thrown  {noformat} java.lang.AssertionError 	at org.apache.lucene.search.suggest.analyzing.AnalyzingInfixSuggester.<init>(AnalyzingInfixSuggester.java:167) 	at org.apache.jackrabbit.oak.plugins.index.lucene.util.SuggestHelper 2.<init>(SuggestHelper.java:127) 	at org.apache.jackrabbit.oak.plugins.index.lucene.util.SuggestHelper.getLookup(SuggestHelper.java:127) 	at org.apache.jackrabbit.oak.plugins.index.lucene.util.SuggestHelper.getLookup(SuggestHelper.java:123) 	at org.apache.jackrabbit.oak.plugins.index.lucene.IndexNode.<init>(IndexNode.java:109) 	at org.apache.jackrabbit.oak.plugins.index.lucene.IndexNode.open(IndexNode.java:69) 	at org.apache.jackrabbit.oak.plugins.index.lucene.IndexTracker.findIndexNode(IndexTracker.java:162) 	at org.apache.jackrabbit.oak.plugins.index.lucene.IndexTracker.acquireIndexNode(IndexTracker.java:137) 	at org.apache.jackrabbit.oak.plugins.index.lucene.LucenePropertyIndex.getPlans(LucenePropertyIndex.java:249) 	at org.apache.jackrabbit.oak.query.QueryImpl.getBestSelectorExecutionPlan(QueryImpl.java:1016) 	at org.apache.jackrabbit.oak.query.QueryImpl.getBestSelectorExecutionPlan(QueryImpl.java:949) 	at org.apache.jackrabbit.oak.query.ast.SelectorImpl.prepare(SelectorImpl.java:288) {noformat}  This happens with {{-ea}} flag i.e. java assertions enabled. It caused [here|https://github.com/apache/lucene-solr/blob/releases/lucene-solr/4.7.1/lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester.java#L167]$$patch1-oak-56accddf_Developer_PatchNaturalnessYe$$Fixed issue with suggester update code. $$1
oak-44585b0c$$AccessDenied when modifying transiently moved item with too many ACEs$$If at least the following preconditions are fulfilled, saving a moved item fails with access denied:  1. there are more PermissionEntries in the PermissionEntryCache than the configured EagerCacheSize 2. an node is moved to a location where the user has write access through a group membership 3. a property is added to the transiently moved item  For example: 1. set the *eagerCacheSize* to '0' 2. create new group *testgroup* and user *testuser* 3. make *testuser* member of *testgroup* 4. create nodes {{/testroot/a}} and {{/testroot/a/b}} and {{/testroot/a/c}} 5. allow *testgroup* {{rep:write}} on {{/testroot/a}} 6. as *testuser* create {{/testroot/a/b/item}} (to verify that the user has write access) 7. as *testuser* move {{/testroot/a/b/item}} to {{/testroot/a/c/item}} 8. {{save()}} -> works 9. as *testuser* move {{/testroot/a/c/item}} back to {{/testroot/a/b/item}} AND add new property to the transient {{/testroot/a/b/item}} 10. {{save()}} -> access denied$$patch1-oak-44585b0c_Developer_PatchNaturalnessYe$$Improved permission validation for move - > copy trees. $$1
oak-ca85ecce$$Released checkpoint can still be retrieved$$The following fails on the 2nd assertion on the MongoMK  {code} assertTrue(store.release(cp)); assertNull(store.retrieve(cp)); {code}  The JavaDoc on the {{release}} method is a bit vague, but I assume it is safe to assume that when it returns {{true}} the checkpoint should be gone. If not, we should update the JavaDoc.$$patch1-oak-ca85ecce_Developer_PatchNaturalnessYe$$Fixed NPE in DocumentNodeStore . retrieve ( ). $$1
oak-920f32d0$$[LucenePropertyIndex] full-text search on first level relative node returns no result$$Following query does not return any result even with a proper index defined [1]. {noformat}//element(*, test:Page)[ " +             "jcr:contains(jcr:content, 'summer') ] {noformat}  [1] {code} {   "jcr:primaryType": "oak:QueryIndexDefinition",   "compatVersion": 2,   "name": "pageIndex",   "type": "lucene",   "async": "async",   "reindex": true,   "aggregates": {     "jcr:primaryType": "nt:unstructured",     "test:Page": {       "jcr:primaryType": "nt:unstructured",       "include0": {         "jcr:primaryType": "nt:unstructured",         "relativeNode": true,         "path": "jcr:content"       }     }   },   "indexRules": {     "jcr:primaryType": "nt:unstructured",     "test:Page": {       "jcr:primaryType": "nt:unstructured",       "properties": {         "jcr:primaryType": "nt:unstructured",         "jcr:lastModified": {           "jcr:primaryType": "nt:unstructured",           "ordered": true,           "propertyIndex": true,           "name": "jcr:content/jcr:lastModified",           "type": "Date"         }       }     }   } } {code}$$patch1-oak-920f32d0_Developer_PatchNaturalnessYe$$Fixed bug in aggregate. Fix issue with aggregate method. $$1
oak-ab1a0cc2$$EmptyNodeState.equals() broken$$EmptyNodeState.equals() returns incorrect results when the other node state is not of type EmptyNodeState and the two states have differing exists() flags.$$patch1-oak-ab1a0cc2_Developer_PatchNaturalnessYe$$added exists check. $$1
oak-06812d25$$Boosting fields not working as expected$$When the boost support was added the intention was to support a usecase like   {quote} For the fulltext search on a node where the fulltext content is derived from multiple field it should be possible to boost specific text contributed by individual field. Meaning that if a title field is boosted more than description, the title (part) in the fulltext field will mean more than the description (part) in the fulltext field. {quote}  This would enable a user to perform a search like _/jcr:root/content/geometrixx-outdoors/en//element(*, cq:Page)\[jcr:contains(., 'Keyword')\]_ and get a result where pages having 'Keyword' in title come above in search result compared to those where Keyword is found in description.  Current implementation just sets the boost while add the field value to fulltext field with the intention that Lucene would use the boost as explained above. However it does not work like that and boost value gets multiplies with other field and hence boosting does not work as expected$$patch1-oak-06812d25_Developer_PatchNaturalnessYe$$don ' t include properties with boost. $$1
oak-e30023ba$$Oak Lucene index doesn't get notified about updates when index is stored on the file system$$It looks like the the lucene IndexTracked class responsible for refreshing the in-memory cache of the lucene index doesn't get the update notification when the index is stored on the file system. This results in searches not working until the next restart$$patch1-oak-e30023ba_Developer_PatchNaturalnessYe$$added missing import. added missing import. add OAK - 2029 Record the last updated status. $$1
oak-be44b816$$QueryManager does not have autorefresh$$Having two sessions A and B. A writes something for example /content/page/text = "text" Accessing B's QueryManager and exexcute a query for "text" nothing will be found. Triggering an explicit refresh on B before the query and the hit is found.  I assume that the autorefresh is missed for that case$$patch1-oak-be44b816_Developer_PatchNaturalnessYe$$update imports. don ' t perform query on session context , because it is not supported by our version of j. $$1
oak-e39b4d96$$Full-text search on the traversing index fails if the condition contains a slash$$A full-text search on the traversing index falls back to a sort of manual evaluation of results.  This is handled by the _FullTextTerm_ class, and it appears that it passes the constraint text through a cleanup process where it strips most of the characters that are neither _Character.isLetterOrDigit(c)_ not in the list _+-:&_  I'm not exactly sure where this list comes from, but I see the '/' character is missing which causes a certain type of query to fail.  Example: {code} //*[jcr:contains(., 'text/plain')] {code}$$patch1-oak-e39b4d96_Developer_PatchNaturalnessYe$$"fix false positive in "" FullTextTerm "" pattern". $$1
oak-c83755c3$$NPE during syncAllExternalUsers in LdapIdentityProvider.createUser$$When executing the JMX method syncAllExternalUsers the following NPE has been encountered. This likely indicates that - for a particular user - there is no attribute '{{uid}}':  {code} java.lang.NullPointerException at org.apache.jackrabbit.oak.security.authentication.ldap.impl.LdapIdentityProvider.createUser(LdapIdentityProvider.java:667) at org.apache.jackrabbit.oak.security.authentication.ldap.impl.LdapIdentityProvider.access 000(LdapIdentityProvider.java:88) at org.apache.jackrabbit.oak.security.authentication.ldap.impl.LdapIdentityProvider 1.getNext(LdapIdentityProvider.java:281) at org.apache.jackrabbit.oak.security.authentication.ldap.impl.LdapIdentityProvider 1.getNext(LdapIdentityProvider.java:273) at org.apache.jackrabbit.commons.iterator.AbstractLazyIterator.hasNext(AbstractLazyIterator.java:39) at org.apache.jackrabbit.oak.spi.security.authentication.external.impl.jmx.SyncMBeanImpl Delegatee.syncAllExternalUsers(SyncMBeanImpl.java:245) at org.apache.jackrabbit.oak.spi.security.authentication.external.impl.jmx.SyncMBeanImpl.syncAllExternalUsers(SyncMBeanImpl.java:426) {code}$$patch1-oak-c83755c3_Developer_PatchNaturalnessYe$$don ' t throw exception if id attribute is not found. Fixed LDAP issue. $$1
oak-eabb4066$$SQL2 query with union, limit and offset can return invalid results$$when using order, limit and offset and a SQL2 query that contains an union of two subqueries that have common results can return invalid results  Example: assuming content tree /test/a/b/c/d/e exists {code:sql} SELECT [jcr:path] FROM [nt:base] AS a WHERE ISDESCENDANTNODE(a, '/test') UNION SELECT [jcr:path] FROM [nt:base] AS a WHERE ISDESCENDANTNODE(a, '/test')" ORDER BY [jcr:path] {code} with limit=3 and offset 2 returns only one row ( instead of 3 )  the correct result set is {noformat} /test/a/b/c /test/a/b/c/d /test/a/b/c/d/e {noformat}$$patch1-oak-eabb4066_Developer_PatchNaturalnessYe$$reduce limit on union query. $$1
oak-016df669$$NodeDocument _modified may go back in time$$In a cluster with multiple DocumentMK instances the _modified field of a NodeDocument may go back in time. This will result in incorrect diff calculations when the DocumentNodeStore uses the _modified field to find changed nodes for a given revision range.$$patch1-oak-016df669_Developer_PatchNaturalnessYe$$Fix update update op. "Remove map entry operation with type = "" REMOVE_MAP_ENTRY "" ; don '. update update op with max ( String , Object ). Fix error in updateOp. "remove unnecessary "" type = "" in UpdateOp constructor". Added update operation for Long value update .. Add constructor to UpdateOp. Fix update op implementation. add UpdateUtils . applyChanges ( ). Added MAX op to UpdateUtils. Fixing # 12508. Fixed MongoDB operation type. added maxUpdates = new BasicDBObject ( ) ;. Fixed case. added max / max updates to update log. $$1
oak-52ca008c$$SplitOperations may not retain most recent committed _commitRoot entry$$In some rare cases it may happen that SplitOperations does not retain the most recent committed _commitRoot entry on a document. This may result in an undetected hierarchy conflict.$$patch1-oak-52ca008c_Developer_PatchNaturalnessYe$$"OAK - 4050 added "" mostRecent = false """. $$1
oak-557eec4f$$o.a.j.o.spi.query.Filter exposes unexported class o.a.j.o.query.ast.SelectorImpl$$The interface {{o.a.j.o.spi.query.Filter}} uses in its public API the class {{o.a.j.o.query.ast.SelectorImpl}}, but while the former is contained in an exported package, the latter is not.$$patch1-oak-557eec4f_Developer_PatchNaturalnessYe$$"Revert "" update to latest plugin """. Remove unnecessary code. Fix an issue with property index search where filter has only empty property restrictions. Remove @ Override from FilterImpl # getSelector. "Revert "" update to latest opengl """. "Revert "" update to property index plan """. "Revert "" update PropertyIndexPlan constructor """. "Revert "" update to 1 . 0 . 1 """. Fixing the build .. $$1
oak-6c54045d$$Access to disconnected MemoryNodeBuilder should throw IllegalStateException$$None$$patch1-oak-6c54045d_Developer_PatchNaturalnessYe$$Improved method to return boolean values for ` exists ` method in MemoryNodeBuilder. "Revert "" updateReadState ( ) "" code for MemoryNodeBuilder """. Fix a warning. $$1
oak-977a31d8$$Error while configuring analyzer by composition$$Error while creating analyzer by composition from osgi due to an illegal argument {{jcr:primaryType}} passed to {{TokenizerFactory.forName(clazz, args)}} in {{NodeStateAnalyzerFactory.loadTokenizer()}}  {noformat} Caused by: java.lang.IllegalArgumentException: Unknown parameters: {jcr:primaryType=nt:unstructured} 	at org.apache.lucene.analysis.core.LowerCaseFilterFactory.<init>(LowerCaseFilterFactory.java:45) {noformat}$$patch1-oak-977a31d8_Developer_PatchNaturalnessYe$$added missing import. added JCR props to ignore list. $$1
oak-f1ba7a42$$:childOrder out of sync when node is made orderable concurrently$$The ChildOrderConflictHandler does not merge the :childOrder when an addExistingProperty conflict occurs.$$patch1-oak-f1ba7a42_Developer_PatchNaturalnessYe$$Fixed child order conflicts. Fixed issue with merging of strings and names in the same order as the other one. $$1
oak-b481a14c$$CacheLIRS concurrency issue$$Some of the methods of the cache can throw a NullPointerException when the cache is used concurrently. Example stack trace:  {code} java.lang.NullPointerException: null org.apache.jackrabbit.oak.cache.CacheLIRS.values(CacheLIRS.java:470)  org.apache.jackrabbit.oak.cache.CacheLIRS 1.values(CacheLIRS.java:1432) org.apache.jackrabbit.oak.plugins.segment.file.FileStore.flush(FileStore.java:205) {code}$$patch1-oak-b481a14c_Developer_PatchNaturalnessYe$$"Revert "" update to latest cache """. "Revert "" update CacheLIRS . mask """. "Revert "" update CacheLIRS to cache """. Fix some compiler warnings. avoid NPE on initial refresh , because there is no loader in CacheLIRS. Fix CacheLIRS .. Fix NPE in CacheLIRS .. "Revert "" update CacheLIRS . find ( key , hash ) """. $$1
oak-69b68890$$TokenLoginModule does not set userId on auth info$$the token login module does not set the userid in the authinfo (because it does not know it). and the LoginModuleImpl does not overwrite the AuthInfo if it already exists.  the consequence: {{Session.getUserID()}} returns {{NULL}} for logins that create a token.  I think that the authinfos should be added even if they already exist. and all users of the public credentials need to be aware that authinfos can exist that are not complete.$$patch1-oak-69b68890_Developer_PatchNaturalnessYe$$"Revert "" update to latest commit """. Add shared state entry. Fixed # 1408. put attributes in shared state. Fix error in token login module. Oups , really correct the user / pass in the token login module. Fix warning. Oops , really add auth info in read only subject. Fix bug introduced in previous commit that caused # 1796. $$1
oak-9d36bede$$Stale cache after MongoMK GC$$After a MongoMK revision GC the docChildrenCache may be stale and lead to a NPE when reading children with deleted and GC'ed siblings.$$patch1-oak-9d36bede_Developer_PatchNaturalnessYe$$invalidate doc children cache on remove. invalidate doc children cache on write. $$1
oak-ecc5bdfd$$Full-text search on the traversing index fails if the condition contains a slash$$A full-text search on the traversing index falls back to a sort of manual evaluation of results.  This is handled by the _FullTextTerm_ class, and it appears that it passes the constraint text through a cleanup process where it strips most of the characters that are neither _Character.isLetterOrDigit(c)_ not in the list _+-:&_  I'm not exactly sure where this list comes from, but I see the '/' character is missing which causes a certain type of query to fail.  Example: {code} //*[jcr:contains(., 'text/plain')] {code}$$patch1-oak-ecc5bdfd_Developer_PatchNaturalnessYe$$don ' t append properties to ' buff ' unless the path is a root path. $$1
oak-94c6c575$$Branch reset does not revert all changes$$This is caused by recent changes done for OAK-3646.$$patch1-oak-94c6c575_Developer_PatchNaturalnessYe$$reset diff after adding node. added missing import. "Revert "" update to latest document node store """. remove trivial copy / paste error. don ' t undo changes if they are not in the same branch. remove ancestor revision debug flag. "Revert "" update to latest document node "" error in DocumentNodeStoreBranch """. $$1
oak-4ed7bc8e$$Inconsistency in Node#setProperty in case of null value$$Setting a null value to a single valued property will result in 'null' being returned while executing the same on a multivalued property will return the removed property.  jr2 returned the removed property in both cases as far as i  remember and i would suggest that we don't change that behavior. in particular since the specification IMO doesn't allow to return null-values for these methods.$$patch1-oak-4ed7bc8e_Developer_PatchNaturalnessYe$$Fix failing tests. $$1
oak-428e32c6$$Query: unexpected result on negative limit / offset$$Currently, running a query with limit of -1 never returns any rows, the same as when using limit = 0.  Either the query engine should fail with a negative limit or offset (IllegalArgumentException), or it should ignore negative values (unlimited result rows for limit, probably no offset for offset = -1).  I would prefer IllegalArgumentException, but I can also live with -1 = unlimited, at least for "limit".$$patch1-oak-428e32c6_Developer_PatchNaturalnessYe$$allow negative limits on query engine. XPathQueryBuilder should set maxCount to Long . MAX_VALUE. update QueryEngine. $$1
oak-ca05fd06$$XPath: querying for nodes named "text", "element", and "rep:excerpt" fails$$Queries that contain "text" or "element" as a node name currently fail, because the the parser assumes "text()" / "element(...)". Example query that fails:  {noformat} /jcr:root/content/text/jcr:content//element(*,nt:unstructured) {noformat}  A workaround is to use the escape mechanism, that is:  {noformat} /jcr:root/tmp/_x0074_ext/jcr:content//element(*,nt:unstructured) {noformat}  It looks like '(' and ')' are valid characters in node names, but to query for those characters, they need to be escaped.$$patch1-oak-ca05fd06_Developer_PatchNaturalnessYe$$fixing XPathToSQL2Converter warning. Allowing more freedom to be opened dot - closed. Allowing more custom schema to be added to the statement. Allow variable name. Allow method to be called with @ param readOpenDotClose. $$1
oak-5138a1e2$$Test failure: CompactionMapTest.removeSome$$Said test fails sporadically:  {noformat} at org.junit.Assert.assertNull(Assert.java:562) at org.apache.jackrabbit.oak.plugins.segment.CompactionMapTest.removeSome(CompactionMapTest.java:156) {noformat}  This is a regression introduced with OAK-3501: the {{recent}} map gets not cleared when {{segmentIdMap}} is empty. This can happen when a recent key is removed again while there are no other changes.$$patch1-oak-5138a1e2_Developer_PatchNaturalnessYe$$Fixed # 1796. $$1
oak-07646fba$$Upgraded version history has UUIDs as jcr:frozenUuid of non-referenceable nodes$$In Jackrabbit Classic each node, even non-referenceable ones, has a UUID as its identifier, and thus the {{jcr:frozenUuid}} properties of frozen nodes are always UUIDs. In contrast Oak uses path identifiers for non-referenceable frozen nodes (see OAK-1009), which presents a problem when dealing with version histories migrated from Jackrabbit Classic.  To avoid this mismatch, the upgrade code should check each frozen node for referenceability and replace the frozen UUID with a path identifier if needed.$$patch1-oak-07646fba_Developer_PatchNaturalnessYe$$added parentFrozenUuid if it is not null. Fixing the build .. Added getTypeEditorProvider ( false ) . getRootEditor ( ) .. $$1
oak-9225a3e2$$UnmergedBranch state growing with empty BranchCommit leading to performance degradation$$In some cluster deployment cases it has been seen that in memory state of UnmergedBranches contains large number of empty commits. For e.g. in  one of of the runs there were 750 entries in the UnmergedBranches and each Branch had empty branch commits.  If there are large number of UnmergedBranches then read performance would degrade as for determining revision validity currently logic scans all branches  Below is some part of UnmergedBranch state  {noformat} Branch 1 1 -> br146d2edb7a7-0-1 (true) (revision: "br146d2edb7a7-0-1", clusterId: 1, time: "2014-06-25 05:08:52.903", branch: true) 2 -> br146d2f0450b-0-1 (true) (revision: "br146d2f0450b-0-1", clusterId: 1, time: "2014-06-25 05:11:40.171", branch: true) Branch 2 1 -> br146d2ef1d08-0-1 (true) (revision: "br146d2ef1d08-0-1", clusterId: 1, time: "2014-06-25 05:10:24.392", branch: true) Branch 3 1 -> br146d2ed26ca-0-1 (true) (revision: "br146d2ed26ca-0-1", clusterId: 1, time: "2014-06-25 05:08:15.818", branch: true) 2 -> br146d2edfd0e-0-1 (true) (revision: "br146d2edfd0e-0-1", clusterId: 1, time: "2014-06-25 05:09:10.670", branch: true) Branch 4 1 -> br146d2ecd85b-0-1 (true) (revision: "br146d2ecd85b-0-1", clusterId: 1, time: "2014-06-25 05:07:55.739", branch: true) Branch 5 1 -> br146d2ec21a0-0-1 (true) (revision: "br146d2ec21a0-0-1", clusterId: 1, time: "2014-06-25 05:07:08.960", branch: true) 2 -> br146d2ec8eca-0-1 (true) (revision: "br146d2ec8eca-0-1", clusterId: 1, time: "2014-06-25 05:07:36.906", branch: true) Branch 6 1 -> br146d2eaf159-1-1 (true) (revision: "br146d2eaf159-1-1", clusterId: 1, time: "2014-06-25 05:05:51.065", counter: 1, branch: true) Branch 7 1 -> br146d2e9a513-0-1 (true) (revision: "br146d2e9a513-0-1", clusterId: 1, time: "2014-06-25 05:04:26.003", branch: true) {noformat}  [~mreutegg] Suggested that these branch might be for those revision which have resulted in a collision and upon checking it indeed appears to be the case  (value true in brackets above indicate that). Further given the age of such revision it looks like they get populated upon startup itself  *Fix* * Need to check why we need to populate the UnermgedBranch * Possibly implement some purge job which would remove such stale entries$$patch1-oak-9225a3e2_Developer_PatchNaturalnessYe$$removed unused import. added missing import. added private final logger. added log output. do not update revisions in snapshot. $$1
oak-61c877d8$$NPE if checking for a non-existing node in version storage$$NPE If a tree given to CompiledPermissionImpl.getTreePermission() does not have a primary type, e.g. for a "hidden" oak node:  {noformat} 	  at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:191) 	  at org.apache.jackrabbit.oak.security.authorization.permission.CompiledPermissionImpl.getTreePermission(CompiledPermissionImpl.java:160) 	  at org.apache.jackrabbit.oak.security.authorization.permission.CompiledPermissionImpl TreePermissionImpl.getChildPermission(CompiledPermissionImpl.java:443) 	  at org.apache.jackrabbit.oak.core.SecureNodeBuilder.getTreePermission(SecureNodeBuilder.java:352) 	  at org.apache.jackrabbit.oak.core.SecureNodeBuilder.exists(SecureNodeBuilder.java:129) 	  at org.apache.jackrabbit.oak.core.SecureNodeBuilder.hasChildNode(SecureNodeBuilder.java:271) 	  at org.apache.jackrabbit.oak.core.AbstractTree.getChildrenCount(AbstractTree.java:248) {noformat}  The tree passed here to get the children count is: {{/jcr:system/jcr:versionStorage}} and the child node not having a primary type is {{:index}}$$patch1-oak-61c877d8_Developer_PatchNaturalnessYe$$Fixed NPE in TreeUtil . getPrimaryTypeName. $$1
oak-1d08cbd3$$DocumentNodeStore.diffManyChildren() reads too many nodes$$DocumentNodeStore.diffManyChildren() compares too many nodes when running in a non-clustered setup and there are many changes below a location with 'many' children.  This is a regression introduced by OAK-2232. The fix changed the way how the minimum revision is calculated based on the two revisions to compare. The seen-at revision of the RevisionComparator is taken into account. However, in a single cluster node setup, the revision range for the current clusterId is never updated. This means the minimum revision is calculated too far back and causes queries with too many nodes than necessary.$$patch1-oak-1d08cbd3_Developer_PatchNaturalnessYe$$workflow adjustments - added missing for ( currentClusterNodeId ) check. $$1
oak-61381ea2$$SQL-2 query parser doesn't detect some illegal statements$$The SQL-2 query parser doesn't detect some illegal statements, for example  {code} select * from [nt:base] where name =+ 'Hello' select * from [nt:base] where name => 'Hello' {code}  Both are currently interpreted as "name = 'Hello'", which is wrong.$$patch1-oak-61381ea2_Developer_PatchNaturalnessYe$$Fixed error in SQL2Parser. "Revert "" fall through to SQL2Parser . parseKeyword ( ) """. $$1
oak-c13708e3$$[RDB] Updated blob still deleted even if deletion interval lower$$If an existing blob is uploaded again, the timestamp of the existing entry is updated in the meta table. Subsequently if a call to delete (RDBBlobStore#countDeleteChunks) is made with {{maxLastModifiedTime}} parameter of less than the updated time above, the entry in the meta table is not touched but the data table entry is wiped out.   Refer https://github.com/apache/jackrabbit-oak/blob/trunk/oak-core/src/main/java/org/apache/jackrabbit/oak/plugins/document/rdb/RDBBlobStore.java#L510$$patch1-oak-c13708e3_Developer_PatchNaturalnessYe$$"Revert "" update RDBBlobStore with last modified < ? """. added log message for chunk delete. $$1
oak-808ac9c0$$Query: use "union" for complex XPath queries that use multiple "or"$$The following XPath query is converted to a union, however there is still an "or" in the converted query, which means the query still can't use all indexes and has to traverse the whole repository:  {noformat} /jcr:root/a/b//element(*, nt:unstructured)[( (@sling:resourceType = 'x'  or @sling:resourceType = 'dam/collection')  or @sling:resourceSuperType = 'dam/collection')]  {noformat}$$patch1-oak-808ac9c0_Developer_PatchNaturalnessYe$$added code to improve performance of union statements. fixed typo in ExpressionUtil . pullOrRight ( ). added code duplication. use Expression . AndCondition instead of Expression . Condition. fixed Expression . pullOrRight ( ). fixed and condition pullOrRight ( ). $$1
oak-2426deae$$Async index update persists conflict markers$$A long running test I performed yesterday failed with a FileNotFoundException in the lucene index. After analyzing the issue it turned  out the async index update persisted a conflict markers introduced by a rebase call. So far I'm not able to reproduce it with a more simple test setup and after a shorter time (the initial test failed after 10 hours). Given the way the async index update work, there shouldn't be any conflicts, because it's the only component writing into this location of the repository.   As an immediate workaround, I'd like to add the AnnotatingConflictHandler & ConflictValidator combo to the merge call to make sure a commit with conflict markers does not get persisted.$$patch1-oak-2426deae_Developer_PatchNaturalnessYe$$Missing imports. add missing commit hooks. Missing semicolon. $$1
oak-b0014b7d$$IndexOutOfBoundsException in FileStore.writeStream$$When writing streams of specific length I get  {code} java.lang.IndexOutOfBoundsException at java.nio.Buffer.checkIndex(Buffer.java:538) at java.nio.HeapByteBuffer.getInt(HeapByteBuffer.java:359) at org.apache.jackrabbit.oak.segment.Segment.getGcGen(Segment.java:318) at org.apache.jackrabbit.oak.segment.file.FileStore.writeSegment(FileStore.java:1371) at org.apache.jackrabbit.oak.segment.SegmentWriter SegmentWriteOperation.internalWriteStream(SegmentWriter.java:661) {code}$$patch1-oak-b0014b7d_Developer_PatchNaturalnessYe$$"Revert "" update to latest oak segment """. added missing import. Added getter for getGcGen. added uuid to tar writer. added UUID support for writeSegment. $$1
oak-4faf31e3$$Lucene index / compatVersion 2: search for 'abc!' does not work$$When using a Lucene fulltext index with compatVersion 2, then the following query does not return any results. When using compatVersion 1, the correct result is returned.  {noformat} SELECT * FROM [nt:unstructured] AS c  WHERE CONTAINS(c.[jcr:description], 'abc!')  AND ISDESCENDANTNODE(c, '/content') {noformat}  With compatVersion 1 and 2, searching for just 'abc' works. Also, searching with '=' instead of 'contains' works.$$patch1-oak-4faf31e3_Developer_PatchNaturalnessYe$$removed unused import. added LucenePropertyIndex # LUCENE_QUERY_OPERATORS. remove todo. $$1
oak-f4d5bbe1$$Incorrect recovery of _lastRev for branch commit$$The recovery process for _lastRevs is incorrect for branch commits. It propagates the revision of the commit to the branch up to the root node instead of the revision of the merge for the changes.$$patch1-oak-f4d5bbe1_Developer_PatchNaturalnessYe$$always use getCommitRevision ( ) instead of getCc ( ) s. always return the same revision. $$1
oak-717186d6$$Moving larger trees cause OutOfMemoryError$${{LargeMoveTest.moveTest}} test runs out of heap space when moving roughly 100000 nodes (128M heap):  {code} java.lang.OutOfMemoryError: Java heap space 	at java.util.Arrays.copyOf(Arrays.java:2786) 	at java.lang.StringCoding.safeTrim(StringCoding.java:64) 	at java.lang.StringCoding.access 300(StringCoding.java:34) 	at java.lang.StringCoding StringEncoder.encode(StringCoding.java:251) 	at java.lang.StringCoding.encode(StringCoding.java:272) 	at java.lang.String.getBytes(String.java:946) 	at org.apache.jackrabbit.mk.util.IOUtils.writeString(IOUtils.java:84) 	at org.apache.jackrabbit.mk.store.BinaryBinding.writeMap(BinaryBinding.java:98) 	at org.apache.jackrabbit.mk.model.ChildNodeEntriesMap.serialize(ChildNodeEntriesMap.java:196) 	at org.apache.jackrabbit.mk.model.AbstractNode.serialize(AbstractNode.java:169) 	at org.apache.jackrabbit.mk.persistence.InMemPersistence.writeNode(InMemPersistence.java:76) 	at org.apache.jackrabbit.mk.store.DefaultRevisionStore.putNode(DefaultRevisionStore.java:276) 	at org.apache.jackrabbit.mk.model.StagedNodeTree StagedNode.persist(StagedNodeTree.java:568) 	at org.apache.jackrabbit.mk.model.StagedNodeTree StagedNode.persist(StagedNodeTree.java:563) 	at org.apache.jackrabbit.mk.model.StagedNodeTree StagedNode.persist(StagedNodeTree.java:563) 	at org.apache.jackrabbit.mk.model.StagedNodeTree StagedNode.persist(StagedNodeTree.java:563) 	at org.apache.jackrabbit.mk.model.StagedNodeTree StagedNode.persist(StagedNodeTree.java:563) 	at org.apache.jackrabbit.mk.model.StagedNodeTree StagedNode.persist(StagedNodeTree.java:563) 	at org.apache.jackrabbit.mk.model.StagedNodeTree StagedNode.persist(StagedNodeTree.java:563) 	at org.apache.jackrabbit.mk.model.StagedNodeTree.persist(StagedNodeTree.java:80) 	at org.apache.jackrabbit.mk.model.CommitBuilder.doCommit(CommitBuilder.java:126) 	at org.apache.jackrabbit.mk.model.CommitBuilder.doCommit(CommitBuilder.java:94) 	at org.apache.jackrabbit.mk.core.MicroKernelImpl.commit(MicroKernelImpl.java:496) 	at org.apache.jackrabbit.oak.kernel.KernelNodeStoreBranch.commit(KernelNodeStoreBranch.java:178) 	at org.apache.jackrabbit.oak.kernel.KernelNodeStoreBranch.setRoot(KernelNodeStoreBranch.java:78) 	at org.apache.jackrabbit.oak.core.RootImpl.purgePendingChanges(RootImpl.java:355) 	at org.apache.jackrabbit.oak.core.RootImpl.commit(RootImpl.java:234) 	at org.apache.jackrabbit.oak.core.LargeMoveTest.moveTest(LargeMoveTest.java:78) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) {code}  This is caused by the inefficient rebase implementation in oak-core as discussed at length in OAK-464.$$patch1-oak-717186d6_Developer_PatchNaturalnessYe$$added json output. added depth for object added by # 210. added depth for property. added expanded json serialization. $$1
oak-039f892d$$int overflow with orderby causing huge slowdown$$Consider the following query: {code} //element(*,slingevent:Job) order by @slingevent:created ascending {code} this query - when running with a large number of slingevent:Job around - will take a very long time due to the fact, that FilterIterators.SortIterator.init() in the following loop: {code} if (list.size() > max * 2) {   // remove tail entries right now, to save memory   Collections.sort(list, orderBy);   keepFirst(list, max); } {code} does a multiplication with 'max', which is by default set to Integer.MAX_VALUE (see FilterIterators.newCombinedFilter). This results in max *2 to overflow (result is -2) - thus that init-loop will sort the list for every additional entry. Which is definitely not the intention.$$patch1-oak-039f892d_Developer_PatchNaturalnessYe$$Fix optimisation. $$1
oak-c3773d53$$Missing commit hooks in upgrade$$There's a TODO in the RepositoryUpgrade class about missing commit hooks. For example the PermissionHook isn't currently run as a part of the upgrade, which breaks permission evaluation even though the actual ACL nodes are present after the upgrade.$$patch1-oak-c3773d53_Developer_PatchNaturalnessYe$$removed unused import. "Revert "" update to latest oak upgrade plugin """. removed unused import. remove workspaceName. Added missing commit hook. added missing method. Add TODO in RepositoryUpgrade. $$1
oak-dde7de85$$Commit.rollback() may remove changes from other commit$$Commit.rollback() removes documents it previously created. With concurrent commits it may happen that this method removes documents some other commit modified in the meantime.$$patch1-oak-dde7de85_Developer_PatchNaturalnessYe$$NPE - 772 added missing reverse revision property. $$1
oak-0be7e8f0$$Tree has wrong parent after move$$After a move operation Tree.getParent() still returns the old parent.  {code} Tree x = r.getChild("x"); Tree y = r.getChild("y");  root.move("x", "y/x"); assertEquals("y", x.getParent().getName());  // Fails {code}$$patch1-oak-0be7e8f0_Developer_PatchNaturalnessYe$$"Revert "" update to latest oak plugin """. added missing imports. Improved constructor access to TreeImpl . children. Improved TreeImpl . getPath ( ). added getChildCount ( ) to TreeImpl # getChild. added iterator on ChildNodeEntries. added support for child modification. added missing call to getBuilder ( ) . setProperty ( name , values ). Fix oak - core remove property method. added code to move a node to the right builder. Fix copy ( ). Fixed # 771. added getNodeStateBuilder. added path separator .. added missing copy of kernel node state builder. added missing backslash. Fix node state builder implementation. added path separator. Fix moveNode ( ) method where the destName was not the same as the srcName and. added missing backslash. added missing method buildPath. $$1
oak-b62f1c26$$Wrong results and NPE with copy operation$$The following code either results in an NPE or in a wrong result depending on which Microkernel instance is used.   {code}     mk.commit("", "+\"/root\":{}", mk.getHeadRevision(), "");     mk.commit("", "+\"/root/N0\":{}*\"/root/N0\":\"/root/N1\"+\"/root/N0/N4\":{}",             mk.getHeadRevision(), ""); {code}  The wrong result is  {code} {     ":childNodeCount": 2,     "N0": {         ":childNodeCount": 1,         "N4": {             ":childNodeCount": 0         }     },     "N1": {         ":childNodeCount": 1,         "N4": {             ":childNodeCount": 0         }     } } {code}  The expected result is {code} {     ":childNodeCount": 2,     "N0": {         ":childNodeCount": 1,         "N4": {             ":childNodeCount": 0         }     },     "N1": {         ":childNodeCount": 0     } } {code}  simple:fs:target/temp: wrong result fs:{homeDir}/target: NPE http-bridge:fs:{homeDir}/target: NPE simple: wrong result$$patch1-oak-b62f1c26_Developer_PatchNaturalnessYe$$update indexer with root path. added missing backslash. could handle move / copy node with /. add TODO. enable indexing of empty files. "Indexing "" moveNode "" method with @ CopyNode """. added simple remove recursive call for now. added TODO. added toString ( ) to SimpleKernelImpl. $$1
oak-59a83d23$$Non-root lucene index throws exception if query constraints match root of sub-tree$$LucenePropetyIndexProvider returns incorrect (normalized) path for root of sub-tree if index is defined on the sub-tree. e.g. {{/jcr:root/test//element(*, nt:base)\[@foo='bar']}} would fail with following defn {noformat} + /test     - foo="bar"     + test1           - foo="bar"     + oak:index            - indexRules/nt:base/properties/foo/propertyIndex="true" {noformat}$$patch1-oak-59a83d23_Developer_PatchNaturalnessYe$$Fixed lucene logic. $$1
oak-4d231938$$TreeTypeProvider returns wrong type for version related node type definitions$$the following paths with result in type {{VERSION}} instead of {{DEFAULT}} and might lead to unexpected results wrt read access:  - /jcr:system/jcr:nodeTypes/rep:system/rep:namedChildNodeDefinitions/jcr:versionStorage - /jcr:system/jcr:nodeTypes/rep:system/rep:namedChildNodeDefinitions/jcr:activities - /jcr:system/jcr:nodeTypes/rep:system/rep:namedChildNodeDefinitions/jcr:configurations$$patch1-oak-4d231938_Developer_PatchNaturalnessYe$$"Revert "" update to latest commit """. Improve doc string .. Fixed a bug where TreeTypeProvider . getType ( ) returns a non - default type. $$1
oak-fdc54465$$Creating multiple checkpoint on same head revision overwrites previous entries$$Currently when a checkpoint is created in DocumentNodeStore then it is saved in form of currentHeadRev=>expiryTime. Now if multiple checkpoints are created where head revision has not changed then only the last one would be saved and previous entries would be overridden as revision is used as key  One fix would be to change the expiry time only if the new expiry time is greater than previous entry. However doing that safely in a cluster (check then save) is currently not possible with DocumentStore API as the modCount check if only supported for Nodes.$$patch1-oak-fdc54465_Developer_PatchNaturalnessYe$$added missing import. added missing commit callback. $$1
oak-93c1aa40$$AsyncIndexUpdate unable to cope with missing checkpoint ref$$The async index uses a checkpoint reference stored under the _:async_ hidden node as a base for running the index diff. It might happen that this reference is stale (pointing to checkpoints that no longer exist) so the async indexer logs a warning that it will reindex everything and will start its work. The trouble is with the #mergeWithConcurrencyCheck which does not cope well with this scenario. Even if the ref checkpoint is null, it will throw a concurrent update exception which will be logged as a misleading debug log _Concurrent update detected in the async index update_.  Overall the code looks stuck in an endless reindexing loop.  {code} *WARN* [pool-9-thread-1] org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate Failed to retrieve previously indexed checkpoint 569d8847-ebb6-4832-a55f-2b0b1a32ae71; re-running the initial async index update *DEBUG* [pool-9-thread-1] org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate Concurrent update detected in the async index update {code}$$patch1-oak-93c1aa40_Developer_PatchNaturalnessYe$$getAsyncIndexUpdate ( ) throws NPE. $$1
oak-716e1237$$Hourly async reindexing on an idle instance$$OAK-1292 introduced the following interesting but not very nice behavior:  On an idle system with no changes for an extended amount of time, the OAK-1292 change blocks the async indexer from updating the reference to the last indexed checkpoint. After one hour (the default checkpoint lifetime), the referenced checkpoint will expire, and the indexer will fall back to full reindexing.  The result of this behavior is that once every hour, the size of an idle instance will grow with dozens or hundreds of megabytes of new index data generated by reindexing. Older index data becomes garbage, but the compaction code from OAK-1804 is needed to make it collectable. A better solution would be to prevent the reindexing from happening in the first place.$$patch1-oak-716e1237_Developer_PatchNaturalnessYe$$Fix IndexUpdate .. Remove unused import. Remove unused import. Fix typo. Fix a constructor. Fix a check for concurrent updates. Fixing a warning. Fix merge issue. $$1
oak-72d24f4b$$IndexCopier might create empty files in case of error occuring while copying$$On some of the setups following logs are seen {noformat} error.log:12.03.2015 03:53:59.785 *WARN* [pool-5-thread-90] org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopier Found local copy for _2uv.cfs in MMapDirectory@/mnt/installation/crx-quickstart/repository/index/e5a943cdec3000bd8ce54924fd2070ab5d1d35b9ecf530963a3583d43bf28293/1 lockFactory=NativeFSLockFactory@/mnt/installation/crx-quickstart/repository/index/e5a943cdec3000bd8ce54924fd2070ab5d1d35b9ecf530963a3583d43bf28293/1 but size of local 0 differs from remote 1070972. Content would be read from remote file only error.log:12.03.2015 03:54:02.883 *WARN* [pool-5-thread-125] org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopier Found local copy for _2rr.si in MMapDirectory@/mnt/installation/crx-quickstart/repository/index/43b36b107f8ce7e162c15b22508aa457ff6ae0083ed3e12d14a7dab67f886def/1 lockFactory=NativeFSLockFactory@/mnt/installation/crx-quickstart/repository/index/43b36b107f8ce7e162c15b22508aa457ff6ae0083ed3e12d14a7dab67f886def/1 but size of local 0 differs from remote 240. Content would be read from remote file only error.log:12.03.2015 03:54:03.467 *WARN* [pool-5-thread-132] org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopier Found local copy for _2ro_3.del in MMapDirectory@/mnt/installation/crx-quickstart/repository/index/43b36b107f8ce7e162c15b22508aa457ff6ae0083ed3e12d14a7dab67f886def/1 lockFactory=NativeFSLockFactory@/mnt/installation/crx-quickstart/repository/index/43b36b107f8ce7e162c15b22508aa457ff6ae0083ed3e12d14a7dab67f886def/1 but size of local 0 differs from remote 42. Content would be read from remote file only error.log:12.03.2015 03:54:03.737 *WARN* [pool-5-thread-135] org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopier Found local copy for _2rm_2.del in MMapDirectory@/mnt/installation/crx-quickstart/repository/index/43b36b107f8ce7e162c15b22508aa457ff6ae0083ed3e12d14a7dab67f886def/1 lockFactory=NativeFSLockFactory@/mnt/installation/crx-quickstart/repository/index/43b36b107f8ce7e162c15b22508aa457ff6ae0083ed3e12d14a7dab67f886def/1 but size of local 0 differs from remote 35. Content would be read from remote file only {noformat}  They indicate that copier has created files of size 0. Looking at the code flow this can happen in case while starting copying some error occurs in between. {{org.apache.lucene.store.Directory#copy}} do take care of removing the file in case of error but that is done only for IOException and not for other cases.  As a fix the logic should ensure that local file gets deleted if the copy was not successful$$patch1-oak-72d24f4b_Developer_PatchNaturalnessYe$$added variable to track if the local index file already exists. removed corrupted files on upload. $$1
oak-ec961a38$$IllegalStateException in MemoryNodeBuilder$${{AuthorizablePropertyTest.testSetPropertyByRelPath()}} sometimes causes an IllegalStateException in {{MemoryNodeBuilder}}. This might be a problem with the latter uncovered by the recent switch to the p2 index mechanism (OAK-511).  {code} java.lang.IllegalStateException     at com.google.common.base.Preconditions.checkState(Preconditions.java:133)     at org.apache.jackrabbit.oak.plugins.memory.MemoryNodeBuilder.read(MemoryNodeBuilder.java:205)     at org.apache.jackrabbit.oak.plugins.memory.MemoryNodeBuilder.getChildNodeNames(MemoryNodeBuilder.java:379)     at org.apache.jackrabbit.oak.plugins.index.p2.strategy.ContentMirrorStoreStrategy.remove(ContentMirrorStoreStrategy.java:66)     at org.apache.jackrabbit.oak.plugins.index.p2.Property2IndexUpdate.apply(Property2IndexUpdate.java:143)     at org.apache.jackrabbit.oak.plugins.index.p2.Property2IndexDiff.apply(Property2IndexDiff.java:232)     at org.apache.jackrabbit.oak.plugins.index.IndexHookManager.apply(IndexHookManager.java:71)     at org.apache.jackrabbit.oak.plugins.index.IndexHookManager.processCommit(IndexHookManager.java:61)     at org.apache.jackrabbit.oak.spi.commit.CompositeHook.processCommit(CompositeHook.java:59)     at org.apache.jackrabbit.oak.kernel.KernelNodeStoreBranch.merge(KernelNodeStoreBranch.java:127)     at org.apache.jackrabbit.oak.core.RootImpl 2.run(RootImpl.java:240)     at org.apache.jackrabbit.oak.core.RootImpl 2.run(RootImpl.java:236)     at java.security.AccessController.doPrivileged(Native Method)     at javax.security.auth.Subject.doAs(Subject.java:337)     at org.apache.jackrabbit.oak.core.RootImpl.commit(RootImpl.java:235)     at org.apache.jackrabbit.oak.jcr.SessionDelegate.save(SessionDelegate.java:255)     at org.apache.jackrabbit.oak.jcr.SessionImpl.save(SessionImpl.java:283)     at org.apache.jackrabbit.oak.jcr.security.user.AbstractUserTest.tearDown(AbstractUserTest.java:72)     at org.apache.jackrabbit.test.AbstractJCRTest.run(AbstractJCRTest.java:456)     at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:83)     at org.junit.runner.JUnitCore.run(JUnitCore.java:157)     at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:76)     at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:195)     at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:63)     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)     at com.intellij.rt.execution.application.AppMain.main(AppMain.java:120)  {code}$$patch1-oak-ec961a38_Developer_PatchNaturalnessYe$$removed unused imports. added better delete method. removed unnecessary loop. $$1
oak-d0f6715d$$NodeDocument.getNodeAtRevision() may read too many revisions$$This is a regression introduced by OAK-1972.  The revision returned with the value may be different from the revision of the change when the change was first committed to a branch and later merged. In this case the value will return the merge revision. The check in getNodeAtRevision() introduced with OAK-1972 then assumes there may be more recent changes in a previous document and starts to scan the revision history. This scan depends on the number of changes that have been applied on the document since the most recent change on the property in question.$$patch1-oak-d0f6715d_Developer_PatchNaturalnessYe$$Improved workflow workflow. $$1
oak-38f5ef13$$Version garbage collector doesn't collect a rolled back document if it was never deleted$$If a commit gets rolled back it can leave (in case the document was never deleted explicitly) a document in a state like: {noformat} {        "_id" : "7:/etc/workflow/packages/2014/10/12/rep:ours",        "_deleted" : {         },        "_commitRoot" : {         },        "jcr:primaryType" : {         },        "_modified" : NumberLong(1413126245),        "_children" : true,        "_modCount" : NumberLong(2) } {noformat}  If the path is fairly busy, the document can get created naturally later and then follow the usual cycle. But, at times, such documents are ephemeral in nature and never re-used. In those cases, such documents can remain silently without getting collected.$$patch1-oak-38f5ef13_Developer_PatchNaturalnessYe$$Fix node delete code. removed new documents in same commit , but not in same branch. $$1
oak-1beb2a50$$Upgrade should not overwrite new oak specific builtin nodetypes$$None$$patch1-oak-1beb2a50_Developer_PatchNaturalnessYe$$added missing import. added missing imports. added missing imports. added TODO to RepositoryUpgrade. added missing init in RepositoryUpgrade. added missing namespaces declaration. added OAK - 1235 note. $$1
oak-ba38c380$$LIRS cache: zero size cache causes IllegalArgumentException$$The LIRS cache does not support a zero size cache currently. Such a configuration causes an IllegalArgumentException.  Instead, no exception should be thrown, and no or a minimum size cache should be used.$$patch1-oak-ba38c380_Developer_PatchNaturalnessYe$$Fix an issue with CacheLIRS . setMaxMemory. $$1
oak-408a566e$$Field boost not working if the property for indexing is picked using aggregate index rules$$For below index definition -  {code} {      jcr:primaryType:"oak:QueryIndexDefinition",    compatVersion:2,    type:"lucene",    async:"async",    reindex:false,    reindexCount:12,    aggregates:{         jcr:primaryType:"oak:Unstructured",       app:Asset:{            jcr:primaryType:"oak:Unstructured",          include0:{               jcr:primaryType:"oak:Unstructured",             path:"jcr:content/metadata/*"          }       }    },    indexRules:{         jcr:primaryType:"nt:unstructured",       app:Asset:{            jcr:primaryType:"nt:unstructured",          properties:{               jcr:primaryType:"nt:unstructured",             foo:{                  jcr:primaryType:"nt:unstructured",                nodeScopeIndex:true,                ordered:true,                propertyIndex:true,                name:"jcr:content/metadata/foo",                type:"Long",                boost:3,                nodeName:"foo"             }          }       }    } } {code}  On executing query of form -   {code} //element(*, app:Asset)  [     jcr:contains(., 'bar' ) ] {code}  should boost the results containing property - 'jcr:content/metadata/foo', but its ignoring index time boosting for it.$$patch1-oak-408a566e_Developer_PatchNaturalnessYe$$Fixed javadoc error. "Revert "" update lucene index editor to exclude "" binary "" ( fixes # 304 )". don ' t use aggregateMode for property fields in LuceneIndexEditor. Fixed LuceneIndexEditor . indexProperty ( ). $$1
oak-c2f5ca6c$$Slow event listeners do not scale as expected$${{org.apache.jackrabbit.oak.jcr.LargeOperationIT#slowListener}} does not scale to {{O n log n}} on the document node store.$$patch1-oak-c2f5ca6c_Developer_PatchNaturalnessYe$$Fixed a bug in the DocumentNodeState class. removed ' from ' and ' to '. Added children variable to class to prevent read / write after close. removed unnecessary use of PathUtils . getName ( ) in DocumentMK .. "Revert "" update to 2005 0198 """. don ' t share strings with other components / files. added path separator for method name. don ' t remove removed elements in added - > removed map. Fixing the build .. Fixed bug in JSON output. remove property call w . close ( ) ; return null ; }. Fix diff. Fix whitespace. added name to doc - node. added better validation. $$1
oak-69ba2a54$$XPath queries with ISO9075 escaped properties don't work$$XPath queries with ISO9075 escaped properties or relative path don't work as expected. Example:   {code} /jcr:root//*/element(*,rep:User)[_x002e_tokens/@jcr:primaryType] {code}  The relative property should be converted to ".tokens/@jcr:primaryType", but is not.  This issue is similar to OAK-1000, but for property names or relative properties.$$patch1-oak-69ba2a54_Developer_PatchNaturalnessYe$$allow path segmentation. read path segment. Fixed XPathToSQL2Converter . readIf ( ). $$1
oak-239de7b8$$Entries in _commitRoot not purged$$Entries in _commitRoot are not purged or moved to previous documents if there are no changes with those revisions. Usually there is always a change associated with a _commitRoot, but in some cases it may happen that the only update on the document is for non-revisioned data like the _children flag.$$patch1-oak-239de7b8_Developer_PatchNaturalnessYe$$"Revert "" update to 0 . 7 . 21 ( added missing imports ) """. "Revert "" update to 1 . 0 . 4 - updated """. "Revert "" update split operations """. collect local changes before collecting them. add bug to commit operations. "$ "" return value is null """. added revision to changes. "Revert "" update split operations """. avoid double negative decrement of garbage count. Fixed bug with document split operations where property_or_deleted is not used. $$1
oak-e33328e0$$Sling I18N queries not supported by Oak$$The Sling I18N component issues XPath queries like the following:  {code:none} //element(*,mix:language)[fn:lower-case(@jcr:language)='en']//element(*,sling:Message)[@sling:message]/(@sling:key|@sling:message) {code}  Such queries currently fail with the following exception:  {code:none} javax.jcr.query.InvalidQueryException: java.text.ParseException: Query: //element(*,mix:language)[fn:lower-(*)case(@jcr:language)='en']//element(*,sling:Message)[@sling:message]/(@sling:key|@sling:message); expected: (         at org.apache.jackrabbit.oak.jcr.query.QueryManagerImpl.executeQuery(QueryManagerImpl.java:115)         at org.apache.jackrabbit.oak.jcr.query.QueryImpl.execute(QueryImpl.java:85)         at org.apache.sling.jcr.resource.JcrResourceUtil.query(JcrResourceUtil.java:52)         at org.apache.sling.jcr.resource.internal.helper.jcr.JcrResourceProvider.queryResources(JcrResourceProvider.java:262)         ... 54 more Caused by: java.text.ParseException: Query: //element(*,mix:language)[fn:lower-(*)case(@jcr:language)='en']//element(*,sling:Message)[@sling:message]/(@sling:key|@sling:message); expected: (         at org.apache.jackrabbit.oak.query.XPathToSQL2Converter.getSyntaxError(XPathToSQL2Converter.java:704)         at org.apache.jackrabbit.oak.query.XPathToSQL2Converter.read(XPathToSQL2Converter.java:410)         at org.apache.jackrabbit.oak.query.XPathToSQL2Converter.parseExpression(XPathToSQL2Converter.java:336)         at org.apache.jackrabbit.oak.query.XPathToSQL2Converter.parseCondition(XPathToSQL2Converter.java:279)         at org.apache.jackrabbit.oak.query.XPathToSQL2Converter.parseAnd(XPathToSQL2Converter.java:252)         at org.apache.jackrabbit.oak.query.XPathToSQL2Converter.parseConstraint(XPathToSQL2Converter.java:244)         at org.apache.jackrabbit.oak.query.XPathToSQL2Converter.convert(XPathToSQL2Converter.java:153)         at org.apache.jackrabbit.oak.query.QueryEngineImpl.parseQuery(QueryEngineImpl.java:86)         at org.apache.jackrabbit.oak.query.QueryEngineImpl.executeQuery(QueryEngineImpl.java:99)         at org.apache.jackrabbit.oak.query.QueryEngineImpl.executeQuery(QueryEngineImpl.java:39)         at org.apache.jackrabbit.oak.jcr.query.QueryManagerImpl.executeQuery(QueryManagerImpl.java:110) {code}$$patch1-oak-e33328e0_Developer_PatchNaturalnessYe$$Allow functions to be used as arguments to XPathToSQL2Converter. improve sql2 converter. $$1
oak-999097e1$$Node builder for existing node return null for base state$${{MemoryNodeBuilder.getBaseState()}} returns null on builder for an existing node.$$patch1-oak-999097e1_Developer_PatchNaturalnessYe$$"Revert "" update to latest memory node builder """. "remove "" baseState "" assignment from MemoryNodeBuilder # isModified". added read ( ). $$1
oak-f620b79b$$TokenLoginModule can't handle case insensitive userids$$Login against TokenLoginModule with an userid different in case throws:   javax.security.auth.login.LoginException: Invalid token credentials.$$patch1-oak-f620b79b_Developer_PatchNaturalnessYe$$Oops , forgot a property. "Revert "" update to use "" userId "" and "" id "" in TokenProviderImpl #. Oops , we should probably use userId or id. "Revert "" merge fails on commit """. allow null values for token providers. added user method. $$1
oak-7fe28a0e$$Trying to remove a missing property throws PathNotFoundException$$The following code snippet throws a {{PathNotFoundException}} if the "missing" property is not present.  {code:java} node.setProperty("missing", (String) null); {code}  A better way to handle such a case would be for the above statement to simply do nothing.$$patch1-oak-7fe28a0e_Developer_PatchNaturalnessYe$$added method to remove property. added valueConverter to NodeImpl , targetValue is better handled by other nodes .. $$1
oak-690fb9f4$$Commit fails even though change made it to the DocumentStore$$In some rare cases it may happen that the DocumentNodeStore considers a commit as failed even though the changes were applied entirely to the DocumentStore. The issue happens when the update of the commit root is applied to the storage of a DocumentStore but then shortly after the communication between Oak the the storage system fails. On the Oak side the call will be considered as failed, but the change was actually applied.  The issue can be reproduced with the test attached to OAK-1641 and a replica-set with 3 nodes. Killing the primary node and restarting it a after a while in a loop will eventually lead to a commit that conflicts itself.$$patch1-oak-690fb9f4_Developer_PatchNaturalnessYe$$remove unused imports. updateCommitRoot ( UpdateOp , Revision ) throws DocumentStoreException. Improve logging for verifyCommitRootUpdateApplied ( ). Fix updateCommitRoot ( ). Log rollback failure. $$1
oak-429baf4d$$TreeTypeProvider treats optimized node type definition info as Ac-Content$$while investigating a bug reported by [~teofili] and [~mpetria] that cause group-import with policy node to fail when run with non-administrative session, i found that the {{TreeTypeProvider}} wrongly identifies the optimized item definition information stored with the node types (e.g. {{/jcr:system/jcr:nodeTypes/rep:AccessControllable/rep:namedChildNodeDefinitions/rep:policy}} ) as access control content and thus doesn't read it properly when using a session that doesn't have jcr:readAccessControl privilege at /jcr:system/jcr:nodeTypes.  the effect of this bug is as follows: the internal calculation of the effective node type and thus item definitions will not work properly for {{rep:policy}} nodes (and similar) as the editing session cannot read the full (oak internal) node type definition as stored below {{/jcr:system/jcr:nodeTypes}}.$$patch1-oak-429baf4d_Developer_PatchNaturalnessYe$$"Revert "" update to use NT_REP_ACL """. $$1
oak-74f22886$$TarMK Cold Standby expose standby read timeout value$$Running into a read timeout on the standby instance logs some uncaught error: {code} org.apache.jackrabbit.oak.plugins.segment.standby.client.SegmentLoaderHandler Exception caught, closing channel. io.netty.handler.timeout.ReadTimeoutException: null {code}  I'm not sure how/if I need to fix this, the sync process will pickup again, but we can expose the timeout value, so if the network connection is known to be poor, a client can increase the timeout to work around this issue.$$patch1-oak-74f22886_Developer_PatchNaturalnessYe$$revert change to StandbyClient constructor. $$1
oak-a8c925e0$$Query constraints marked as invalid in the case of an mvp$$It seems that in the case of a query that has more constraints on the same property, like bq. //*[(@prop = 'aaa' and @prop = 'bbb' and @prop = 'ccc')]  the filter is marked as invalid (_#isAlwaysFalse_) and the query returns no results.  This is incorrect and affects queries that search for multi-valued properties on nodes.  This comes from/affects OAK-1075.$$patch1-oak-a8c925e0_Developer_PatchNaturalnessYe$$added workflow improvements. $$1
oak-e6d4f9a6$$XPath to SQL-2 conversion fails due to escaping error$$The problem is that the comment is not properly escaped (a C-style comment), so that "*/" in the XPath query accidentally ends the comment in the SQL-2 query.  The following query can't be converted to SQL-2, because it contains "*/":  {noformat} /jcr:root/etc//*[@type = 'product'  and ((@size = 'M' or */@size= 'M' or */*/@size = 'M'  or */*/*/@size = 'M' or */*/*/*/@size = 'M' or */*/*/*/*/@size = 'M'))] {noformat}  I think this was introduced by OAK-2354  http://svn.apache.org/viewvc?view=revision&amp;revision=1645616$$patch1-oak-e6d4f9a6_Developer_PatchNaturalnessYe$$"Revert "" escape xpath query string """. added helper method for building xpath comment on leaf nodes. $$1
oak-2f95b81f$$Repeated MongoMK.rebase() always adds new revision$$MongoMK always adds a new revision to the branch on rebase, even when the branch is already up-to-date.$$patch1-oak-2f95b81f_Developer_PatchNaturalnessYe$$using correct method to compare branch with old one. $$1
oak-1bf5c550$$potential clash of commit id's after restart$$the commit id's in the current implementation are counter-based, i.e. every commit (on HEAD or on a branch) gets its id by incrementing counter.  only the current HEAD id is recorded/persisted. on startup the counter is initialized with the current HEAD id.   assume the following sequence:  - ...startup... - counter == HEAD == 99 - commit on HEAD -> new HEAD rev: ++counter == 100 - create branch -> new branch rev: ++counter == 101 - ...restart... - counter == HEAD == 100 - commit on HEAD -> new HEAD rev: ++counter == 101 => clashes with older branch rev!   since a commit is never overwritten the above scenario results in a private branch revision marked as HEAD, i.e. the revision history is corrupted.$$patch1-oak-1bf5c550_Developer_PatchNaturalnessYe$$Remove unused local variable. updated in - memory readHead ( ). Improved javadoc in Persistence. added NODES table. Improved method to return ids from readHead ( ). added method to return last commit ID. Improved H2Persistence interface. "Revert "" update to node "" statement". Remove unused variable. "Revert "" update to NODES ( rawId , data , time ) """. add release referesh before touch. opengled = false note in opengled = false statement. added node deletes. Fix # 1296. Fix # 210. $$1
oak-b76b31f7$$Background update may create journal entry with incorrect id$$The conflict check does not consider changes that are made visible between the rebase and the background read.$$patch1-oak-b76b31f7_Developer_PatchNaturalnessYe$$Fix method name. "Revert "" update to latest snapshot """. "avoid "" null """. Add missing method. $$1
oak-29d3d8f1$$Lucene index not created if no node is indexed$$If a Lucene property index is defined for a property which is not present in any of the nodes then {{LuceneIndexWriter}} would create any lucene index for that.  For eg if we have an index of {{foo}} and none of the node has property {{foo}} set in that case {{LuceneIndexWriter}} would not create an {{IndexWriter}} and hence no directory would be created. Later when system performs a query like {{select jcr:path from nt:base where foo = 'bar'}} then {{LucenePropertyIndex}} would not participate in the query as no Lucene index would be found and system would revert to traversal.  As a fix Lucene index should still be created even if it does not contain any document$$patch1-oak-29d3d8f1_Developer_PatchNaturalnessYe$$added missing import. enable reindex mode if missing state. added reindex flag to LuceneIndexEditorContext. Fixed issue with closing writer on reindex. enable reindex mode. $$1
oak-311e8b33$$SegmentWriter doesn't properly check the length of external blob IDs$$To store the length field of an external binary ID, the following encoding is used:  {noformat} 1110 + 4bit + 8bit {noformat}  which allows to store numbers between 0 and 2{^}12^ - 1.   The current implementation of {{SegmentWriter}} allows the length of binary IDs to range between 0 and 2{^}13^ - 1, writing incorrect data when the length of the binary ID ranges from 2{^}12^ to 2{^}13^ - 1.  When reading this incorrect data back, an {{IllegalStateException}} is thrown complaining that the first byte of the length fields has an unexpected value record type. See OAK-1842 for an example.$$patch1-oak-311e8b33_Developer_PatchNaturalnessYe$$Improved check for binary ID encoding. $$1
oak-459bd065$$Node#setProperty(String, Calendar) doesn't take time zone in account$$Node#setProperty(String, Calendar) doesn't take time zone in account.  It looks the Calendar value is straightly stored as a long without take in consideration the time zone,  Unit test to follow$$patch1-oak-459bd065_Developer_PatchNaturalnessYe$$ISO -8601 - 1 . 5 use string literals in property value. $$1
oak-194999ed$$SplitOperations purges _commitRoot entries too eagerly$$OAK-2528 introduced purging of _commitRoot entries without associated local changes on the document. Those _commitRoot entries are created when a child nodes is added and the _children flag is touched on the parent.  The purge operation is too eager and removes all such entries, which may result in an undetected hierarchy conflict.$$patch1-oak-194999ed_Developer_PatchNaturalnessYe$$"Revert "" update split operations """. fix commit bug. $$1
oak-f0fbacab$$Node becomes invalid after Session#move()$$moving or renaming an existing (saved) node renders that node instance invalid and any access on that node instance will throw IllegalStateException.$$patch1-oak-f0fbacab_Developer_PatchNaturalnessYe$$Improved method to check if a node - > stale state. Use internal location for getLocation ( ). Make getItemLocator ( ) return items that do not exist ( because we know the revision ) .. $$1
oak-ea7a6199$$NoSuchElementException thrown by NodeDocument$$Following error is seen with latest 1.0.9-SNAPSHOT builds on some system  {noformat} Caused by: java.util.NoSuchElementException: null         at java.util.TreeMap.key(TreeMap.java:1221)         at java.util.TreeMap.firstKey(TreeMap.java:285)         at java.util.Collections UnmodifiableSortedMap.firstKey(Collections.java:1549)         at com.google.common.collect.ForwardingSortedMap.firstKey(ForwardingSortedMap.java:73)         at org.apache.jackrabbit.oak.plugins.document.NodeDocument.getNodeAtRevision(NodeDocument.java:819)         at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.readNode(DocumentNodeStore.java:930) {noformat}  Most likely the above occurs because a {{TreeMap}} associated with some key in NodeDocument is empty.  {noformat} 23.01.2015 01:57:23.308 *WARN* [pool-11-thread-5]org.apache.jackrabbit.oak.plugins.observation.NodeObserver Error whiledispatching observation eventscom.google.common.util.concurrent.UncheckedExecutionException:com.google.common.util.concurrent.UncheckedExecutionException:java.util.NoSuchElementException         at com.google.common.cache.LocalCache Segment.get(LocalCache.java:2199)         at com.google.common.cache.LocalCache.get(LocalCache.java:3932)         at com.google.common.cache.LocalCache LocalManualCache.get(LocalCache.java:4721)         at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.getChildren(DocumentNodeStore.java:731)         at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.diffImpl(DocumentNodeStore.java:1666)         at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.access 200(DocumentNodeStore.java:105)         at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore 7.call(DocumentNodeStore.java:1260)         at org.apache.jackrabbit.oak.plugins.document.MongoDiffCache.getChanges(MongoDiffCache.java:88)         at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.diffChildren(DocumentNodeStore.java:1255)         at org.apache.jackrabbit.oak.plugins.document.DocumentNodeState.compareAgainstBaseState(DocumentNodeState.java:260)         at org.apache.jackrabbit.oak.plugins.observation.EventGenerator Continuation.run(EventGenerator.java:172)         at org.apache.jackrabbit.oak.plugins.observation.EventGenerator.generate(EventGenerator.java:118)         at org.apache.jackrabbit.oak.plugins.observation.NodeObserver.contentChanged(NodeObserver.java:156)         at org.apache.jackrabbit.oak.spi.commit.BackgroundObserver 1 1.call(BackgroundObserver.java:117)         at org.apache.jackrabbit.oak.spi.commit.BackgroundObserver 1 1.call(BackgroundObserver.java:111)         at java.util.concurrent.FutureTask.run(FutureTask.java:262)         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)         at java.util.concurrent.ThreadPoolExecutor Worker.run(ThreadPoolExecutor.java:615)         at java.lang.Thread.run(Thread.java:744) Caused by: com.google.common.util.concurrent.UncheckedExecutionException:java.util.NoSuchElementException         at com.google.common.cache.LocalCache Segment.get(LocalCache.java:2199)         at com.google.common.cache.LocalCache.get(LocalCache.java:3932)         at com.google.common.cache.LocalCache LocalManualCache.get(LocalCache.java:4721)         at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.getNode(DocumentNodeStore.java:704)         at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.readChildren(DocumentNodeStore.java:786)         at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore 4.call(DocumentNodeStore.java:734)         at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore 4.call(DocumentNodeStore.java:731)         at com.google.common.cache.LocalCache LocalManualCache 1.load(LocalCache.java:4724)         at com.google.common.cache.LocalCache LoadingValueReference.loadFuture(LocalCache.java:3522)         at com.google.common.cache.LocalCache Segment.loadSync(LocalCache.java:2315)         at com.google.common.cache.LocalCache Segment.lockedGetOrLoad(LocalCache.java:2278)         at com.google.common.cache.LocalCache Segment.get(LocalCache.java:2193)        ... 18 common frames omitted Caused by: java.util.NoSuchElementException: null         at java.util.TreeMap.key(TreeMap.java:1221)         at java.util.TreeMap.firstKey(TreeMap.java:285)         at java.util.Collections UnmodifiableSortedMap.firstKey(Collections.java:1549)         at com.google.common.collect.ForwardingSortedMap.firstKey(ForwardingSortedMap.java:73)         at org.apache.jackrabbit.oak.plugins.document.NodeDocument.getNodeAtRevision(NodeDocument.java:819)         at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.readNode(DocumentNodeStore.java:930)         at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore 3.call(DocumentNodeStore.java:707)         at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore 3.call(DocumentNodeStore.java:704)         at com.google.common.cache.LocalCache LocalManualCache 1.load(LocalCache.java:4724)         at com.google.common.cache.LocalCache LoadingValueReference.loadFuture(LocalCache.java:3522)         at com.google.common.cache.LocalCache Segment.loadSync(LocalCache.java:2315)         at com.google.common.cache.LocalCache Segment.lockedGetOrLoad(LocalCache.java:2278)         at com.google.common.cache.LocalCache Segment.get(LocalCache.java:2193) {noformat}$$patch1-oak-ea7a6199_Developer_PatchNaturalnessYe$$added OAK - 2442 to NodeDocument # getLatestValue. $$1
oak-36e70bd7$$NodeBuilder.reset might lead to inconsistent builder$$The following test fails: {code} NodeBuilder root = new MemoryNodeBuilder(BASE); NodeBuilder x = root.child("x"); NodeBuilder y = x.child("y");  root.reset(BASE); assertTrue(root.hasChildNode("x")); assertFalse(x.hasChildNode("y"));  // fails {code}$$patch1-oak-36e70bd7_Developer_PatchNaturalnessYe$$reset revision to 0. $$1
oak-9f7c1df0$$Upgraded version history has UUIDs as jcr:frozenUuid of non-referenceable nodes$$In Jackrabbit Classic each node, even non-referenceable ones, has a UUID as its identifier, and thus the {{jcr:frozenUuid}} properties of frozen nodes are always UUIDs. In contrast Oak uses path identifiers for non-referenceable frozen nodes (see OAK-1009), which presents a problem when dealing with version histories migrated from Jackrabbit Classic.  To avoid this mismatch, the upgrade code should check each frozen node for referenceability and replace the frozen UUID with a path identifier if needed.$$patch1-oak-9f7c1df0_Developer_PatchNaturalnessYe$$"Revert "" update to latest oak upgrade """. Fix node state changes. added missing call to setChildOrder ( ). added missing method. added missing property. fixedFrozenUuid = false ;. added missing return statement. $$1
oak-2f85bd78$$Node name having non space whitspace chars should not be allowed$$Due to the changes done in OAK-1174 node with non space whitespace chars like '\n', '\r' etc can be created. This is not desirable and also JR2 does not allow such node to be created so check must be added to prevent such a name from getting created.  As discussed in [1] this is regression due to usage of incorrect utility method as part of [2] the fix can be simply using a {{Character#isWhitespace}} instead of {{Character#isSpaceChar}}  [1] http://mail-archives.apache.org/mod_mbox/jackrabbit-oak-dev/201509.mbox/%3CCAHCW-mkkGtxkn%2B9xfXuvMTfgykewjMPsLwrVH%2B00%2BXaBQjA0sg%40mail.gmail.com%3E [2] https://github.com/apache/jackrabbit-oak/commit/342809f7f04221782ca6bbfbde9392ec4ff441c2$$patch1-oak-2f85bd78_Developer_PatchNaturalnessYe$$"Revert "" update to allowOtherWhitespaceChars """. "Revert "" Allow other whitespace characters """. $$1
oak-d2da7499$$Blob GC throws NPE$$Blob GC when registered without a shared data store throws NPE. The {{ClusterRepositoryInfo#getId}} method should check if clusterId is registered or not.$$patch1-oak-d2da7499_Developer_PatchNaturalnessYe$$added missing import. added check for null. $$1
oak-5931a4a7$$Non-blocking reindexing doesn't finish properly$$The non blocking reindexer needs to run at least 2 cycles before setting the index definition back to synchronous mode. Currently it is too eager to mark the status as 'done' which confuses the _PropertyIndexAsyncReindex_ mbean into thinking the indexing is over and so skipping the final round that is supposed to do the switch back to sync mode.$$patch1-oak-5931a4a7_Developer_PatchNaturalnessYe$$"Revert "" update to latest oak 4 . 9 . 0 """. Add missing import. added async reindex god for index managers. Fix case. $$1
oak-9238264d$$XPath failures for typed properties$$It looks like there are some failures in xpath queries that expect a match only on properties of a certain type (which is to be inferred from the query)$$patch1-oak-9238264d_Developer_PatchNaturalnessYe$$"Revert "" update to latest oak plugin """. Fix warning. "Revert "" skip current node """. don ' t skip current node if ( elm != null ). Fix an issue with SelectorImpl . properties ( ) where the target is not an empty collection. $$1
oak-65aa40dd$$Condition check broken in MemoryDocumentStore$$The Operation.CONTAINS_MAP_ENTRY condition check does not work correctly in the MemoryDocumentStore and may return false even when the condition is not met.$$patch1-oak-65aa40dd_Developer_PatchNaturalnessYe$$Fixed bug in MongoDB document store. $$1
accumulo-add180fb$$MockAccumulo doesn't throw informative errors$$Users are unable to tell if an error has occurred and whether it is due to unimplemented features in MockAccumulo.$$patch1-accumulo-add180fb_Developer_PatchNaturalnessYe$$Added throw if table does not exist .. throw exception if table does not exist. throw exception if table does not exist. throw exception if table does not exist. throw exception if table exists. throw NotImplementedException. throw exception on setLocalityGroups ( String , Set < Text > ). throw NotImplementedException. $$1
accumulo-c831e44d$$key.followingKey(PartialKey.ROW_COLFAM_COLQUAL_COLVIS) can produce a key with an invalid COLVIS$$Need a new algorithm for calculating the next biggest column visibility, because tagging \0 to the end creates an invalid column visibility. We might be able to minimize the timestamp for this (i.e. set timestamp to Long.MIN_VALUE, but keep column and row elements the same).$$patch1-accumulo-c831e44d_Developer_PatchNaturalnessYe$$static import of Key . toBytes. Remove unused import in Key class. Remove redundant toPrintableString ( ) method. add private modifier to rowColumnStringBuilder. $$1
accumulo-1b35d263$$ZooKeeperInstance only uses first ZooKeeper in list of quorum$$Had tests running which had a quorum of 3 ZooKeeper servers. One appears to have died and the test was then unable to connect to the Accumulo shell, hanging on trying to connect to ZooKeeper.  There was no client.conf file present, so a ClientConfiguration was constructed from accumulo-site.xml.  {code} this.zooKeepers = clientConf.get(ClientProperty.INSTANCE_ZK_HOST); {code}  When the commons configuration AbstractConfiguration class is used with the get() method, only the first element in the value is returned, as the implementation treats the other items as a list because of the default separator of a comma.  It's easily reproduced with the following:  {code}     ZooKeeperInstance inst = new ZooKeeperInstance("accumulo", "localhost,127.0.0.1");     System.out.println(inst.getZooKeepers()); {code}  The above will print  {noformat} localhost {noformat}  instead of the expected  {noformat} localhost,127.0.0.1 {noformat}$$patch1-accumulo-1b35d263_Developer_PatchNaturalnessYe$$Don ' t do list interpolation on CompositeConfiguration .. $$1
accumulo-47c64d9a$$ClientConfiguration.getAllPropertiesWithPrefix doesn't work$$I think I introduced this method for trace.span.receiver.*, and didn't write a test for it.  My mistake.$$patch1-accumulo-47c64d9a_Developer_PatchNaturalnessYe$$Remove the .. $$1
accumulo-ef0f6ddc$$Accumulo Shell does not respect 'exit' when executing file$$If there is an {{exit}} statement in the file given via {{accumulo shell -f file}}, the execution seems to skip it and go on to the next command instead of terminating.  To recreate: {noformat} [mike@home ~] cat bug.accumulo exit scan -np -t !METADATA [mike@home ~] bin/accumulo shell -f /home/mike/bug.accumulo {noformat}  Expected output: None Actual output: A full scan of the !METADATA$$patch1-accumulo-ef0f6ddc_Developer_PatchNaturalnessYe$$Fixing my bad in MockShell .. passw argument is not used , but @@ can be modified by the new user / pass. add token user / passw option to startup script. don ' t show fake option if it is not used. accumulo - 438 allow closing the scanners on exit. $$1
accumulo-e29dc4f5$$The update() method on the ProxyServer should throw a MutationsRejectedException$$None$$patch1-accumulo-e29dc4f5_Developer_PatchNaturalnessYe$$add missing throw. remove deprecated exception in AccumuloProxy . update ( ). remove unused throw in AccumuloProxy . recv_update. disable async call by default. Delete some unused exceptions. gh - 661 - update_result now returns null. Fixed # 1796. Fix visibility for BatchWriterPlusException. add exception cache to default constructor. throw exception if it is not null. addCellsToWriter ( Map , BatchWriterPlusException bwpe ). Fix bwpe. Add batch writer exception check. Fix possible NPE in ProxyServer . flush ( ). throw bwpe . exception if writer cache is gone. add exception. $$1
accumulo-872b6db3$$ColumnVisibility parse tree nodes do not have correct location offsets for AND and OR nodes$$Trying to do some transformations on visibility strings and running into issues working with the parse tree:  Clojure 1.5.1 user=> (import [org.apache.accumulo.core.security ColumnVisibility]) org.apache.accumulo.core.security.ColumnVisibility user=> (def vis (ColumnVisibility. "(W)|(U|V)")) #'user/vis user=> (.getTermStart (first (.getChildren (.getParseTree vis)))) 1 user=> (.getTermEnd (first (.getChildren (.getParseTree vis)))) 2 user=> (.getTermStart (second (.getChildren (.getParseTree vis)))) 0 user=> (.getTermEnd (second (.getChildren (.getParseTree vis)))) 8  Shouldn't those last two be 5 and 8?$$patch1-accumulo-872b6db3_Developer_PatchNaturalnessYe$$gh - 454 - update ColumnVisibility @@. $$1
accumulo-36225565$$In Accumulo 1.7.0, connecting to a minicluster started via bin/accumulo minicluster doesn't work$$In Accumulo 1.7.0, connecting to a minicluster started via "bin/accumulo minicluster" doesn't work.  When connecting, it appears to ignore the ZK port supplied in the command and is attempting to listen to ZK on 2181.  For example: accumulo-1.7.0 > bin/accumulo minicluster  Mini Accumulo Cluster    Directory:            /var/folders/rv/44k88tps4ql0dc1f68ck4d2w0000gn/T/1437925819514-0   Logs:                 /var/folders/rv/44k88tps4ql0dc1f68ck4d2w0000gn/T/1437925819514-0/logs   Instance Name:        miniInstance   Root Password:        secret   ZooKeeper:            localhost:56783   Shutdown Port:        4445    To connect with shell, use the following command :     accumulo shell -zh localhost:56783 -zi miniInstance -u root  Successfully started on Sun Jul 26 11:50:28 EDT 2015 ===================  From a new terminal:  accumulo-1.7.0 > accumulo shell -zh localhost:56783 -zi miniInstance -u root Password: ******* . 60 seconds later . 2015-07-26 11:52:44,436 [tracer.ZooTraceClient] ERROR: Unabled to get destination tracer hosts in ZooKeeper, will retry in 5000 milliseconds java.lang.RuntimeException: Failed to connect to zookeeper (localhost:2181) within 2x zookeeper timeout period 30000 	at org.apache.accumulo.fate.zookeeper.ZooSession.connect(ZooSession.java:124)  Shell - Apache Accumulo Interactive Shell - - version: 1.7.0 - instance name: miniInstance - instance id: a371d4ac-8bc7-4a6a-865f-5f3c8e27fbe1 - - type 'help' for a list of available commands - root@miniInstance>$$patch1-accumulo-36225565_Developer_PatchNaturalnessYe$$add zooKeeperHosts back in as a property. $$1
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1-accumulo-a64151e6_Developer_PatchNaturalnessYe$$Fix possible path separator in GC. gh - 454 - added test for url - caster. Handle invalid delete candidates for GC .. $$1
accumulo-0cf2ff72$$Remove username from initialization$$This is an artifact from a brief transition area during the 1.5 development. We have a flag for the user to set what the root username is, except it's never used. We should remove both the variable and the flag for it.$$patch1-accumulo-0cf2ff72_Developer_PatchNaturalnessYe$$gh - 320 - Removed use of deprecated attribute in Initialize. $$1
accumulo-17344890$$BlockedOutputStream can hit a StackOverflowError$$This issue mostly came up after a resolution to ACCUMULO-2668 that allows a byte[] to be passed directly to the underlying stream from the NoFlushOutputStream.  The problem appears to be due to the BlockedOutputStream.write(byte[], int, int) implementation that recursively writes out blocks/buffers out. When the stream is passed a large mutation (128MB was sufficient to trigger the error for me), this will cause a StackOverflowError.   This is appears to be specifically with encryption at rest turned on.  A simple fix would be to unroll the recursion.$$patch1-accumulo-17344890_Developer_PatchNaturalnessYe$$Fix possible NPE in BlockedOutputStream . write ( ). $$1
accumulo-ff8c2383$$MockTableOperations.deleteRow does not handle null for start or end keys$$The deleteRow function does not check for null values for start or end keys. These null values are passed down into key constructor which will throw a NullPointerException: java.lang.NullPointerException 	at org.apache.accumulo.core.data.Key.<init>(Key.java:103) 	at org.apache.accumulo.core.client.mock.MockTableOperations.deleteRows(MockTableOperations.java:315)  The API semantics dictate: if (start == null ) then start == Text() if (end == null ) then end == maxKey()$$patch1-accumulo-ff8c2383_Developer_PatchNaturalnessYe$$Fix NPE in MockTableOperations. $$1
accumulo-b082fc1e$$MockTable's addMutation does not check for empty mutation$$When calling addMutation or addMutations on a MockBatchWriter, the updates stored in the mutation are iterated over then committed in the MockTable class.   When this occurs in the TabletServerBatchWriter (eventually called from the BatchWriterImpl), however, the mutation size is first checked and if the mutation size is 0, an IllegalArgumentException is thrown.  In practice, if you have code that tries to submit an empty mutation to a BatchWriter, it will fail and throw an exception in the real world, but this will not be caught in tests against MockAccumulo.$$patch1-accumulo-b082fc1e_Developer_PatchNaturalnessYe$$throw exception if mutation size is zero. add missing import. add mutations accept null for row in MockBatchWriter. $$1
accumulo-891584fb$$Shell displays authTimeout poorly$$The authTimeout in the shell is displayed badly when executing {{about -v}}. Even though it is configured in integer minutes, it is converted to seconds for display as a floating point number with 2 decimals. This makes no sense, since the decimals will always be {{.00}}.  We can keep the units in seconds, I guess, but this needs to be displayed with {{%ds}} not {{%.2fs}}. This was broken in ACCUMULO-3224 by using TimeUnit to convert the number, instead of dividing by 1000.0 as we were doing manually before.$$patch1-accumulo-891584fb_Developer_PatchNaturalnessYe$$add format of auth timeout in shell script. $$1
accumulo-ddd2c3bc$$InputTableConfig missing isOfflineScan field in Serializer$$InputTableConfig write(DataOutput dataOutput) forgets to write out the isOfflineScan field, which makes it always false when it gets unserialized.$$patch1-accumulo-ddd2c3bc_Developer_PatchNaturalnessYe$$add offlineScan flag to InputTableConfig. add offline scan flag to InputTableConfig. $$1
accumulo-cfbf5999$$ProxyServer does not set column information on BatchScanner$$The createScanner method uses the options from the thrift request to call fetchColumn() and fetchColumnFamily(). The createBatchScanner should be doing have the same feature, though the statements are absent from the code.$$patch1-accumulo-cfbf5999_Developer_PatchNaturalnessYe$$gh - 66 added missing fetchColumnFamily call. $$1
accumulo-474b2577$$stop-all doesn't work: Error BAD_CREDENTIALS for user root$${noformat}   bin/accumulo admin stopAll 2013-02-27 14:56:14,072 [util.Admin] ERROR: org.apache.accumulo.core.client.AccumuloSecurityException: Error BAD_CREDENTIALS for user root - Username or Password is Invalid org.apache.accumulo.core.client.AccumuloSecurityException: Error BAD_CREDENTIALS for user root - Username or Password is Invalid 	at org.apache.accumulo.core.client.impl.MasterClient.execute(MasterClient.java:119) 	at org.apache.accumulo.server.util.Admin.stopServer(Admin.java:107) 	at org.apache.accumulo.server.util.Admin.main(Admin.java:95) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) 	at java.lang.reflect.Method.invoke(Method.java:597) 	at org.apache.accumulo.start.Main 1.run(Main.java:97) 	at java.lang.Thread.run(Thread.java:662) Caused by: ThriftSecurityException(user:root, code:BAD_CREDENTIALS) 	at org.apache.accumulo.core.master.thrift.MasterClientService shutdown_result shutdown_resultStandardScheme.read(MasterClientService.java:8424) 	at org.apache.accumulo.core.master.thrift.MasterClientService shutdown_result shutdown_resultStandardScheme.read(MasterClientService.java:8410) 	at org.apache.accumulo.core.master.thrift.MasterClientService shutdown_result.read(MasterClientService.java:8360) 	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78) 	at org.apache.accumulo.core.master.thrift.MasterClientService Client.recv_shutdown(MasterClientService.java:312) 	at org.apache.accumulo.core.master.thrift.MasterClientService Client.shutdown(MasterClientService.java:297) 	at org.apache.accumulo.server.util.Admin 1.execute(Admin.java:110) 	at org.apache.accumulo.server.util.Admin 1.execute(Admin.java:107) 	at org.apache.accumulo.core.client.impl.MasterClient.execute(MasterClient.java:113) 	... 8 more  {noformat}$$patch1-accumulo-474b2577_Developer_PatchNaturalnessYe$$Fixed error in ClientOpts .. Fixed default value for password. $$1
accumulo-f99b5654$$Missing toString, hashCode and equals methods on BatchWriterConfig$$Tried to test equality of two BatchWriterConfig objects, found they're missing all of the methods from Object that they should be implementing.$$patch1-accumulo-f99b5654_Developer_PatchNaturalnessYe$$imports. add default values. add javadoc. Added missing check for null max latency .. Added comment to BatchWriterConfig. add 122 if timeout is 0. gh - 66 fixed javadoc for BatchWriterConfig. add batch writer config. Fix UTF - 8 encoding of batch writer config. add private method to BatchWriterConfig. gh - 454 - update comment. Fixed bug in BatchWriterConfig . equals. $$1
accumulo-fb25913c$$AccumuloVFSClassloader incorrectly treats folders as folders of jar files$$Specifying a directory of classes is incorrectly interpreted as a directory of jars in the general.dynamic.classpaths configuration property.  Example: adding a path such as *_ ACCUMULO_HOME/core/target/classes_* gets incorrectly interpreted as *_ ACCUMULO_HOME/core/target/classes/\*_* and evaluates to *_ ACCUMULO_HOME/core/target/classes/org_* and *_ ACCUMULO_HOME/core/target/classes/META-INF_*, but *NOT* to *_ ACCUMULO_HOME/core/target/classes_* as expected.$$patch1-accumulo-fb25913c_Developer_PatchNaturalnessYe$$Added a shutdown thread to signal the end of ACCumuloVFSClassLoader . close (. Add log output. adding fix for case sensitive file systems. Added TODO for 2nd tier class loader. Removed unused interface. Reduced the print of class path .. Allow AccumuloVFSClassLoader to be used as a standard ClassLoader. gh - 66 catch any type of exceptions when running context managers from AccumuloVFS under. add accessor for context manager. $$1
accumulo-692efde2$$VisibilityFilter does not catch BadArgumentException$$If an invalid column visibility makes it into the system, then the VisibilityFilter may not handle it properly.   The accept method handles VisibilityParseException, but some of the parse code throws a BadArgumentException which is not handled.$$patch1-accumulo-692efde2_Developer_PatchNaturalnessYe$$Add missing import. Add missing argument exception catch block. $$1
accumulo-8ec4cb84$$delete mutations not working through the Proxy$$Aru Sahni writes:  {quote} I'm new to Accumulo and am still trying to wrap my head around its ways. To further that challenge, I'm using Pyaccumulo, which doesn't present much in terms of available reference material.  Right now I'm trying to understand how Accumulo manages record (key-value pair) deletions.  conn = Accumulo(host, port, user, password) table = 'test_table' conn.create_table(table) writer = conn.create_batch_writer(table) mut = Mutation('mut_01') mut.put(cf='item', cq='name', value='car') writer.add_mutation(mut) writer.close() conn.close()  Will generate a record (found via a shell scan):  mut_01 item:name []    car  However the subsequent mutation...  writer = conn.create_batch_writer(table) mut = Mutation('mut_01') mut.put(cf='item', cq='name', is_delete=True) writer.add_mutation(mut) writer.close()  Results in:  mut_01 item:name []  How should one expect the deleted row to be represented? That record sticks around even after I force a compaction of the table.  I was expecting it to not show up in any iterators, or at least provide an easy way to see if the cell has been deleted. {quote}  [~ecn] has confirmed the problem.$$patch1-accumulo-8ec4cb84_Developer_PatchNaturalnessYe$$gh - 454 - put Delete Delete Delete with timestamp. $$1
accumulo-5eceb10e$$Unable to assign single tablet table migrated to 1.6.0$$Sorry for the screen caps, no copy/paste from machines.  Background- several tables migrated from 1.5.1 to 1.6.0. Only one of which was a single tablet. Upon starting, we noticed that that single table was not loading and the master was reporting an unassigned tablet. Had a stack trace in the monitor (attached).  Also attached is a a metadata scan of the table in question (ID: 12). I was able to get a functional copy of the table by offlining 12 and cloning it. It functioned without issues. Attached is a copy of it's metadata scan as well (ID: 9o)  The stack trace leads me to it being a specific issue with the contents of srv:dir, and the only difference is the relative vs. absolute file names. This cluster was not changed to multiple namenodes and ../tables/default_tablet does not exist. There are other tables which still use the relative naming scheme, and the system does not seem to be having issues with them.$$patch1-accumulo-5eceb10e_Developer_PatchNaturalnessYe$$gh - 66 bumped up import. add ACCUMULO - 2974 Need to include the TableID in a relative path to. add missing import. add missing import. gh - 6647 Fix path to table files to make it work. $$1
accumulo-65390f8c$$Mock does not implement locality groups or merging$$The Mock Instance does not implement locality groups and throws an exception if one attempts to set them. It would be useful for the unit tests that I am writing for the Accumulo proxy to have at least minimal locality group functionality in the Mock instance, for example simply storing the groups and returning the stored groups when asked for.  *Edit: Tablet merging would be useful as well.$$patch1-accumulo-65390f8c_Developer_PatchNaturalnessYe$$add missing import. throw new NotImplementedException ( ). online ( ) throws AccumuloException , NoSuchTableException. add missing throw. throw exception if tableName does not exist. add missing imports. added localityGroups to MockTable. add method to set localityGroups ( ). $$1
accumulo-9d8cc45d$$Bulk random walk test failed$$The bulk random walk test failed while running on a 10 node cluster w/ the following error message.  {noformat} 18 23:36:05,167 [bulk.Setup] INFO : Starting bulk test on 459a04a0   19 00:24:33,950 [randomwalk.Framework] ERROR: Error during random walk java.lang.Exception: Error running node Bulk.xml         at org.apache.accumulo.server.test.randomwalk.Module.visit(Module.java:253)         at org.apache.accumulo.server.test.randomwalk.Framework.run(Framework.java:61)         at org.apache.accumulo.server.test.randomwalk.Framework.main(Framework.java:114)         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)         at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)         at java.lang.reflect.Method.invoke(Method.java:597)         at org.apache.accumulo.start.Main 1.run(Main.java:89)         at java.lang.Thread.run(Thread.java:662) Caused by: java.lang.Exception: Error running node bulk.Verify         at org.apache.accumulo.server.test.randomwalk.Module.visit(Module.java:253)         at org.apache.accumulo.server.test.randomwalk.Module.visit(Module.java:249)         ... 8 more Caused by: java.lang.Exception: Bad key at r0d646 cf:000 [] 1326932285943 false -1         at org.apache.accumulo.server.test.randomwalk.bulk.Verify.visit(Verify.java:51)         at org.apache.accumulo.server.test.randomwalk.Module.visit(Module.java:249)         ... 9 more {noformat}  Looking at the table the rows [r0d646, r0edd9] and [r0f056, r10467] all had -1 values.  There was a tablet that overlapped the first range of -1 rows exactly 268;r0edd9;r0d645.  This tablet had only the following activity on a tablet server and was then merged out of existence.  The merge operation was 268;r10eff;r093b1.  {noformat} 19 00:05:10,966 [tabletserver.Tablet] DEBUG: Files for low split 268;r0edd9;r0d645  [/b-0001azp/I0001azt.rf, /b-0001azp/I0001azu.rf, /t-0001ale/A0001an3.rf] 19 00:05:10,974 [tabletserver.Tablet] TABLET_HIST: 268;r0f055;r0d645 split 268;r0edd9;r0d645 268;r0f055;r0edd9 19 00:05:10,975 [tabletserver.Tablet] TABLET_HIST: 268;r0edd9;r0d645 opened  19 00:05:15,029 [tabletserver.Tablet] TABLET_HIST: 268;r0edd9;r0d645 import /b-0001azi/I0001azm.rf 17138 0 19 00:05:15,103 [tabletserver.Tablet] DEBUG: Starting MajC 268;r0edd9;r0d645 [/b-0001azi/I0001azm.rf, /b-0001azp/I0001azt.rf, /b-0001azp/I0001azu.rf, /t-0001ale/A0001an3.rf] --> /t-0001apj/A0001bri.rf_tmp 19 00:05:15,339 [tabletserver.Tablet] TABLET_HIST: 268;r0edd9;r0d645 import /b-0001azx/I0001azy.rf 16620 0 19 00:05:15,651 [tabletserver.Compactor] DEBUG: Compaction 268;r0edd9;r0d645 181,080 read | 60,360 written | 553,761 entries/sec |  0.327 secs 19 00:05:15,661 [tabletserver.Tablet] TABLET_HIST: 268;r0edd9;r0d645 MajC [/b-0001azi/I0001azm.rf, /b-0001azp/I0001azt.rf, /b-0001azp/I0001azu.rf, /t-0001ale/A0001an3.rf] --> /t-0001apj/A0001bri.rf 19 00:05:30,672 [tabletserver.Tablet] DEBUG: Starting MajC 268;r0edd9;r0d645 [/b-0001azx/I0001azy.rf] --> /t-0001apj/C0001brn.rf_tmp 19 00:05:30,810 [tabletserver.Compactor] DEBUG: Compaction 268;r0edd9;r0d645 60,360 read | 60,360 written | 534,159 entries/sec |  0.113 secs 19 00:05:30,824 [tabletserver.Tablet] TABLET_HIST: 268;r0edd9;r0d645 MajC [/b-0001azx/I0001azy.rf] --> /t-0001apj/C0001brn.rf 19 00:05:30,943 [tabletserver.Tablet] DEBUG: initiateClose(saveState=true queueMinC=false disableWrites=false) 268;r0edd9;r0d645 19 00:05:30,943 [tabletserver.Tablet] DEBUG: completeClose(saveState=true completeClose=true) 268;r0edd9;r0d645 19 00:05:30,947 [tabletserver.Tablet] TABLET_HIST: 268;r0edd9;r0d645 closed 19 00:05:30,947 [tabletserver.TabletServer] DEBUG: Unassigning 268;r0edd9;r0d645@(null,xxx.xxx.xxx.xxx:9997[134d7425fc59413],null) 19 00:05:30,949 [tabletserver.TabletServer] INFO : unloaded 268;r0edd9;r0d645 19 00:05:30,949 [tabletserver.TabletServer] INFO : unloaded 268;r0edd9;r0d645  {noformat}   For the second range of -1 values [r0f056, r10467], r0f056 corresponds to the split point r0f055.  Howerver, there is no split point corresponding to r10467. All of the tablets w/ a split of r0f055 lived on one tablet server.    {noformat} 19 00:02:21,262 [tabletserver.Tablet] TABLET_HIST: 268<;r0d645 split 268;r0f055;r0d645 268<;r0f055 19 00:02:21,263 [tabletserver.Tablet] TABLET_HIST: 268;r0f055;r0d645 opened  19 00:02:21,264 [tabletserver.Tablet] TABLET_HIST: 268<;r0f055 opened  19 00:02:44,504 [tabletserver.Tablet] TABLET_HIST: 268<;r0f055 split 268;r11da6;r0f055 268<;r11da6 19 00:02:44,505 [tabletserver.Tablet] TABLET_HIST: 268;r11da6;r0f055 opened  19 00:05:10,974 [tabletserver.Tablet] TABLET_HIST: 268;r0f055;r0d645 split 268;r0edd9;r0d645 268;r0f055;r0edd9 19 00:05:10,975 [tabletserver.Tablet] TABLET_HIST: 268;r0f055;r0edd9 opened  19 00:05:15,023 [tabletserver.Tablet] TABLET_HIST: 268;r11da6;r0f055 split 268;r0f622;r0f055 268;r11da6;r0f622 19 00:05:15,024 [tabletserver.Tablet] TABLET_HIST: 268;r0f622;r0f055 opened  {noformat}  All of the tablets mentioned so far were all merged away in the same merge operation, making this operation a possible place were data loss occurred.  However, I can not pinpoint the issue at this point in time.  Below is a little info about the merge from the master logs showing which tablets were involved in the merge.  {noformat} 19 00:05:30,616 [master.EventCoordinator] INFO : Merge state of 268;r10eff;r093b1 set to WAITING_FOR_CHOPPED 19 00:05:30,677 [master.Master] INFO : Asking xxx.xxx.xxx.xxx:9997[134d7425fc5940c] to chop 268;r09927;r0903a 19 00:05:30,678 [master.Master] INFO : Asking xxx.xxx.xxx.xxx:9997[134d7425fc5940c] to chop 268;r0ca9e;r09927 19 00:05:30,678 [master.Master] INFO : Asking xxx.xxx.xxx.xxx:9997[134d7425fc5940a] to chop 268;r0d2b5;r0ca9e 19 00:05:30,678 [master.Master] INFO : Asking xxx.xxx.xxx.xxx:9997[134d7425fc59412] to chop 268;r0d645;r0d2b5 19 00:05:30,678 [master.Master] INFO : Asking xxx.xxx.xxx.xxx:9997[134d7425fc59413] to chop 268;r0edd9;r0d645 19 00:05:30,678 [master.Master] INFO : Asking xxx.xxx.xxx.xxx:9997[134d7425fc59413] to chop 268;r0f055;r0edd9 19 00:05:30,678 [master.Master] INFO : Asking xxx.xxx.xxx.xxx:9997[134d7425fc59413] to chop 268;r0f622;r0f055 19 00:05:30,678 [master.Master] INFO : Asking xxx.xxx.xxx.xxx:9997[134d7425fc59413] to chop 268;r0f68b;r0f622 19 00:05:30,678 [master.Master] INFO : Asking xxx.xxx.xxx.xxx:9997[134d7425fc59413] to chop 268;r10c14;r0f68b 19 00:05:30,678 [master.Master] INFO : Asking xxx.xxx.xxx.xxx:9997[134d7425fc59413] to chop 268;r110f7;r10c14 {noformat}  When this test verifies its data and detects data loss, there is no easy way to determine at what time the data loss occurred.  It might be useful to modify the data in the bulk test such that it is easier to determine the time when data was lost.  For example the continuous ingest test creates linked list and it is possible to determine tight time bounds when a node was ingested.  However that may change the nature of this test and the bugs that it might find.$$patch1-accumulo-9d8cc45d_Developer_PatchNaturalnessYe$$Remove unused constructor .. Fixed cruft. add TTL to ColumnAgeOffFilter. add fille. gh - 661 - updated constructor of AgeOffFilter does not have the same constructor .. gh - 454 - patched AgeOffFilter doesn ' t have needed fields. Make versioning iterator copyable. Remove unused constructor .. Fixed a bug in TimestampFilter . deepCopy. Added a constructor that can be used to deep - copy the iterator. gh - 293 , made versioning iterator constructor nullable. deep copy RegExFilter. $$1
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch1-accumulo-15476a0d_Developer_PatchNaturalnessYe$$Fix MockTable logic. $$1
accumulo-ebf22df0$$Make sure iterators handle deletion entries properly$$In minor compaction scope and in non-full major compaction scopes the iterator may see deletion entries. These entries should be preserved by all iterators except ones that are strictly scan-time iterators that will never be configured for the minc or majc scopes. Deletion entries are only removed during full major compactions.$$patch1-accumulo-ebf22df0_Developer_PatchNaturalnessYe$$add comment about SortedKeyValueIterator ' s next move. Fix typo in SortedKeyValueIterator javadocs. don ' t delete keys that are no longer used .. $$1
accumulo-cd7feb4d$$RegExFilter deepCopy NullPointerException$$If any of the regex matcher objects are null (i.e. for example, if you only specify a regex for the column family), the deepCopy call will throw a NullPointerException.$$patch1-accumulo-cd7feb4d_Developer_PatchNaturalnessYe$$gh - 661 - RegExFilter doesn ' t copy the matchers in the source iterator. fixed regexp pattern matching. $$1
accumulo-f848178e$$RangeInputSplit doesn't serialize table name$$Found another missed member in the serialization of RangeInputSplit: the table name.  Not a huge deal because the table information should still be in the Configuration for most users, but this does break in "advanced" uses of mapreduce. Work around is to re-set the table in the RangeInputSplit in your overridden InputFormat.getRecordReader or make sure the Configuration is consistent from getRecordReader and getSplits.$$patch1-accumulo-f848178e_Developer_PatchNaturalnessYe$$table = in . readUTF ( ) ; } @ Override public void readFields ( ) throws IOException. Table should write directly to the output stream. $$1
accumulo-6c565dfb$$Shell's setiter is not informative when using a bad class name$$In the shell, I did setiter using a class that wasn't found. Rather then a message about it not being found, I just get told that I have an invalid argument. Even turning on debug, I had to use the stack trace to figure out why it was erroring.$$patch1-accumulo-6c565dfb_Developer_PatchNaturalnessYe$$Fix whitespace. $$1
accumulo-5ca779a0$$hashCode for Mutation has an unfortunate implementation$$While looking at how a tablet server processes constraint violations, I happened to look into Mutation's hashCode implementation:  {code}   @Override   public int hashCode() {     return toThrift(false).hashCode();   } {code}  Clicking through to TMutation hashCode finds this gem:  {code}   @Override   public int hashCode() {     return 0;   } {code}$$patch1-accumulo-5ca779a0_Developer_PatchNaturalnessYe$$Fix hash code for mutations. $$1
accumulo-2f0643a9$$Off-by-one error in FamilyIntersectingIterator$$In the buildDocKey() function within the FamilyIntersectingIterator there is a bug that shortens the docID by 1.  This causes the wrong doc's data to be returned in the results of a query using this Iterator.$$patch1-accumulo-2f0643a9_Developer_PatchNaturalnessYe$$Fix log in FamilyIntersectingIterator. $$1
accumulo-97f16db4$$AccumuloVFSClassloader creates conflicting local cache directory names when vfs.cache.dir property is set.$$When the vfs.cache.dir property is not set, the AccumuloVFSClassloader will use java.io.tmpdir as a base directory for the local cache of jars and then generate a unique directory name using a combination of the processid, hostname and userid executing the JVM.  When the vfs.cache.dir property is set, that value is used as the base directory and  an attempt to generate a unique directory is made using an AtomicInteger. This isn't suitable because for non-long lived processes, this will always be 1 - and there's a good chance that directory already exists and is owned by another user, and not writable to by the user in question.   This leads to a failure of the invoked accumulo component to start.  Modify the behavior of the unique directory creation when vfs.cache.dir is set so that it employs the same mechanism for unique directory naming that is used when it is not set.$$patch1-accumulo-97f16db4_Developer_PatchNaturalnessYe$$remove unused variable. Remove hard coded test. gh - 66 bump up version. $$1
accumulo-4aeaeb2a$$stacking combiners produces a strange result$$Paste the following into your shell:  {noformat} deletetable test createtable test setiter -t test -p 16 -scan -n test_1 -class org.apache.accumulo.core.iterators.user.SummingCombiner  count:a  STRING setiter -t test -p 17 -scan -n test_2 -class org.apache.accumulo.core.iterators.user.SummingCombiner  count:a  STRING setiter -t test -p 18 -scan -n test_3 -class org.apache.accumulo.core.iterators.user.SummingCombiner  count:a  STRING setiter -t test -p 10 -scan -n test_4 -class org.apache.accumulo.core.iterators.user.SummingCombiner  count  STRING insert row count a 1 insert row count a 1 insert row count b 1 insert row count b 1 insert row count b 1 insert row count c 1 scan {noformat}  I expect:  {noformat} row count:a []    2 row count:b []    3 row count:c []    1 {noformat}  But instead, I get this: {noformat} row count:a []    12 {noformat}$$patch1-accumulo-4aeaeb2a_Developer_PatchNaturalnessYe$$Fix typo in Combiner where Key is not a literal Key object. $$1
accumulo-2fd7633f$$RangeInputSplit Writable methods don't serialize IteratorSettings$$Was trying to figure out why some information was getting lost on a RangeInputSplit after serialization, and found out it was because the serialization and deserialization of the class didn't include the configured IteratorSettings.  This likely isn't a big problem for normal users as, when no IteratorSettings are configured on the RangeInputSplit, it falls back to pulling from the Configuration, but it's possible, with "non-standard" uses of mapreduce, that information could be missing in the Configuration that the mappers receive, and would subsequently error.$$patch1-accumulo-2fd7633f_Developer_PatchNaturalnessYe$$gh - 66 fixed readme. add iterators and level to range input split. $$1
accumulo-5594b2e0$$importdirectory failing on split table$$bulk import for the wikisearch example isn't working properly: files are not being assigned to partitions if there are splits.$$patch1-accumulo-5594b2e0_Developer_PatchNaturalnessYe$$gh - 454 - remove debug line. gh - 454 - remove unused import. add missing imports. gh - 45 - added missing import. gh - 375 - added missing final check. gh - 663 - Added debug level for bulk imports. remove too much newline. gh - 66 bumped up imports. Added debug statement. Improved method to send data to ACU for later processing .. remove maxnum and add proper enum value. remove unused import. $$1
accumulo-76d727f0$$RegExFilter does not properly regex when using multi-byte characters$$The current RegExFilter class uses a ByteArrayBackedCharSequence to set the data to match against. The ByteArrayBackedCharSequence contains a line of code that prevents the matcher from properly matching multi-byte characters.  Line 49 of ByteArrayBackedCharSequence.java is: return (char) (0xff & data[offset + index]);                                                                                                This incorrectly casts a single byte from the byte array to a char, which is 2 bytes in Java. This prevents the RegExFilter from properly performing Regular Expressions on multi-byte character encoded values.  A patch for the RegExFilter.java file has been created and will be submitted.$$patch1-accumulo-76d727f0_Developer_PatchNaturalnessYe$$add import. remove unused import. add ENCODING constant to RegExFilter. Fixed regexp matching bug in regexp filter. Set encoding explicitly in RegExFilter # setEncoding. add RegExFilter . ENCODING. Allow RegExFilter to use UTF - 8 encoding for regexps. Added setEncoding method to ScanIterator config. $$1
accumulo-28294266$$TraceProxy.trace should not throw InvocationTargetException$$In {{TraceProxy.trace}} there is the following code snippet: {code}         try {           return method.invoke(instance, args);         } catch (Throwable ex) {           ex.printStackTrace();           throw ex;         } {code} When this is an InvocationTargetException, it can really mess with the calling code's exception handling logic.$$patch1-accumulo-28294266_Developer_PatchNaturalnessYe$$add RuntimeException check. $$1
accumulo-4d23d784$$CompactCommand description is incorrect$$The compact command has the following description  {code} root@accumulo> compact -? usage: compact [<table>{ <table>}] [-?] [-b <begin-row>] [--cancel] [-e <end-row>] [-nf] [-ns <namespace> | -p <pattern> | -t <tableName>]  [-pn <profile>]  [-w] description: sets all tablets for a table to major compact as soon as possible (based on current time)   -?,--help                       display this help   -b,--begin-row <begin-row>      begin row (inclusive)      --cancel                     cancel user initiated compactions   -e,--end-row <end-row>          end row (inclusive)   -nf,--noFlush                   do not flush table data in memory before compacting.   -ns,--namespace <namespace>     name of a namespace to operate on   -p,--pattern <pattern>          regex pattern of table names to operate on   -pn,--profile <profile>         iterator profile name   -t,--table <tableName>          name of a table to operate on   -w,--wait                       wait for compact to finish {code}  However, the --begin-row is not inclusive.  Here is a simple demonstration. {code} createtable compacttest addsplits a b c insert "a" "1" "" "" insert "a" "2" "" "" insert "b" "3" "" "" insert "b" "4" "" "" insert "c" "5" "" "" insert "c" "6" "" "" flush -w scan -t accumulo.metadata -np compact -b a -e c -t compacttest -w scan -t accumulo.metadata -np deletetable compacttest -f {code}  You will see that file associated with the 'a' split is still a F flush file, which the files in the 'b' and 'c' split are A files.  Not sure if the fix is to update the commands description, which would be easy, or to make the begin row actually inclusive.$$patch1-accumulo-4d23d784_Developer_PatchNaturalnessYe$$gh - 66 - add startRowOpt with schema in java. $$1
accumulo-b007b22e$$Combiner default behavior is dangerous$$Currently if the users does not give the combiner any columns to work against, it will work against all columns.  This is dangerous, if a user accidentally forgets to specify columns then their data could be unintentionally corrupted.  Something different needs to be done.    Also classes that extend combiner should call super.validateOptions().$$patch1-accumulo-b007b22e_Developer_PatchNaturalnessYe$$Fixed typo in Combiner javadocs. $$1
accumulo-73ce9cfb$$not possible to create a Mutation object from scala w/o some extra helper code$$issue:   it's not possible to create a Mutation object from scala without employing a standalone java jar wrapper. the preferred method for creating the object has you do it in two stages: create with table row, then employ Mutation.put() to populate the object with the actual mutation data. when you do this in scala, you get a  java.lang.IllegalStateException: Can not add to mutation after serializing it at org.apache.accumulo.core.data.Mutation.put(Mutation.java:168) at org.apache.accumulo.core.data.Mutation.put(Mutation.java:163) at org.apache.accumulo.core.data.Mutation.put(Mutation.java:211)  error. I *think* this has something to do with the byte array going out of scope in Scala but somehow not in Java. If you concat the operations (constuctor().put(data, data, ...) you don't run into the error, but scala sees a Unit return type, so you can't actually add the mutation to a BatchWriter. The only way I was able to get around this was to create a stand-alone jar with a method that created then returned a populated mutation object.   I wasn't sure whether or not to call this a bug or an enhancement. given that you probably want Accumulo to play nice with Scala I decided to call it a bug.   below is a link to the stack overflow thread I created whilst figuring all this out:   http://stackoverflow.com/questions/29497547/odd-error-when-populating-accumulo-1-6-mutation-object-via-spark-notebook/29527189#29527189$$patch1-accumulo-73ce9cfb_Developer_PatchNaturalnessYe$$Added a private method to retrieve the serialized snapshot of a mutation .. Fixed bug in Mutually exclusive constructor .. Fix toThrift. $$1
accumulo-13eb19c2$$AccumuloInputFormat cannot fetch empty column family$$The following fails: {code:java} Job job = new Job(); HashSet<Pair<Text,Text>> cols = new HashSet<Pair<Text,Text>>(); cols.add(new Pair<Text,Text>(new Text(""), null)); AccumuloInputFormat.fetchColumns(job, cols); Set<Pair<Text,Text>> setCols = AccumuloInputFormat.getFetchedColumns(job); assertEquals(cols.size(), setCols.size()); {code}$$patch1-accumulo-13eb19c2_Developer_PatchNaturalnessYe$$Fix bug in Controller . getFetchedColumns. $$1
accumulo-116d5928$$Make sure iterators handle deletion entries properly$$In minor compaction scope and in non-full major compaction scopes the iterator may see deletion entries. These entries should be preserved by all iterators except ones that are strictly scan-time iterators that will never be configured for the minc or majc scopes. Deletion entries are only removed during full major compactions.$$patch1-accumulo-116d5928_Developer_PatchNaturalnessYe$$gh - 66 fixed a small bug in GrepIterator. deep copy GrepIterator , to keep compatibility with GrepIterator interface. $$1
accumulo-c489d866$$"du" on a table without files does not report$${noformat} shell> createtable t shell> du t shell> {noformat}  expected:  {noformat} shell> du t              0 t shell> {noformat}$$patch1-accumulo-c489d866_Developer_PatchNaturalnessYe$$Ignore empty tables in metadata. Do not include empty tables in usage map. $$1
accumulo-397f86f6$$RegExFilter does not properly regex when using multi-byte characters$$The current RegExFilter class uses a ByteArrayBackedCharSequence to set the data to match against. The ByteArrayBackedCharSequence contains a line of code that prevents the matcher from properly matching multi-byte characters.  Line 49 of ByteArrayBackedCharSequence.java is: return (char) (0xff & data[offset + index]);                                                                                                This incorrectly casts a single byte from the byte array to a char, which is 2 bytes in Java. This prevents the RegExFilter from properly performing Regular Expressions on multi-byte character encoded values.  A patch for the RegExFilter.java file has been created and will be submitted.$$patch1-accumulo-397f86f6_Developer_PatchNaturalnessYe$$add import. remove unused import. add ENCODING constant to RegExFilter. Fixed regexp matching bug in regexp filter. Set encoding explicitly in RegExFilter # setEncoding. add RegExFilter . ENCODING. Allow RegExFilter to use UTF - 8 encoding for regexps. Added setEncoding method to ScanIterator config. $$1
accumulo-25cf3ccd$$Authorizations has inconsistent serialization$$The same set of authorizations may not serialize to the same value each time, if specified in a different order when constructed (like new Authorizations("a", "b") and new Authorizations("b", "a")), because serialization reproducibility depends on the insert order in the underlying HashSet.  So, one could get the following to happen: {code:java} true == auths1.equals(auths2) && !auths1.serialize().equals(auths2.serialize()); {code}$$patch1-accumulo-25cf3ccd_Developer_PatchNaturalnessYe$$Remove unused imports. Fixed a bug in Authorizations. gh - 414 - update workflow workflow. make Authorizations constructor public. gh - 454 - update Authorizations. Fixed trivial equals implementation. gh - 454 - update code. $$1
accumulo-81d25bc2$$display the exact number of tablet servers$$This is a regression of ACCUMULO-1140$$patch1-accumulo-81d25bc2_Developer_PatchNaturalnessYe$$Fixed small print .. Allow override of NumberType constructor. $$1
accumulo-dc95cb69$$FileOperations expects RFile filenames to contain only 1 dot.$$If I attempt to create or read an RFile that contains more than 1 dot in the filename, FileOperations throws an IllegalArgumentException("File name " + name + " has no extension"). Please allow creation/import of RFiles that have more than 1 dot in the filename.$$patch1-accumulo-dc95cb69_Developer_PatchNaturalnessYe$$Fix an error in FileOperations . get ( name ) that could cause mangling directory. $$1
accumulo-eb0f9b41$$CompactCommand description is incorrect$$The compact command has the following description  {code} root@accumulo> compact -? usage: compact [<table>{ <table>}] [-?] [-b <begin-row>] [--cancel] [-e <end-row>] [-nf] [-ns <namespace> | -p <pattern> | -t <tableName>]  [-pn <profile>]  [-w] description: sets all tablets for a table to major compact as soon as possible (based on current time)   -?,--help                       display this help   -b,--begin-row <begin-row>      begin row (inclusive)      --cancel                     cancel user initiated compactions   -e,--end-row <end-row>          end row (inclusive)   -nf,--noFlush                   do not flush table data in memory before compacting.   -ns,--namespace <namespace>     name of a namespace to operate on   -p,--pattern <pattern>          regex pattern of table names to operate on   -pn,--profile <profile>         iterator profile name   -t,--table <tableName>          name of a table to operate on   -w,--wait                       wait for compact to finish {code}  However, the --begin-row is not inclusive.  Here is a simple demonstration. {code} createtable compacttest addsplits a b c insert "a" "1" "" "" insert "a" "2" "" "" insert "b" "3" "" "" insert "b" "4" "" "" insert "c" "5" "" "" insert "c" "6" "" "" flush -w scan -t accumulo.metadata -np compact -b a -e c -t compacttest -w scan -t accumulo.metadata -np deletetable compacttest -f {code}  You will see that file associated with the 'a' split is still a F flush file, which the files in the 'b' and 'c' split are A files.  Not sure if the fix is to update the commands description, which would be easy, or to make the begin row actually inclusive.$$patch1-accumulo-eb0f9b41_Developer_PatchNaturalnessYe$$gh - 454 - Add missing semicolon. gh - 454 - Add missing argument. gh - 31 - Updated optUtil startRowOpt javadoc. gh - 661 @ TableOperations . merge does not have a length , but @@ -. gh - 66 compaction contains some data. gh - 66 - added missing startRowOptExclusive argument. remove the redundant startRowOpt description from MergeCommand. $$1
accumulo-cfb832a1$$ProxyServer ignores value of isDeleted on ColumnUpdate$$The ProxyServer ignores the actual boolean value of the isDeleted flag on a ColumnUpdate.  If the isDeleted value is set, regardless of the actual boolean value, the ProxyServer marks the update as a delete.  The ProxyServer should be updated to check the value of the flag.$$patch1-accumulo-cfb832a1_Developer_PatchNaturalnessYe$$gh - 661 don ' t delete timestamps on put ( ). $$1
accumulo-dc9f23d9$$TimestampFilter should serialize start and end as longs in the IteratorSetting$$Although the TimestampFilter supports using longs to set the start or end timestamp, it formats them as strings using SimpleDateFormat when storing or retrieving them in the IteratorSetting.  This results in exceptions when the timestamps being used aren't able to be formatted as _yyyyMMddHHmmssz_. For example, try {{setEnd(253402300800001,true)}}  Instead, {{setStart()}} and {{setEnd()}} could just as easily use {{String.valueOf(long i)}} to store the values, and {{init()}} could retrieve them using {{Long.valueOf(String s)}}.$$patch1-accumulo-dc9f23d9_Developer_PatchNaturalnessYe$$gh - 66 fixed syntax error. add long prefix. Fixed typo in TimestampFilter javadoc. gh - 127 - updated TimestampFilter with CLI option. Fixed typo in timestamp filter javadoc. Fixed error in TimestampFilter # setStart. Fixed error in TimestampFilter . setEnd. add long prefix option , but use string format. add long parser and long prefix options , they don ' t add them when they are not inclusive. $$1
accumulo-94c2a31f$$calling MiniAccumuloCluster.stop multiple times fails with NPE$$On the mailing list [~ctubbsii] mentioned seeing some NPEs in the stderr for {{mvn verify}}.  I see one here when running mvn verify with either hadoop profile:  {quote} Exception in thread "Thread-0" java.lang.NullPointerException 	at org.apache.accumulo.minicluster.MiniAccumuloCluster.stopProcessWithTimeout(MiniAccumuloCluster.java:449) 	at org.apache.accumulo.minicluster.MiniAccumuloCluster.stop(MiniAccumuloCluster.java:376) 	at org.apache.accumulo.minicluster.MiniAccumuloCluster 1.run(MiniAccumuloCluster.java:318) {quote}  The relevant piece of code (in 1.5.2-SNAP) is the {{executor.execute}} below  {code}   private int stopProcessWithTimeout(final Process proc, long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException {     FutureTask<Integer> future = new FutureTask<Integer>(new Callable<Integer>() {         @Override         public Integer call() throws InterruptedException {           proc.destroy();           return proc.waitFor();         }     });      executor.execute(future);      return future.get(timeout, unit);   } {code}  Reading through the code for stop, it nulls out executor when it's done. So the easy way to get an NPE is calling stop() multiple times on a MAC instance. Since we have a shutdown hook that calls stop, that means that a single user invocation of stop should result in a NPE later.  Since start() doesn't allow multiple starts, we probably shouldn't allow multiple stops. That would mean adding logic to the shutdown hook to check if we're already stopped or making a private unguarded version of stop that allows multiple calls and using that from the hook.  criteria for closing this issue:  * MAC should document wether calling stop() multiple times is allowed * fix MAC.stop to either guard against multiple calls or handle them gracefully * find out why this only gets an NPE in one place. Do we rely on the shutdown hook everywhere?$$patch1-accumulo-94c2a31f_Developer_PatchNaturalnessYe$$gh - 16 - added missing import. disable parallel compilation for minicluster. Fix possible race condition. Added a Preconditions . checkNotNull check. $$1
accumulo-adee0f12$$Validity checks missing for readFields and Thrift deserialization$$Classes in o.a.a.core.data (and potentially elsewhere) that support construction from a Thrift object and/or population from a {{DataInput}} (via a {{readFields()}} method) often lack data validity checks that the classes' constructors enforce. The missing checks make it possible for an attacker to create invalid objects by manipulating the bytes being read. The situation is analogous to the need to check objects deserialized from their Java serialized form within the {{readObject()}} method.$$patch1-accumulo-adee0f12_Developer_PatchNaturalnessYe$$add check for null fields in Key. Fix NPEs in Mutation . constructor. $$1
accumulo-699b8bf0$$ShutdownTServer never sets requestedShutdown$$ACCUMULO-1259 made ShutdownTServer a bit more sane WRT to what it was doing and the FATE repo interface.  One attempt it makes it to not repeatedly invoke shutdownTServer on the master..  Except {{requestedShutdown}} is never set to {{true}}.$$patch1-accumulo-699b8bf0_Developer_PatchNaturalnessYe$$add requestedShutdown flag to shutdown TServer at the end of table. $$1
accumulo-6ff92b12$$Accumulo Shell does not respect 'exit' when executing file$$If there is an {{exit}} statement in the file given via {{accumulo shell -f file}}, the execution seems to skip it and go on to the next command instead of terminating.  To recreate: {noformat} [mike@home ~] cat bug.accumulo exit scan -np -t !METADATA [mike@home ~] bin/accumulo shell -f /home/mike/bug.accumulo {noformat}  Expected output: None Actual output: A full scan of the !METADATA$$patch1-accumulo-6ff92b12_Developer_PatchNaturalnessYe$$passw argument is not used , but @@ can we use the existing value of passw. add token user / passw option to startup script. don ' t show fake option on startup. no exit loop on execFile. Fixing my bad in MockShell .. $$1
accumulo-1f7dd2d5$$History command incorrectly numbers commands$$When you use the history command, it will provide you with a list of previous commands that have been executed, each with a command number. However, if you try to use history expansion by number to invoke one of those commands, you will be off by one.  I think this is because the history command in added to the list after it shows you the list, and pushes everything else up by one. Uncertain if this is something we do wrong, or if this is an upstream JLine bug.$$patch1-accumulo-1f7dd2d5_Developer_PatchNaturalnessYe$$add Missing import. use the history API directly. $$1
accumulo-db4a291f$$master killed a tablet server$$Master killed a tablet server for having long hold times.  The tablet server had this error during minor compaction:  {noformat} 01 23:57:20,073 [security.ZKAuthenticator] ERROR: org.apache.zookeeper.KeeperException NoNodeException: KeeperErrorCode = NoNode for /accumulo/88cd0f63-a36a-4218-86b1-9ba1d2cccf08/users/user004 org.apache.zookeeper.KeeperException NoNodeException: KeeperErrorCode = NoNode for /accumulo/88cd0f63-a36a-4218-86b1-9ba1d2cccf08/users/user004         at org.apache.zookeeper.KeeperException.create(KeeperException.java:102)         at org.apache.zookeeper.KeeperException.create(KeeperException.java:42)         at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1243)         at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1271)         at org.apache.accumulo.core.zookeeper.ZooUtil.recursiveDelete(ZooUtil.java:103)         at org.apache.accumulo.core.zookeeper.ZooUtil.recursiveDelete(ZooUtil.java:117)         at org.apache.accumulo.server.zookeeper.ZooReaderWriter.recursiveDelete(ZooReaderWriter.java:67)         at sun.reflect.GeneratedMethodAccessor53.invoke(Unknown Source)         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)         at java.lang.reflect.Method.invoke(Method.java:597)         at org.apache.accumulo.server.zookeeper.ZooReaderWriter 1.invoke(ZooReaderWriter.java:169)         at  Proxy4.recursiveDelete(Unknown Source)         at org.apache.accumulo.server.security.ZKAuthenticator.dropUser(ZKAuthenticator.java:252)         at org.apache.accumulo.server.security.Auditor.dropUser(Auditor.java:104)         at org.apache.accumulo.server.client.ClientServiceHandler.dropUser(ClientServiceHandler.java:136)         at sun.reflect.GeneratedMethodAccessor52.invoke(Unknown Source)         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)         at java.lang.reflect.Method.invoke(Method.java:597)         at cloudtrace.instrument.thrift.TraceWrap 1.invoke(TraceWrap.java:58)         at  Proxy2.dropUser(Unknown Source)         at org.apache.accumulo.core.client.impl.thrift.ClientService Processor dropUser.process(ClientService.java:2257)         at org.apache.accumulo.core.tabletserver.thrift.TabletClientService Processor.process(TabletClientService.java:2037)         at org.apache.accumulo.server.util.TServerUtils TimedProcessor.process(TServerUtils.java:151)         at org.apache.thrift.server.TNonblockingServer FrameBuffer.invoke(TNonblockingServer.java:631)         at org.apache.accumulo.server.util.TServerUtils THsHaServer Invocation.run(TServerUtils.java:199)         at java.util.concurrent.ThreadPoolExecutor Worker.runTask(ThreadPoolExecutor.java:886)         at java.util.concurrent.ThreadPoolExecutor Worker.run(ThreadPoolExecutor.java:908)         at org.apache.accumulo.core.util.LoggingRunnable.run(LoggingRunnable.java:34)         at java.lang.Thread.run(Thread.java:662)  {noformat}  This tablet was the result of a split that occurred during a delete.  The master missed this tablet when taking tablets offline.  We need to do a consistency check on the offline tablets before deleting the table information in zookeeper.$$patch1-accumulo-db4a291f_Developer_PatchNaturalnessYe$$add example method to create partitioned Wikipedia dataset. gh - 66 fixed readability issue. hadoop - 2 . 0 - updated WikipediaInputFormat to write the location data for each host. Fixing the log level for zkErro. add missing import. add notifyAll. Add a try catch block to prevent minc init from failing in case of no flush id. Fix a subtle bug I happened to introduce last night .. Put back LOG messages at end of minc run. add catch block to prevent NoNodeException. Fix NoNodeException in Tablet. add missing imports. gh - 16 - Updated imports. made Article constructor private. Fixed readFields ( ) method. add missing import. add missing import. Added a try catch block to prevent replication from failing replications. Added a try catch block for tablets that have no flush id. Ignore NoNodeException in TabletServer . java. add example implementation. add custom user property. add hadoop config to WikipediaConfiguration. $$1
accumulo-813109d7$$Verify all methods in the ProxyService that take table names actually throw TableNotFoundException when the table is missing.$$None$$patch1-accumulo-813109d7_Developer_PatchNaturalnessYe$$add all the exceptions. add missing throw in addConstraint. add missing exception signature to method. add ouch3 throw in AccumuloProxy . java. Added missing throw in AccumuloProxy . removeTableProperty ( ). Added missing exception signature to method recv_removeTableProperty ( ). add ouch3 throw in AccumuloProxy . java. add missing throw in AccumuloProxy # setTableProperty ( ). Added missing exception signature to AccumuloProxy . recv_setTableProperty ( ). add ouch3 throw in AccumuloProxy . java. add exception type to AccumuloProxy . getResult ( ). add exception type to AccumuloProxy . getResult ( ). add exception type to AccumuloProxy . getResult ( ). catch TableNotFoundException on AccumuloProxy. catch TableNotFoundException on AccumuloProxy. catch TableNotFoundException on AccumuloProxy. add OUCH3_FIELD_DESC. add TableNotFoundException to by name. add case for OUCH3. add OUCH3 to tmpMap. add TableNotFoundException in removeConstraint_result. gh - 454 - added missing ouch3 in addConstraintResult .. add ouch3 to AccumuloProxy . clear ( ). Fixed # 1862. gh - 40179 Fix typo in AccumuloProxy enum. add OUCH3 in AccumuloProxy. Add OUCH3 to thrift test. Fixed check for OC - 7785. Fix typo in compareTo method. "Add ouch3 to "" toString ( ) "" line". gh - 454 - ignore AccumuloProxy ouch3 in thrift. add OUCH3_FIELD_DESC if it is not null. gh - 454 - update thrift to use bit set and not set by default. add missing ouch3 exception in incoming response. add OUCH3_FIELD_DESC constant to AccumuloProxy. add TableNotFoundException to by name. add case OUCH3. add OUCH3 to tmpMap. add ouch3 to AccumuloProxy so it can rethrow as well. Add missing OOME. add ouch3 to AccumuloProxy . clear ( ). Fixed validateOuch3 return type .. Fixed typo in AccumuloProxy .. add OUCH3 in AccumuloProxy. Add support for OUCH3 in thrift. workflow improvements. Fix typo in compareTo method. "Add ouch3 to "" toString ( ) "" line". gh - 454 - ignore AccumuloProxy ouch3 in thrift. add OUCH3_FIELD_DESC if it is not null. gh - 661 - update AccumuloProxy to use bit set and not set by default. add missing ouch3 exception in incoming response. add OUCH3_FIELD_DESC constant to AccumuloProxy. add TableNotFoundException to by name. add case OUCH3. add OUCH3 to tmpMap. add ouch3 to AccumuloProxy so it can rethrow as well. gh - 45921 - Restore original exception if it was set on thrift. add ouch3 to AccumuloProxy . clear ( ). Fixed ouch3 = null ; if it is not set then set the default value to null. Fixed typo in AccumuloProxy .. add OUCH3 in AccumuloProxy. Add OUCH3 to thrift test. gh - 454 - check for same ouch3 value from thrift jar. Fix typo in compareTo method. "Add ouch3 to "" toString ( ) "" line". gh - 454 - Ignore AccumuloProxy ouch3 message in case of error. add OUCH3_FIELD_DESC if it is not null. gh - 454 - update AccumuloProxy to not ignore ouch3. add missing ouch3 exception in incoming response. add missing import. gh - 233 - restore original behaviour. add scanner cache configuration for AccumuloProxy. Added TableNotFoundException check. gh - 661 remove hardcoded method. add missing if / else. Added missing UUID in ProxyServer. gh - 66 fixed gh - 66. $$1
accumulo-0dc92ca1$$Stat calculation of STDEV may be inaccurate$$The math is sound, but it is susceptible to rounding errors. We should address that.  See http://www.strchr.com/standard_deviation_in_one_pass and http://www.cs.berkeley.edu/~mhoemmen/cs194/Tutorials/variance.pdf$$patch1-accumulo-0dc92ca1_Developer_PatchNaturalnessYe$$Add StorelessUnivariateStatistic to Stat class. $$1
accumulo-941e3cb1$$Resolve table name to table id once in Accumulo input format$$AccumuloInputFormat (and I suspect AccumuloOutputFormat) sends the table name to each mapper.  The mapper uses this table name to create a scanner.  In the case of the following events a map reduce job could read from two different table ids.      # start M/R job reading table A  # rename table A (tableId=1) to table C  # rename table B (tableId=2) to table A  If the input format passed table id 1 to the mappers, then the renames would not cause a problem.$$patch1-accumulo-941e3cb1_Developer_PatchNaturalnessYe$$Fixed issue with replication of tables with MockInstance. gh - 454 - update AbstractInputFormat to use table name instead of table id. Fixed uninitialized table id in AbstractInputFormat. removeInstance ( ) from AbstractInputFormat. gh - 454 - Offline scan requires credentials for mock instance. gh - 454 - update AbstractInputFormat to use table name instead of tableId. add tableId = ""; if it is a mock instance , use the value of "" ;. removeInstance ( ) from AbstractInputFormat. $$1
accumulo-50db442b$$CompactCommand description is incorrect$$The compact command has the following description  {code} root@accumulo> compact -? usage: compact [<table>{ <table>}] [-?] [-b <begin-row>] [--cancel] [-e <end-row>] [-nf] [-ns <namespace> | -p <pattern> | -t <tableName>]  [-pn <profile>]  [-w] description: sets all tablets for a table to major compact as soon as possible (based on current time)   -?,--help                       display this help   -b,--begin-row <begin-row>      begin row (inclusive)      --cancel                     cancel user initiated compactions   -e,--end-row <end-row>          end row (inclusive)   -nf,--noFlush                   do not flush table data in memory before compacting.   -ns,--namespace <namespace>     name of a namespace to operate on   -p,--pattern <pattern>          regex pattern of table names to operate on   -pn,--profile <profile>         iterator profile name   -t,--table <tableName>          name of a table to operate on   -w,--wait                       wait for compact to finish {code}  However, the --begin-row is not inclusive.  Here is a simple demonstration. {code} createtable compacttest addsplits a b c insert "a" "1" "" "" insert "a" "2" "" "" insert "b" "3" "" "" insert "b" "4" "" "" insert "c" "5" "" "" insert "c" "6" "" "" flush -w scan -t accumulo.metadata -np compact -b a -e c -t compacttest -w scan -t accumulo.metadata -np deletetable compacttest -f {code}  You will see that file associated with the 'a' split is still a F flush file, which the files in the 'b' and 'c' split are A files.  Not sure if the fix is to update the commands description, which would be easy, or to make the begin row actually inclusive.$$patch1-accumulo-50db442b_Developer_PatchNaturalnessYe$$gh - 66 - added missing startRowOptExclusive argument. remove the redundant startRowOpt description from MergeCommand. gh - 454 - Add missing semicolon. gh - 454 - Add missing argument. gh - 66 - add startRowOpt for 1 . 7. gh - 661 @ TableOperations . merge does not have a length , but @@ -. gh - 66 compaction contains some data. $$1
accumulo-9396979b$$bulk imported files showing up in metadata after bulk import fails$$Bulk import fails.  The file is moved to the failures directory.  But references in the !METADATA table remain.$$patch1-accumulo-9396979b_Developer_PatchNaturalnessYe$$add import. Improved MetadataConstraints . addIfNotPresent .. gh - 16 - fixed metadata table schema validation error. Fixed bug in MetadataConstraints. Add time constraint for bulk write operations. gh - 16 - fixed issue with metadata constraint check. gh - 231 - add missing violations for schema file. gh - 66 log file for metadata constraint violations. add comment for empty value check. $$1
accumulo-7651b777$$Shell.config()'s return value is ignored.$${{Shell.config()}} returns a boolean which is true if there was an error configuring the shell, but the value is never observed. This can result in other unintended errors (like trying to use the ConsoleReader member when it's not initialized).$$patch1-accumulo-7651b777_Developer_PatchNaturalnessYe$$add configError back. Added missing javadoc. ignore help on startup , because it ' s already been checked in .. set exitCode to 1. ignore exception in shell. add return. config doesn ' t print error code if config fails. ignore config setting the shell failed , just return false for now. ignore configError in MockShell. remove configError check. $$1
accumulo-d9ab8449$$Don't cache credentials in client-side Connector$$AuthenticationToken objects are Destroyable. However, this cannot be exercised properly in the client code, because the Connector immediately serializes the credentials and stores them as long as the Connector lives.  It should be possible to destroy a token after creating a Connector, and thereby forcing any further RPC calls initiated by that Connector to fail to authenticate. This means that serialization on the client side to a TCredentials object needs to occur just before the RPC call.$$patch1-accumulo-d9ab8449_Developer_PatchNaturalnessYe$$add missing import. add missing import. throw exception if token is destroyed. add missing import. gh - 16 - fixed gh - 16. add missing import. gh - 59 - Adding a throw in credentials . toThrift ( ). Fixed constructor. throw exception if token is destroyed. $$1
accumulo-27d4ee21$$Token class option always requires token property$$In testing out ACCUMULO-2815, I attempted to manually provide a KerberosToken to authenticate myself and then launch the shell, but ran into an issue. The KerberosToken (in its current state) needs no options: it's wholly functional on its own.  {{accumulo shell -tc org.apache.accumulo.core.client.security.tokens.KerberosToken}}  gives an error  {noformat} 2014-12-16 11:41:09,712 [shell.Shell] ERROR: com.beust.jcommander.ParameterException: Must supply either both or neither of '--tokenClass' and '--tokenProperty' {noformat}  And providing an empty option just prints the help message {{accumulo shell -tc org.apache.accumulo.core.client.security.tokens.KerberosToken -l ""}}  I'm guessing the latter is just how the JCommander DynamicParameter is implemented, but I don't see a reason why every authentication *must* have some properties provided to it.$$patch1-accumulo-27d4ee21_Developer_PatchNaturalnessYe$$add final modifier to hasToken. Fix ' must supply both ' and ' tokenClass ' properties by default. $$1
accumulo-15e83709$$Consolidate ZK code WRT retries$$A couple of general ZK things that should be fixed up:  # Multiple means of automatic retrying of recoverable ZooKeeper errors through use of an InvocationHandler and a Proxy around IZooReader(Writer) # Encapsulate retry logic # Switch over callers to use the retrying instance instead of the non-retrying instance$$patch1-accumulo-15e83709_Developer_PatchNaturalnessYe$$Add missing import. add missing import. Fixed a bug in ZooUtil . equals. Adding missing throws. Fixed typo in ZooUtil putData ( ) where the node exists policy was not respected by. Include all the recursive copyPersistent ( ) calls into the destination directory ; see ZooUtil . java. Fixed typo in ZooCache log. $$1
accumulo-019edb16$$Incompatible API changes in 1.6.0$$While examining API changes for 1.6.0 I noticed some non-deprecated methods were removed.  I am not sure how important these are, but technically these methods are in the public API.  Opening this issue to document what I found.  I compared 1.6.0 to 1.5.0.  In ACCUMULO-1674 the following methods were removed  {noformat} package org.apache.accumulo.core.client.mapreduce.lib.util ConfiguratorBase.getToken ( Class<?>, Configuration ) [static]  :  byte[ ] ConfiguratorBase.getTokenClass ( Class<?> ,Configuration) [static]  :  String {noformat}  In ACCUMULO-391 the following method was removed  {noformat} package org.apache.accumulo.core.client.mapreduce.lib.util InputConfigurator.getTabletLocator ( Class<?>, Configuration ) [static]  : TabletLocator  {noformat}  In ACCUMULO-391 the following method was removed and not properly fixed in ACCUMULO-2586  {noformat} accumulo-core.jar, RangeInputSplit.class package org.apache.accumulo.core.client.mapred InputFormatBase.RangeInputSplit.InputFormatBase.RangeInputSplit ( String table, Range range, String[ ] locations ) package org.apache.accumulo.core.client.mapreduce InputFormatBase.RangeInputSplit.InputFormatBase.RangeInputSplit ( String table, Range range, String[ ] locations )  {noformat}   It seems like the following were removed in ACCUMULO-1854   {noformat} package org.apache.accumulo.core.client.mapred InputFormatBase.RecordReaderBase<K.setupIterators (JobConf job, Scanner scanner )  :  void package org.apache.accumulo.core.client.mapreduce InputFormatBase.RecordReaderBase<K.setupIterators (TaskAttemptContext context, Scanner scanner)  :  void {noformat}  In ACCUMULO-1018 the following method was removed  {noformat} package org.apache.accumulo.core.client MutationsRejectedException.MutationsRejectedException ( List, HashMap, Set, Collection, int cause, Throwable cvsList )  {noformat}$$patch1-accumulo-019edb16_Developer_PatchNaturalnessYe$$gh - 454 - Fix iterators for splits. $$1
accumulo-9fcca2ed$$MockTableOperations.tableIdMap always returns tableName as ID$$Noticed and fixed this during ACCUMULO-378.  An exception was thrown unexpectedly when trying to use tableIdMap with a MockInstance. Lift fix from 93c8bddc71d1ee190649eeab263205185d75421c into main tree.$$patch1-accumulo-9fcca2ed_Developer_PatchNaturalnessYe$$add missing import. added tableIdCounter to MockAccumulo .. added missing column to mock table. Fix tableIdMap. gh - 454 - added missing constructor .. add getTableId to MockTable. $$1
accumulo-a3267d3e$$DateLexicoder fails to correctly order dates prior to 1970$$DateLexicoder incorrectly orders dates before 1970 at the end of all other dates.  Therefore, the order was correct for all dates if the user only wrote dates before 1970, or only dates after 1970, but not if they did both.  The DateLexicoder should be fixed to store using a signed LongLexicoder internally, instead of the ULongLexicoder that it used before.$$patch1-accumulo-a3267d3e_Developer_PatchNaturalnessYe$$gh - 66 fixed java sytax error. $$1
accumulo-f2920c26$$Value implementation provides conflicting statements$$The javadoc for the no-arg constructor for {{Value}} states that it "Creates a zero-size sequence." However, the implementation of get will error in this case. {code} public byte[] get() {     if (this.value == null) {       throw new IllegalStateException("Uninitialized. Null constructor " + "called w/o accompanying readFields invocation");     } {code}  Either we need to change the javadoc to be more explicit or change the behaviour of various accessors in the class. I would consider both solutions to be breaking of the API contract since we are changing what clients can expect from us.$$patch1-accumulo-f2920c26_Developer_PatchNaturalnessYe$$add missing import. Allow null - byte values for Value constructor to be constructed with the default constructor .. add note on setting a null value to a Value constructor. Allow null to be initialized on reflection .. $$1
accumulo-31aea2ad$$WAL handling fails to deal with 1.4 -> 1.5 -> 1.6$$After doing a 1.4 -> 1.5 -> 1.6 upgrade that still has WALs for some tables, the 1.6 instance fails to correctly handle the 1.4 recovered WALs.  This can happen either through not waiting long enough after the upgrade to 1.5 or because of an offline table brought online on 1.6 (ala ACCUMULO-2816).$$patch1-accumulo-31aea2ad_Developer_PatchNaturalnessYe$$Fix typo in GarbageCollectWriteAheadLogs. Add newline for server names that are not in the list of servers to remove from the server. add ( ) for uuid - > server mapping. fix error in 1 . 4 WAL recovery script. Fixed WAL files to make it an absolute path for 1 . 4. add count values for walog recovery. log statistics. gh - 1406 Add column qualifiers to logEntries. gh - 661 Tablet does not have meta value. Add doomed logging. Add back missing constructor .. add filename and metaReference to DfsLogger. add getter for meta reference. $$1
accumulo-72fd6bec$$MiniAccumuloConfig doesn't set 0 for monitor log4j port$$MonitorLoggingIT will fail on a host if the monitor is already running because MAC doesn't configure itself to use an ephemeral port. We haven't really noticed this because MAC doesn't start a monitor by default.$$patch1-accumulo-72fd6bec_Developer_PatchNaturalnessYe$$add missing property. $$1
accumulo-3143b9c5$$delete mutations not working through the Proxy$$Aru Sahni writes:  {quote} I'm new to Accumulo and am still trying to wrap my head around its ways. To further that challenge, I'm using Pyaccumulo, which doesn't present much in terms of available reference material.  Right now I'm trying to understand how Accumulo manages record (key-value pair) deletions.  conn = Accumulo(host, port, user, password) table = 'test_table' conn.create_table(table) writer = conn.create_batch_writer(table) mut = Mutation('mut_01') mut.put(cf='item', cq='name', value='car') writer.add_mutation(mut) writer.close() conn.close()  Will generate a record (found via a shell scan):  mut_01 item:name []    car  However the subsequent mutation...  writer = conn.create_batch_writer(table) mut = Mutation('mut_01') mut.put(cf='item', cq='name', is_delete=True) writer.add_mutation(mut) writer.close()  Results in:  mut_01 item:name []  How should one expect the deleted row to be represented? That record sticks around even after I force a compaction of the table.  I was expecting it to not show up in any iterators, or at least provide an easy way to see if the cell has been deleted. {quote}  [~ecn] has confirmed the problem.$$patch1-accumulo-3143b9c5_Developer_PatchNaturalnessYe$$gh - 66 put Delete Delete Delete Delete Delete Delete Delete Delete Delete Delete Delete Delete Delete Delete Delete. $$1
accumulo-17654199$$File never picked up for replication$$I was running some tests and noticed that a single file was getting ignored. The logs were warning that the Status message that was written to {{accumulo.metadata}} didn't have a createdTime on the Status record.  The odd part is that all other Status messages had a createdTime and were successfully replicated. Looking at the writes from the TabletServer logs, the expected record *was* written by the TabletServer, and writing a test with the full series of Status records written does net the correct Status (which was different than what was observed in the actual table).  Looking into it, the log which was subject to this error was the first WAL that was used when the instance was started. Because the table configurations are lazily configured when they are actually used, I believe that the StatusCombiner that is set on {{accumulo.metadata}} was not seen by the TabletServer, and the VersioningIterator "ate" the first record.  I need to come up with a way that I can be sure that all tservers will have seen the Combiner set on accumulo.metadata before any data is written to it to avoid losing a record like this.$$patch1-accumulo-17654199_Developer_PatchNaturalnessYe$$add some code that was accidentally left in the last commit. add missing static modifier to method. add tablet . updatePersistedTime. Added replication log for datafile workflow. add minorCompactionFinished callback. add new datafile to datafileManager. getNumFiles returns actual number of files in the datafile manager. gh - 66 missing import. Add missing import. add missing import. Add missing import. gh - 66 fixed whitespace. Add missing javadoc. Add missing javadoc. Add ZooKeeper metadata related settings , in order to reduce JAR downloading. gh - 661 - update replication table metadata after adding some new WALs. allow mocking of a Writer. replication log expanded. $$1
accumulo-2d97b875$$Validity checks missing for readFields and Thrift deserialization$$Classes in o.a.a.core.data (and potentially elsewhere) that support construction from a Thrift object and/or population from a {{DataInput}} (via a {{readFields()}} method) often lack data validity checks that the classes' constructors enforce. The missing checks make it possible for an attacker to create invalid objects by manipulating the bytes being read. The situation is analogous to the need to check objects deserialized from their Java serialized form within the {{readObject()}} method.$$patch1-accumulo-2d97b875_Developer_PatchNaturalnessYe$$Fix NPEs in Mutation constructor. $$1
accumulo-9339ecf8$$AuthenticationTokenSecretManager might delete key while ZooAuthenticationKeyWatcher enumerates existing keys$$Noticed the following race condition.  The secret manager (in the master) on startup will enumerate the old keys used for creating delegation tokens and delete the keys that are expired.  At the same time, the watcher (in each tserver) might see some updates to these keys and update the secret manager. There's a race condition there that the watcher might try to read a key that the secret manager just deleted.  Need to catch the NoNodeException in the watcher and just accept that it's ok if one of these children are deleted to avoid a scary error in the monitor.$$patch1-accumulo-9339ecf8_Developer_PatchNaturalnessYe$$add missing import. Log expired key warning in ZooAuthenticationKeyWatcher. $$1
accumulo-7ec60f1b$$Incorrect boundry matching for MockTableOperations.deleteRows$$The api for deleteRows specifies: Delete rows between (start, end] but the current implementation for MockTableOperations.deleteRows is implemented as (start, end)  Here is the failing test case  {code:java} public class TestDelete {   private static final String INSTANCE = "mock";   private static final String TABLE = "foo";   private static final String USER = "user";   private static final String PASS = "password";   private static final Authorizations AUTHS = new Authorizations();    @Test   public void testDelete() throws TableNotFoundException, AccumuloException,       AccumuloSecurityException, TableExistsException {      MockInstance mockAcc = new MockInstance(INSTANCE);     Connector conn = mockAcc.getConnector(USER, new PasswordToken(PASS));     conn.tableOperations().create(TABLE);     conn.securityOperations().grantTablePermission(USER, TABLE, TablePermission.READ);     conn.securityOperations().grantTablePermission(USER, TABLE, TablePermission.WRITE);      Mutation mut = new Mutation("2");     mut.put("colfam", "colqual", "value");     BatchWriter writer = conn.createBatchWriter(TABLE, new BatchWriterConfig());     writer.addMutation(mut);      Scanner scan = conn.createScanner(TABLE, AUTHS);     scan.setRange(new Range("2", "2"));      assertEquals(1, countRecords(scan));          // this should delete (1,2]      conn.tableOperations().deleteRows(TABLE, new Text("1"), new Text("2"));      scan = conn.createScanner(TABLE, AUTHS);     scan.setRange(new Range("2", "2"));          // this will fail if row 2 exists     assertEquals(0, countRecords(scan));   }    private int countRecords(Scanner scan) {     int cnt = 0;     for (Entry<Key, Value> entry : scan) {       cnt++;     }     scan.close();     return cnt;   } } {code}$$patch1-accumulo-7ec60f1b_Developer_PatchNaturalnessYe$$add zero byte array to mock table. add zero - length substrings. $$1
accumulo-be2fdba7$$importdirectory failing on split table$$bulk import for the wikisearch example isn't working properly: files are not being assigned to partitions if there are splits.$$patch1-accumulo-be2fdba7_Developer_PatchNaturalnessYe$$gh - 454 - remove debug line. remove maxnum and add proper enum value. gh - 454 - remove unused import. remove unused import. add comment about SortedKeyValueIterator ' s next move. Fix typo in SortedKeyValueIterator javadocs. add missing imports. gh - 45 - added missing import. gh - 375 - added missing final check. gh - 663 - Added debug level for bulk imports. remove too much newline. don ' t delete keys that are no longer used .. gh - 66 bumped up imports. Added debug statement. Improved method to send data to ACU for later processing .. $$1
accumulo-9453bcfa$$MockTable doesn't obey useVersions parameter$$The constructor for {{MockTable}} will call {{IteratorUtil.generateInitialTableProperties()}}, and thus set a versioning iterator on itself regardless of whether the useVersion parameter is set to true or false.   I believe {{MockTable}}'s constructor should call IteratorUtil.generateInitialTableProperties() only if useVersions is true, otherwise, it should populate {{settings}} with a new {{TreeMap}}$$patch1-accumulo-9453bcfa_Developer_PatchNaturalnessYe$$gh - 66 fixed MockTable constructor. gh - 66 - added static method to generateInitialTableProperties ( ). add default iterators. remove the unnecessary empty map. $$1
accumulo-d6472040$$Don't allow viewfs in instance.volumes$$I think one of our folks put viewfs into instance.volumes on accident. File references in accumulo.root and accumulo.metadata were then written with viewfs in the path. The garbage collector then throws errors as compactions occur and it tries delete and move the files to the hdfs users trash directory.  viewfs should never be allowed in instance.volumes property. It should fail.$$patch1-accumulo-d6472040_Developer_PatchNaturalnessYe$$Accidentally log a warning about failing resolve of view fs. Fix check for viewfs volume uri , fail if it doesn ' t look like a URI. Remove try catch block for view fs. $$1
accumulo-11d11e0d$$DefaultLoadBalancer takes a long time when tablets are highly unbalanced$$After creating a thousand splits on a large cluster, I noticed the master was only moving tablets to one server at a time.$$patch1-accumulo-11d11e0d_Developer_PatchNaturalnessYe$$Make ServerCounts package local. Fixed a bug in DefaultLoadBalancer. gh - 454 - Updated default loadBalancer with tooMuch and tooLittle. $$1
accumulo-46f62443$$MockAccumulo doesn't throw informative errors$$Users are unable to tell if an error has occurred and whether it is due to unimplemented features in MockAccumulo.$$patch1-accumulo-46f62443_Developer_PatchNaturalnessYe$$throw exception if table exists. throw NotImplementedException. throw NotImplementedException for getLocalityGroups ( ). throw NotImplementedException. Added throw if table does not exist .. throw exception if table does not exist. throw exception if table does not exist. throw exception if table does not exist. $$1
accumulo-27300d81$$Fix incorrect usage of ByteBuffer$$While working on ACCUMULO-4098 I found one place where ByteBuffer was being used incorrectly.   Looking around the code, I have found other places that are using ByteBuffer incorrectly.  Some of the problems I found are as follows :   * Calling {{ByteBuffer.array()}} without calling {{ByteBuffer.hasArray()}}.  * Using {{ByteBuffer.position()}} or {{ByteBuffer.limit()}} without adding {{ByteBuffer.arrayOffset()}} when dealing with an array returned by {{ByteBuffer.array()}}.  * Using {{ByteBuffer.arrayOffset()}} without adding {{ByteBuffer.position()}} when dealing with an array returned by {{ByteBuffer.array()}}.$$patch1-accumulo-27300d81_Developer_PatchNaturalnessYe$$gh - 66 fixed ArrayByteSequence .. gh - 668 - added support for array access in ArrayByteSequence constructor. Add toString ( ) to ArrayByteSequence. Fix bug in ByteBufferUtil. Fix toText ( ByteBuffer ) where byteBuffer . hasArray ( ) is false. gh - 454 - patched ByteBufferUtil # convertToArray to use toArray. gh - 122 - Synchronization buffer use of ByteBufferUtil . toBytes ( ). $$1
accumulo-a2c2d38a$$ConditionalWriterIT is failing$$I noticed that the ConditionalWriterIT was failing in master.   Using the following command with {{git bisect}} I tracked it down to commit {{3af75fc}} for ACCUMULO-4077 as the change which broke the IT.  Have not looked into why its failing yet.  {noformat} mvn clean verify -Dit.test=ConditionalWriterIT -Dfindbugs.skip -Dcheckstyle.skip -Dtest=foo -DfailIfNoTests=false {noformat}$$patch1-accumulo-a2c2d38a_Developer_PatchNaturalnessYe$$Fixed a bug in UnsynchronizedBuffer where the array was not synchronized .. $$1
accumulo-a450ac2f$$MockBatchScanner inappropriately filters on ranges$$I believe I have a legitimate case where an iterator will return something outside of the seeked-to range.  This appears to work in a live system, but fails to work in test cases using the MockBatchScanner.  I believe this is because the MockBatchScanner filters on the supplied ranges in addition to seeking the iterators to each range.  Either we need to remove this range filter, or fix the real system to do the same thing.  I prefer the former of course.$$patch1-accumulo-a450ac2f_Developer_PatchNaturalnessYe$$add missing import. add deepCopy to RangesFilter constructor. Fix typo in MockBatchScanner. add missing import. $$1
accumulo-a5e3ed3b$$Validity checks missing for readFields and Thrift deserialization$$Classes in o.a.a.core.data (and potentially elsewhere) that support construction from a Thrift object and/or population from a {{DataInput}} (via a {{readFields()}} method) often lack data validity checks that the classes' constructors enforce. The missing checks make it possible for an attacker to create invalid objects by manipulating the bytes being read. The situation is analogous to the need to check objects deserialized from their Java serialized form within the {{readObject()}} method.$$patch1-accumulo-a5e3ed3b_Developer_PatchNaturalnessYe$$add check for null fields in Key. $$1
accumulo-6138a80f$$Instance secret written out with other configuration items to RFiles and WALogs when encryption is turned on$$The encryption at rest feature records configuration information in order to encrypted RFiles and WALogs so that if the configuration changes, the files can be read back.  The code that does this recording hovers up all the "instance.*" entries, and does not pick out the instance.secret as a special one not to write.  Thus the instance secret goes into each file in the clear, which is non-ideal to say the least.  Patch forthcoming.$$patch1-accumulo-6138a80f_Developer_PatchNaturalnessYe$$remove instance secret key from crypto module factory. $$1
accumulo-8dad5e0f$$FirstEntryInRowIterator is broken and has no test$$In 1.4 and trunk, the iterator throws a NullPointerException when seeked.  In 1.3 the iterator runs, but there is a question as to what it should do when it is seeked to the middle of a row.  Currently, it returns the first key found within the range.  I believe this should be changed to ignore the remaining portion of that row and return the first key of the next row.  Should this change be made in 1.3, or should I leave it as is and just change it in 1.4 and greater?$$patch1-accumulo-8dad5e0f_Developer_PatchNaturalnessYe$$gh - 454 - Skipping first entry in row iterator with null last row found. do not consume last row found if we don ' t have source to last entry in range. $$1
flink-e494c279$$KeySelectorUtil.getSelectorForKeys and TypeExtractor.getKeySelectorTypes are incompatible$$The following code snippet fails, because {{KeySelectorUtil.getSelectorForKeys}} returns the base {{Tuple}} type.  ```java TypeInformation<Tuple2<Integer, Integer>> typeInfo = TypeExtractor .getForObject(Tuple2.of(0, 0));  ExecutionConfig config = new ExecutionConfig();  KeySelector<Tuple2<Integer, Integer>, ?> keySelector = KeySelectorUtil.getSelectorForKeys( new Keys.ExpressionKeys<>(new int[]{0}, typeInfo), typeInfo, config);  // fails with InvalidTypesException TypeExtractor.getKeySelectorTypes(keySelector, typeInfo);  ```  However if I manually define the key selector as follows the snippet works fine due to the key type being an integer.  ```java KeySelector<Tuple2<Integer, Integer>, Integer> keySelector =  new KeySelector<Tuple2<Integer, Integer>, Integer>() { 	@Override 	public Integer getKey(Tuple2<Integer, Integer> value) throws Exception { 		return value.f0; 	} }; ```  The error message looks like this: org.apache.flink.api.common.functions.InvalidTypesException: Usage of class Tuple as a type is not allowed. Use a concrete subclass (e.g. Tuple1, Tuple2, etc.) instead. 	at org.apache.flink.api.java.typeutils.TypeExtractor.createTypeInfoWithTypeHierarchy(TypeExtractor.java:401) 	at org.apache.flink.api.java.typeutils.TypeExtractor.privateCreateTypeInfo(TypeExtractor.java:379) 	at org.apache.flink.api.java.typeutils.TypeExtractor.getUnaryOperatorReturnType(TypeExtractor.java:279) 	at org.apache.flink.api.java.typeutils.TypeExtractor.getKeySelectorTypes(TypeExtractor.java:229) 	at org.apache.flink.api.java.typeutils.TypeExtractor.getKeySelectorTypes(TypeExtractor.java:223)$$patch1-flink-e494c279_Developer_PatchNaturalnessYe$$Add missing import. Fix copy / paste error. Fix typo in KeySelectorUtil. Add missing constructor .. Fix copy / paste error. $$1
flink-8dc70f2e$$Optimizer does not push properties out of bulk iterations$$Flink's optimizer should be able to reuse interesting properties from outside the loop. In order to do that it is sometimes necessary to append a NoOp node to the step function which recomputes the required properties.  This is currently not working for {{BulkIterations}}, because the plans with the appended NoOp nodes are not added to the overall list of candidates.  This not only leads to sub-optimal plan selection but sometimes to the rejection of valid jobs. The following job, for example, will be falsely rejected by flink.  {code} ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();  		DataSet<Tuple1<Long>> input1 = env.generateSequence(1, 10).map(new MapFunction<Long, Tuple1<Long>>() { 			@Override 			public Tuple1<Long> map(Long value) throws Exception { 				return new Tuple1<>(value); 			} 		});  		DataSet<Tuple1<Long>> input2 = env.generateSequence(1, 10).map(new MapFunction<Long, Tuple1<Long>>() { 			@Override 			public Tuple1<Long> map(Long value) throws Exception { 				return new Tuple1<>(value); 			} 		});  		DataSet<Tuple1<Long>> distinctInput = input1.distinct();  		IterativeDataSet<Tuple1<Long>> iteration = distinctInput.iterate(10);  		DataSet<Tuple1<Long>> iterationStep = iteration 				.coGroup(input2) 				.where(0) 				.equalTo(0) 				.with(new CoGroupFunction<Tuple1<Long>, Tuple1<Long>, Tuple1<Long>>() { 					@Override 					public void coGroup( 							Iterable<Tuple1<Long>> first, 							Iterable<Tuple1<Long>> second, 							Collector<Tuple1<Long>> out) throws Exception { 						Iterator<Tuple1<Long>> it = first.iterator();  						if (it.hasNext()) { 							out.collect(it.next()); 						} 					} 				});  		DataSet<Tuple1<Long>> iterationResult = iteration.closeWith(iterationStep);  		iterationResult.output(new DiscardingOutputFormat<Tuple1<Long>>()); {code}$$patch1-flink-8dc70f2e_Developer_PatchNaturalnessYe$$make constructor public. Remove unused imports. Add missing import. Fix newly introduced warnings in BulkIterationNode .. Fix a bug in BulkIterationNode. Fix a bug in BulkIterationNode. Add missing import. Suppress unchecked warning. Fix a bug in the workflow of workflow properties generation. Add missing import. Fixed a bug in UnaryOperatorNode constructor .. Fix copy and paste error in UnaryOperatorNode constructor. $$1
flink-734ba01d$$Cancelling a running job can lead to restart instead of stopping$$I just tried cancelling a regularly running job. Instead of the job stopping, it restarted.   {code} 2016-02-29 10:39:28,415 INFO  org.apache.flink.yarn.YarnJobManager                          - Trying to cancel job with ID 5c0604694c8469cfbb89daaa990068df. 2016-02-29 10:39:28,416 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Source: Out of order data generator -> (Flat Map, Timestamps/Watermarks) (1/1) (e3b05555ab0e373defb925898de9f200) switched from RUNNING to CANCELING .... 2016-02-29 10:39:28,488 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - TriggerWindow(TumblingTimeWindows(60000), FoldingStateDescriptor{name=window-contents, defaultValue=(0,9223372036854775807,0), serializer=null}, EventTimeTrigger(), WindowedStream.apply(WindowedStream.java:397)) (19/24) (c1be31b0be596d2521073b2d78ffa60a) switched from CANCELING to CANCELED 2016-02-29 10:40:08,468 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Source: Out of order data generator -> (Flat Map, Timestamps/Watermarks) (1/1) (e3b05555ab0e373defb925898de9f200) switched from CANCELING to FAILED 2016-02-29 10:40:08,468 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - TriggerWindow(TumblingTimeWindows(60000), FoldingStateDescriptor{name=window-contents, defaultValue=(0,9223372036854775807,0), serializer=null}, EventTimeTrigger(), WindowedStream.apply(WindowedStream.java:397)) (1/24) (5ad172ec9932b24d5a98377a2c82b0b3) switched from CANCELING to FAILED 2016-02-29 10:40:08,472 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - TriggerWindow(TumblingTimeWindows(60000), FoldingStateDescriptor{name=window-contents, defaultValue=(0,9223372036854775807,0), serializer=null}, EventTimeTrigger(), WindowedStream.apply(WindowedStream.java:397)) (2/24) (5404ca28ac7cf23b67dff30ef2309078) switched from CANCELING to FAILED 2016-02-29 10:40:08,473 INFO  org.apache.flink.yarn.YarnJobManager                          - Status of job 5c0604694c8469cfbb89daaa990068df (Event counter: {auto.offset.reset=earliest, rocksdb=hdfs:///user/robert/rocksdb, generateInPlace=soTrue, parallelism=24, bootstrap.servers=cdh544-worker-0:9092, topic=eventsGenerator, eventsPerKeyPerGenerator=2, numKeys=1000000000, zookeeper.connect=cdh544-worker-0:2181, timeSliceSize=60000, eventsKerPey=1, genPar=1}) changed to FAILING. java.lang.Exception: Task could not be canceled. 	at org.apache.flink.runtime.executiongraph.Execution 5.onComplete(Execution.java:902) 	at akka.dispatch.OnComplete.internal(Future.scala:246) 	at akka.dispatch.OnComplete.internal(Future.scala:244) 	at akka.dispatch.japi CallbackBridge.apply(Future.scala:174) 	at akka.dispatch.japi CallbackBridge.apply(Future.scala:171) 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32) 	at scala.concurrent.impl.ExecutionContextImpl  anon 3.exec(ExecutionContextImpl.scala:107) 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) 	at scala.concurrent.forkjoin.ForkJoinPool WorkQueue.runTask(ForkJoinPool.java:1339) 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka.tcp://flink@10.240.242.143:50119/user/taskmanager#640539146]] after [10000 ms] 	at akka.pattern.PromiseActorRef  anonfun 1.apply mcV sp(AskSupport.scala:333) 	at akka.actor.Scheduler  anon 7.run(Scheduler.scala:117) 	at scala.concurrent.Future InternalCallbackExecutor .scala concurrent Future InternalCallbackExecutor  unbatchedExecute(Future.scala:694) 	at scala.concurrent.Future InternalCallbackExecutor .execute(Future.scala:691) 	at akka.actor.LightArrayRevolverScheduler TaskHolder.executeTask(Scheduler.scala:467) 	at akka.actor.LightArrayRevolverScheduler  anon 8.executeBucket 1(Scheduler.scala:419) 	at akka.actor.LightArrayRevolverScheduler  anon 8.nextTick(Scheduler.scala:423) 	at akka.actor.LightArrayRevolverScheduler  anon 8.run(Scheduler.scala:375) 	at java.lang.Thread.run(Thread.java:745) 2016-02-29 10:40:08,477 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - TriggerWindow(TumblingTimeWindows(60000), FoldingStateDescriptor{name=window-contents, defaultValue=(0,9223372036854775807,0), serializer=null}, EventTimeTrigger(), WindowedStream.apply(WindowedStream.java:397)) (3/24) (fc527d65ec8df3ccf68f882d968e776e) switched from CANCELING to FAILED 2016-02-29 10:40:08,487 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - TriggerWindow(TumblingTimeWindows(60000), FoldingStateDescriptor{name=window-contents, defaultValue=(0,9223372036854775807,0), serializer=null}, EventTimeTrigger(), WindowedStream.apply(WindowedStream.java:397)) (4/24) (afb1aa3c2d8acdee0f138cf344238e4e) switched from CANCELING to FAILED 2016-02-29 10:40:08,488 INFO  org.apache.flink.runtime.executiongraph.restart.FixedDelayRestartStrategy  - Delaying retry of job execution for 3000 ms ... 2016-02-29 10:40:08,488 INFO  org.apache.flink.yarn.YarnJobManager                          - Status of job 5c0604694c8469cfbb89daaa990068df (Event counter: {auto.offset.reset=earliest, rocksdb=hdfs:///user/robert/rocksdb, generateInPlace=soTrue, parallelism=24, bootstrap.servers=cdh544-worker-0:9092, topic=eventsGenerator, eventsPerKeyPerGenerator=2, numKeys=1000000000, zookeeper.connect=cdh544-worker-0:2181, timeSliceSize=60000, eventsKerPey=1, genPar=1}) changed to RESTARTING. 2016-02-29 10:40:11,490 INFO  org.apache.flink.yarn.YarnJobManager                          - Status of job 5c0604694c8469cfbb89daaa990068df (Event counter: {auto.offset.reset=earliest, rocksdb=hdfs:///user/robert/rocksdb, generateInPlace=soTrue, parallelism=24, bootstrap.servers=cdh544-worker-0:9092, topic=eventsGenerator, eventsPerKeyPerGenerator=2, numKeys=1000000000, zookeeper.connect=cdh544-worker-0:2181, timeSliceSize=60000, eventsKerPey=1, genPar=1}) changed to CREATED. 2016-02-29 10:40:11,490 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Source: Out of order data generator -> (Flat Map, Timestamps/Watermarks) (1/1) (1319b2f44d78d99948ffde4350c052d9) switched from CREATED to SCHEDULED 2016-02-29 10:40:11,490 INFO  org.apache.flink.yarn.YarnJobManager                          - Status of job 5c0604694c8469cfbb89daaa990068df (Event counter: {auto.offset.reset=earliest, rocksdb=hdfs:///user/robert/rocksdb, generateInPlace=soTrue, parallelism=24, bootstrap.servers=cdh544-worker-0:9092, topic=eventsGenerator, eventsPerKeyPerGenerator=2, numKeys=1000000000, zookeeper.connect=cdh544-worker-0:2181, timeSliceSize=60000, eventsKerPey=1, genPar=1}) changed to RUNNING. {code}$$patch1-flink-734ba01d_Developer_PatchNaturalnessYe$$Don ' t call cancelingComplete ( ) on failed executions. Don ' t mark terminal state as failed or terminal state .. $$1
flink-d033fa8f$$Allow KeySelectors to implement ResultTypeQueryable$$See https://github.com/apache/flink/pull/354$$patch1-flink-d033fa8f_Developer_PatchNaturalnessYe$$Fix TypeExtractor . validateInputType ( ). $$1
flink-32a003d5$$NullPointerException while translating union node$$The NepheleJobGraphGenerator throws a NullPointerException when translating a binary union operator. The BinaryUnionPlanNode is not replaced by a NAryUnionPlanNode and thus is still treated as a DualInputVertex. Accessing the driver code of the BinaryUnionPlanNode causes then the NullPointerException.$$patch1-flink-32a003d5_Developer_PatchNaturalnessYe$$Added missing @ Override annotation. Fix a bug in the union compiler. $$1
flink-fb7ce0e3$$Bug in PojoSerializer's copy() method$$The PojoSerializer's {{copy()}} method does not work properly with {{null}} values. An exception could look like:  {code} Caused by: java.io.IOException: Thread 'SortMerger spilling thread' terminated due to an exception: null 	at org.apache.flink.runtime.operators.sort.UnilateralSortMerger ThreadBase.run(UnilateralSortMerger.java:792) Caused by: java.io.EOFException 	at org.apache.flink.runtime.io.disk.RandomAccessInputView.nextSegment(RandomAccessInputView.java:83) 	at org.apache.flink.runtime.memorymanager.AbstractPagedInputView.advance(AbstractPagedInputView.java:159) 	at org.apache.flink.runtime.memorymanager.AbstractPagedInputView.readByte(AbstractPagedInputView.java:270) 	at org.apache.flink.runtime.memorymanager.AbstractPagedInputView.readUnsignedByte(AbstractPagedInputView.java:277) 	at org.apache.flink.types.StringValue.copyString(StringValue.java:839) 	at org.apache.flink.api.common.typeutils.base.StringSerializer.copy(StringSerializer.java:83) 	at org.apache.flink.api.java.typeutils.runtime.PojoSerializer.copy(PojoSerializer.java:261) 	at org.apache.flink.runtime.operators.sort.NormalizedKeySorter.writeToOutput(NormalizedKeySorter.java:449) 	at org.apache.flink.runtime.operators.sort.UnilateralSortMerger SpillingThread.go(UnilateralSortMerger.java:1303) 	at org.apache.flink.runtime.operators.sort.UnilateralSortMerger ThreadBase.run(UnilateralSortMerger.java:788) {code}  I'm working on a fix for that...$$patch1-flink-fb7ce0e3_Developer_PatchNaturalnessYe$$Fixed a bug in PojoSerializer. fix a crash in POJO serializer. Fixed corresponding write of boolean fields .. $$1
flink-02c08456$$Type extractor cannot determine type of function$$This function fails in the type extractor.  {code} public static final class DuplicateValue<T> implements MapFunction<Tuple1<T>, Tuple2<T, T>> { 		 	@Override 	public Tuple2<T, T> map(Tuple1<T> vertex) { 		return new Tuple2<T, T>(vertex.f0, vertex.f0); 	} } {code}$$patch1-flink-02c08456_Developer_PatchNaturalnessYe$$Fix TypeExtractor. $$1
flink-9cd96df7$$Void is not added to TypeInfoParser$$List l = Arrays.asList(new Tuple2<Void,Long>(null, 1L)); TypeInformation t = TypeInfoParser.parse("Tuple2<Void,Long>"); DataSet<Tuple2<Void,Long>> data = env.fromCollection(l, t); data.print(); Throws: Exception in thread "main" java.lang.IllegalArgumentException: String could not be parsed: Class 'Void' could not be found for use as custom object. Please note that inner classes must be declared static. at org.apache.flink.api.java.typeutils.TypeInfoParser.parse(TypeInfoParser.java:90) at org.apache.flink.hadoopcompatibility.mapreduce.example.ParquetOutput.main(ParquetOutput.java:92) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at com.intellij.rt.execution.application.AppMain.main(AppMain.java:134) Caused by: java.lang.IllegalArgumentException: Class 'Void' could not be found for use as custom object. Please note that inner classes must be declared static. at org.apache.flink.api.java.typeutils.TypeInfoParser.parse(TypeInfoParser.java:290) at org.apache.flink.api.java.typeutils.TypeInfoParser.parse(TypeInfoParser.java:133) at org.apache.flink.api.java.typeutils.TypeInfoParser.parse(TypeInfoParser.java:88) ... 6 more$$patch1-flink-9cd96df7_Developer_PatchNaturalnessYe$$Fix regexp for ( java \\ . lang \\ . ). Added date parser for structure changes. Fixed infoParser for Date and DateTime special cases. Fixed infoParser . java. $$1
flink-117ba95f$$Checkpoint stats show ghost numbers$$[~StephanEwen] reported an issue with the display of checkpoint stats. A pipeline with a stateful source and stateless intermediate operator shows stats for the stateless intermediate operator. The numbers are most likely the same as for the source operator.$$patch1-flink-117ba95f_Developer_PatchNaturalnessYe$$Fix new operator stats for SimpleCheckpointStatsTracker. $$1
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch1-flink-45fb6d82_Developer_PatchNaturalnessYe$$Fix warning. Partitioned coGroupDescriptor from com . gdx . tools . rmb938 to com. add missing import. add custom partitioner to join descriptor. $$1
flink-e3759a5e$$CEP operator does not forward watermarks properly$$The CEP stream operator don't emit a proper watermark when using event time.$$patch1-flink-e3759a5e_Developer_PatchNaturalnessYe$$Fix keyed CEP pattern operator emitWatermark. emit watermark on stream events. $$1
flink-8b40bb7a$$ArrayKeySelector returns wrong positions (or fails)$$The {{ArrayKeySelector}} is broken and returns wrong values in all cases except for [0] as a single only key position.$$patch1-flink-8b40bb7a_Developer_PatchNaturalnessYe$$KeyedDataStream made KeySelector protected. Remove redundant code. Remove unused variable. Remove unused imports. Fix copy / paste error. Make ArrayKeySelector final. Fix copy / paste error in KeySelectorUtil. $$1
flink-0a4c7694$$Events at unitialized input channels are lost$$If a program sends an event backwards to the producer task, it might happen that some of it input channels have not been initialized yet (UnknownInputChannel). In that case, the events are lost and will never be received at the producer.$$patch1-flink-0a4c7694_Developer_PatchNaturalnessYe$$Missing import. Add uninitialized fields to BufferReader , to allow for cleaner dom implementation of deferred loading .. Fix BufferReader . hasInputChannelWithData ( ). check for unknown input channel. Don ' t send uninitialized tasks in BufferReader , closes # 1796. Don ' t send uninitialized events in BufferReader .. $$1
flink-44061882$$Invalid execution graph cleanup for jobs with colocation groups$$Currently, upon restarting an execution graph, we clean-up the colocation constraints for each group present in an ExecutionJobVertex respectively.  This can lead to invalid reconfiguration upon a restart or any other activity that relies on state cleanup of the execution graph. For example, upon restarting a DataStream job with iterations the following steps are executed:  1) IterationSource colgroup constraints are reset 2) IterationSource execution vertices reset and create new colocation constraints 3) IterationSink colgroup constraints are reset 4) IterationSink execution vertices reset and create different colocation constraints.  This can be trivially fixed by reseting colocation groups independently from ExecutionJobVertices, thus, updating them once per reconfiguration.$$patch1-flink-44061882_Developer_PatchNaturalnessYe$$Add missing import. Added missing imports. add comment for intermediate results in ExecutionGraph. Added missing vertex in creation order. reset colocation groups on first execution. remove coLocationGroup reset constraint from ExecutionJobVertex. $$1
flink-380ef878$$IndexOutOfBoundsException when receiving empty buffer at remote channel$$Receiving buffers from remote input channels with size 0 results in an {{IndexOutOfBoundsException}}.  {code} Caused by: java.lang.IndexOutOfBoundsException: index: 30 (expected: range(0, 30)) 	at io.netty.buffer.AbstractByteBuf.checkIndex(AbstractByteBuf.java:1123) 	at io.netty.buffer.PooledUnsafeDirectByteBuf.getBytes(PooledUnsafeDirectByteBuf.java:156) 	at io.netty.buffer.PooledUnsafeDirectByteBuf.getBytes(PooledUnsafeDirectByteBuf.java:151) 	at io.netty.buffer.SlicedByteBuf.getBytes(SlicedByteBuf.java:179) 	at io.netty.buffer.AbstractByteBuf.readBytes(AbstractByteBuf.java:717) 	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientHandler.decodeBufferOrEvent(PartitionRequestClientHandler.java:205) 	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientHandler.decodeMsg(PartitionRequestClientHandler.java:164) 	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientHandler.channelRead(PartitionRequestClientHandler.java:118) {code}$$patch1-flink-380ef878_Developer_PatchNaturalnessYe$$Fix possible NPE if PartitionRequestClientHandler context is null. PartitionRequestClientHandler should honor empty buffers for read ( ). Fix checkstyle. $$1
flink-495a5c3c$$Streaming does not correctly forward ExecutionConfig to runtime$$When running streaming jobs you see this log entry: "Environment did not contain an ExecutionConfig - using a default config."  Some parts of the code use an ExecutionConfig at runtime. This will be a default config without registered serializers and other user settings.$$patch1-flink-495a5c3c_Developer_PatchNaturalnessYe$$Add missing import. Add missing import. Add missing import. Fix StreamingJobGraphGenerator # exportJobGraph to throw exception. $$1
flink-3e233a38$$Set state checkpointer before default state for PartitionedStreamOperatorState$$Currently the default state is set before the passed StateCheckpointer instance for operator states.  What currently happens because of this is that the default value is serialized with Java serialization and then deserialized on the opstate.value() call using the StateCheckpointer most likely causing a failure.  This can be trivially fixed by swaping the order of the 2 calls.$$patch1-flink-3e233a38_Developer_PatchNaturalnessYe$$Add back missing checkpointer setting. $$1
flink-27e40205$$Type extractor cannot determine type of function$$This function fails in the type extractor.  {code} public static final class DuplicateValue<T> implements MapFunction<Tuple1<T>, Tuple2<T, T>> { 		 	@Override 	public Tuple2<T, T> map(Tuple1<T> vertex) { 		return new Tuple2<T, T>(vertex.f0, vertex.f0); 	} } {code}$$patch1-flink-27e40205_Developer_PatchNaturalnessYe$$Fix TypeExtractor. Fix TypeExtractor. Fix generics warning. Fix TypeExtractor . createTypeInfoFromInputs ( ). Fix TypeExtractor from javadoc. $$1
flink-8f321c72$$FileOutputFormat writes to wrong path if path ends with '/'$$The FileOutputFormat duplicates the last directory of a path, if the path ends  with a slash '/'. For example, if the output path is specified as {{/home/myuser/outputPath/}} the output is written to {{/home/myuser/outputPath/outputPath/}}.  This bug was introduced by commit 8fc04e4da8a36866e10564205c3f900894f4f6e0$$patch1-flink-8f321c72_Developer_PatchNaturalnessYe$$Add comment about Path class .. Fix checkstyle. Fix checkAndTrimPathArg ( ). Trim path string arguments before constructor .. Don ' t validate path arguments when constructor is called .. Don ' t remove trailing separators in Path . normalizePath ( ). Remove backslash which prevents path separators from being read by Flink. $$1
flink-5308ac83$$InstanceConnectionInfo returns wrong hostname when no DNS entry exists$$If there is no DNS entry for an address (like 10.4.122.43), then the {{InstanceConnectionInfo}} returns the first octet ({{10}}) as the hostame.$$patch1-flink-5308ac83_Developer_PatchNaturalnessYe$$Don ' t include host name in instanceConnectionInfo. Fixed misc problems with variable name field. Log warning about no hostname in instance connection info. Remove dead code. Fixed readability of instanceconnectioninfo. Remove unused boolean write. $$1
flink-76bebd42$$MutableHashTable fails when spilling partitions without overflow segments$$When one performs a join operation with many and large records then the join operation fails with the following exception when it tries to spill a {{HashPartition}}.  {code} java.lang.RuntimeException: Bug in Hybrid Hash Join: Request to spill a partition with less than two buffers. 	at org.apache.flink.runtime.operators.hash.HashPartition.spillPartition(HashPartition.java:302) 	at org.apache.flink.runtime.operators.hash.MutableHashTable.spillPartition(MutableHashTable.java:1108) 	at org.apache.flink.runtime.operators.hash.MutableHashTable.nextSegment(MutableHashTable.java:1277) 	at org.apache.flink.runtime.operators.hash.HashPartition BuildSideBuffer.nextSegment(HashPartition.java:524) 	at org.apache.flink.runtime.memory.AbstractPagedOutputView.advance(AbstractPagedOutputView.java:140) 	at org.apache.flink.runtime.memory.AbstractPagedOutputView.write(AbstractPagedOutputView.java:201) 	at org.apache.flink.runtime.memory.AbstractPagedOutputView.write(AbstractPagedOutputView.java:178) 	at org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.serialize(BytePrimitiveArraySerializer.java:74) 	at org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.serialize(BytePrimitiveArraySerializer.java:30) 	at org.apache.flink.runtime.operators.hash.HashPartition.insertIntoBuildBuffer(HashPartition.java:257) 	at org.apache.flink.runtime.operators.hash.MutableHashTable.insertIntoTable(MutableHashTable.java:856) 	at org.apache.flink.runtime.operators.hash.MutableHashTable.buildInitialTable(MutableHashTable.java:685) 	at org.apache.flink.runtime.operators.hash.MutableHashTable.open(MutableHashTable.java:443) 	at org.apache.flink.runtime.operators.hash.HashTableTest.testSpillingWhenBuildingTableWithoutOverflow(HashTableTest.java:234) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at org.junit.runners.model.FrameworkMethod 1.runReflectiveCall(FrameworkMethod.java:47) 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44) 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271) 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70) 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50) 	at org.junit.runners.ParentRunner 3.run(ParentRunner.java:238) 	at org.junit.runners.ParentRunner 1.schedule(ParentRunner.java:63) 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236) 	at org.junit.runners.ParentRunner.access 000(ParentRunner.java:53) 	at org.junit.runners.ParentRunner 2.evaluate(ParentRunner.java:229) 	at org.junit.runners.ParentRunner.run(ParentRunner.java:309) 	at org.junit.runner.JUnitCore.run(JUnitCore.java:160) 	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:78) 	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:212) 	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:68) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:140) {code}  The reason is that the {{HashPartition}} does not include the number of used memory segments by the {{BuildSideBuffer}} when it counts the currently occupied memory segments.$$patch1-flink-76bebd42_Developer_PatchNaturalnessYe$$Fix default implementation of HashPartition . getNumOccupiedMemorySegments. Add getNumOccupiedMemorySegments ( ) to HashPartition. $$1
flink-af477563$$Bug in Hybrid Hash Join: Request to spill a partition with less than two buffers.$$The following exception is thrown when running the example triangle listing with an unmodified master build (4cadc3d6).  {noformat} ./bin/flink run ~/flink-examples/flink-java-examples/target/flink-java-examples-0.10-SNAPSHOT-EnumTrianglesOpt.jar ~/rmat/undirected/s19_e8.ssv output {noformat}  The only changes to {{flink-conf.yaml}} are {{taskmanager.numberOfTaskSlots: 8}} and {{parallelism.default: 8}}.  I have confirmed with input files [s19_e8.ssv|https://drive.google.com/file/d/0B6TrSsnHj2HxR2lnMHR4amdyTnM/view?usp=sharing] (40 MB) and [s20_e8.ssv|https://drive.google.com/file/d/0B6TrSsnHj2HxNi1HbmptU29MTm8/view?usp=sharing] (83 MB). On a second machine only the larger file caused the exception.  {noformat} org.apache.flink.client.program.ProgramInvocationException: The program execution failed: Job execution failed. 	at org.apache.flink.client.program.Client.runBlocking(Client.java:407) 	at org.apache.flink.client.program.Client.runBlocking(Client.java:386) 	at org.apache.flink.client.program.Client.runBlocking(Client.java:353) 	at org.apache.flink.client.program.ContextEnvironment.execute(ContextEnvironment.java:64) 	at org.apache.flink.examples.java.graph.EnumTrianglesOpt.main(EnumTrianglesOpt.java:125) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.lang.reflect.Method.invoke(Method.java:497) 	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:434) 	at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:350) 	at org.apache.flink.client.program.Client.runBlocking(Client.java:290) 	at org.apache.flink.client.CliFrontend.executeProgramBlocking(CliFrontend.java:675) 	at org.apache.flink.client.CliFrontend.run(CliFrontend.java:324) 	at org.apache.flink.client.CliFrontend.parseParameters(CliFrontend.java:977) 	at org.apache.flink.client.CliFrontend.main(CliFrontend.java:1027) Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed. 	at org.apache.flink.runtime.jobmanager.JobManager  anonfun handleMessage 1.applyOrElse(JobManager.scala:425) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply mcVL sp(AbstractPartialFunction.scala:33) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:33) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:25) 	at org.apache.flink.runtime.LeaderSessionMessageFilter  anonfun receive 1.applyOrElse(LeaderSessionMessageFilter.scala:36) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply mcVL sp(AbstractPartialFunction.scala:33) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:33) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:25) 	at org.apache.flink.runtime.LogMessages  anon 1.apply(LogMessages.scala:33) 	at org.apache.flink.runtime.LogMessages  anon 1.apply(LogMessages.scala:28) 	at scala.PartialFunction class.applyOrElse(PartialFunction.scala:118) 	at org.apache.flink.runtime.LogMessages  anon 1.applyOrElse(LogMessages.scala:28) 	at akka.actor.Actor class.aroundReceive(Actor.scala:465) 	at org.apache.flink.runtime.jobmanager.JobManager.aroundReceive(JobManager.scala:107) 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516) 	at akka.actor.ActorCell.invoke(ActorCell.scala:487) 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:254) 	at akka.dispatch.Mailbox.run(Mailbox.scala:221) 	at akka.dispatch.Mailbox.exec(Mailbox.scala:231) 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) 	at scala.concurrent.forkjoin.ForkJoinPool WorkQueue.runTask(ForkJoinPool.java:1339) 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) Caused by: java.lang.RuntimeException: Bug in Hybrid Hash Join: Request to spill a partition with less than two buffers. 	at org.apache.flink.runtime.operators.hash.HashPartition.spillPartition(HashPartition.java:288) 	at org.apache.flink.runtime.operators.hash.MutableHashTable.spillPartition(MutableHashTable.java:1108) 	at org.apache.flink.runtime.operators.hash.MutableHashTable.insertBucketEntry(MutableHashTable.java:934) 	at org.apache.flink.runtime.operators.hash.MutableHashTable.insertIntoTable(MutableHashTable.java:859) 	at org.apache.flink.runtime.operators.hash.MutableHashTable.buildTableFromSpilledPartition(MutableHashTable.java:819) 	at org.apache.flink.runtime.operators.hash.MutableHashTable.prepareNextPartition(MutableHashTable.java:517) 	at org.apache.flink.runtime.operators.hash.MutableHashTable.nextRecord(MutableHashTable.java:556) 	at org.apache.flink.runtime.operators.hash.NonReusingBuildFirstHashMatchIterator.callWithNextKey(NonReusingBuildFirstHashMatchIterator.java:104) 	at org.apache.flink.runtime.operators.JoinDriver.run(JoinDriver.java:208) 	at org.apache.flink.runtime.operators.RegularPactTask.run(RegularPactTask.java:489) 	at org.apache.flink.runtime.operators.RegularPactTask.invoke(RegularPactTask.java:354) 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:579) 	at java.lang.Thread.run(Thread.java:745) {noformat}$$patch1-flink-af477563_Developer_PatchNaturalnessYe$$Added getter for getNumOccupiedMemorySegments. Fix bug in Hybrid Hash Partition .. Fixed the hash table size .. $$1
flink-a56aad74$$Race leading to IndexOutOfBoundsException when querying for buffer while releasing SpillablePartition$$When running a code as simple as:   {noformat} 		ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();  		DataSet<Edge<String, NullValue>> edges = getEdgesDataSet(env); 		Graph<String, NullValue, NullValue> graph = Graph.fromDataSet(edges, env);  		DataSet<Tuple2<String, Long>> degrees = graph.getDegrees(); degrees.writeAsCsv(outputPath, "\n", " "); 			env.execute();  on the Freindster data set: https://snap.stanford.edu/data/com-Friendster.html; on 30 Wally nodes   I get the following exception: java.lang.Exception: The data preparation for task 'CoGroup (CoGroup at inDegrees(Graph.java:701))' , caused an error: Error obtaining the sorted input: Thread 'SortMerger Reading Thread' terminated due to an exception: Fatal error at remote task manager 'wally028.cit.tu-berlin.de/130.149.249.38:53730'. 	at org.apache.flink.runtime.operators.RegularPactTask.run(RegularPactTask.java:471) 	at org.apache.flink.runtime.operators.RegularPactTask.invoke(RegularPactTask.java:362) 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:559) 	at java.lang.Thread.run(Thread.java:722) Caused by: java.lang.RuntimeException: Error obtaining the sorted input: Thread 'SortMerger Reading Thread' terminated due to an exception: Fatal error at remote task manager 'wally028.cit.tu-berlin.de/130.149.249.38:53730'. 	at org.apache.flink.runtime.operators.sort.UnilateralSortMerger.getIterator(UnilateralSortMerger.java:607) 	at org.apache.flink.runtime.operators.RegularPactTask.getInput(RegularPactTask.java:1145) 	at org.apache.flink.runtime.operators.CoGroupDriver.prepare(CoGroupDriver.java:98) 	at org.apache.flink.runtime.operators.RegularPactTask.run(RegularPactTask.java:466) 	... 3 more Caused by: java.io.IOException: Thread 'SortMerger Reading Thread' terminated due to an exception: Fatal error at remote task manager 'wally028.cit.tu-berlin.de/130.149.249.38:53730'. 	at org.apache.flink.runtime.operators.sort.UnilateralSortMerger ThreadBase.run(UnilateralSortMerger.java:784) Caused by: org.apache.flink.runtime.io.network.netty.exception.RemoteTransportException: Fatal error at remote task manager 'wally028.cit.tu-berlin.de/130.149.249.38:53730'. 	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientHandler.decodeMsg(PartitionRequestClientHandler.java:227) 	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientHandler.channelRead(PartitionRequestClientHandler.java:162) 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339) 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324) 	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339) 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324) 	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:242) 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339) 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324) 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:847) 	at io.netty.channel.nio.AbstractNioByteChannel NioByteUnsafe.read(AbstractNioByteChannel.java:131) 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511) 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468) 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382) 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354) 	at io.netty.util.concurrent.SingleThreadEventExecutor 2.run(SingleThreadEventExecutor.java:111) 	at java.lang.Thread.run(Thread.java:722) Caused by: java.io.IOException: Index: 133, Size: 0  {noformat}  Code works fine for the twitter data set, for instance, which is bigger in size, but contains less vertices.$$patch1-flink-a56aad74_Developer_PatchNaturalnessYe$$Fix memory leak in SpillableSubpartitionView. Removed unused modifier. $$1
flink-6bc6dbec$$Sliding Window Keeps Emitting Elements After Source Stops Producing$$This happens when the source produces some elements, then the source stops for a while and then produces again some elements before stopping again. After this, the window will just keep emitting the last emitted element indefinitely.$$patch1-flink-6bc6dbec_Developer_PatchNaturalnessYe$$reset elementsSinceLastPreAggregate = 0 ;. reset elementsSinceLastPreAggregate after reduce. $$1
flink-8e3e2f8f$$Operator checkpoint statistics state size overflow$$State sizes ({{long}}) of checkpoint stats overflow when summing them up per operator, because the sum is stored in an {{int}}.$$patch1-flink-8e3e2f8f_Developer_PatchNaturalnessYe$$Fix the build. $$1
flink-b654e989$$kryo serialization problem$$Performing a cross of two dataset of POJOs I have got the exception below. The first time I run the process, there was no problem. When I run it the second time, I have got the exception. My guess is that it could be a race condition related to the reuse of the Kryo serializer object. However, it could also be "a bug where type registrations are not properly forwarded to all Serializers", as suggested by Stephan.  ------------------------------------------------------------------------ 2015-10-01 18:18:21 INFO  JobClient:161 - 10/01/2015 18:18:21	Cross(Cross at main(FlinkMongoHadoop2LinkPOI2CDA.java:160))(3/4) switched to FAILED  com.esotericsoftware.kryo.KryoException: Encountered unregistered class ID: 114 	at com.esotericsoftware.kryo.util.DefaultClassResolver.readClass(DefaultClassResolver.java:119) 	at com.esotericsoftware.kryo.Kryo.readClass(Kryo.java:641) 	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:752) 	at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize(KryoSerializer.java:210) 	at org.apache.flink.api.java.typeutils.runtime.TupleSerializer.deserialize(TupleSerializer.java:127) 	at org.apache.flink.api.java.typeutils.runtime.TupleSerializer.deserialize(TupleSerializer.java:30) 	at org.apache.flink.runtime.operators.resettable.AbstractBlockResettableIterator.getNextRecord(AbstractBlockResettableIterator.java:180) 	at org.apache.flink.runtime.operators.resettable.BlockResettableMutableObjectIterator.next(BlockResettableMutableObjectIterator.java:111) 	at org.apache.flink.runtime.operators.CrossDriver.runBlockedOuterSecond(CrossDriver.java:309) 	at org.apache.flink.runtime.operators.CrossDriver.run(CrossDriver.java:162) 	at org.apache.flink.runtime.operators.RegularPactTask.run(RegularPactTask.java:489) 	at org.apache.flink.runtime.operators.RegularPactTask.invoke(RegularPactTask.java:354) 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:581) 	at java.lang.Thread.run(Thread.java:745)$$patch1-flink-b654e989_Developer_PatchNaturalnessYe$$Added serialVersionUID to KryoSerializer. make constructor call explicit. fix a warning. KryoSerializer should handle EOFException properly. $$1
flink-fef9f115$$Keyed State does not work with DOP=1$$When changing the DOP from 3 to 1 in StatefulOperatorTest.apiTest() the test fails. The reason seems to be that the element is not properly set when chaining is happening.  Also, requiring this: {code} headContext.setNextInput(nextRecord); streamOperator.processElement(nextRecord); {code}  to be called seems rather fragile. Why not set the element in {{processElement()}}. This would also make for cleaner encapsulation, since now all outside code must assume that operators have a {{StreamingRuntimeContext}} on which they set the next element.  The state/keyed state machinery seems dangerously undertested.$$patch1-flink-fef9f115_Developer_PatchNaturalnessYe$$Fix runtime context to get next input record. $$1
flink-d738430c$$BarrierBuffer does not properly clean up temp files$$None$$patch1-flink-d738430c_Developer_PatchNaturalnessYe$$Fix checkstyle. Fix BarrierBuffer cleanup. $$1
flink-6ecd0f82$$Prevent partitioning pushdown unless partitions fields match exactly$$Consider an operation grouped on fields (A, B), followed by an operation grouped on field (A).  Right now, the optimizer can push down the partitioning on (A), which serves both operations (the first step locally still groups by A and B). This may however by a bad idea for the cases where the field A has a low cardinality, or the value distribution is skewed.  Since we cannot determine that robustly yet, I suggest to disable this optimization for now.$$patch1-flink-6ecd0f82_Developer_PatchNaturalnessYe$$Fix bug in TwoInputNode add operator to getPartitionedFields. $$1
flink-68912126$$FixedLengthRecordSorter can not write to output cross MemorySegments.$$FixedLengthRecordSorter can not write to output cross MemorySegments, it works well as it's only called to write a single record before. Should fix it and add more unit test.$$patch1-flink-68912126_Developer_PatchNaturalnessYe$$Reset offset to 0 when we are done with the sorter. $$1
flink-ce68cbd9$$fieldsGrouping for multiple output streams fails$$If a Spout or Bolt declares multiple output streams and another Bolt connects to one of those streams via "fieldsGrouping",  the call to {{FlinkTopologyBuilder.createTopology()}} fails with the following exception:  {noformat} org.apache.flink.api.common.InvalidProgramException: Specifying keys via field positions is only valid for tuple data types. Type: PojoType<org.apache.flink.stormcompatibility.util.SplitStreamType, fields = [streamId: String, value: GenericType<java.lang.Object>]> 	at org.apache.flink.api.java.operators.Keys ExpressionKeys.<init>(Keys.java:209) 	at org.apache.flink.api.java.operators.Keys ExpressionKeys.<init>(Keys.java:203) 	at org.apache.flink.streaming.api.datastream.DataStream.groupBy(DataStream.java:285) 	at org.apache.flink.stormcompatibility.api.FlinkTopologyBuilder.createTopology(FlinkTopologyBuilder.java:200) 	at org.apache.flink.stormcompatibility.api.FlinkTopologyBuilderTest.testFieldsGroupingOnMultipleBoltOutputStreams(FlinkTopologyBuilderTest.java:73) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.lang.reflect.Method.invoke(Method.java:606) 	at org.junit.runners.model.FrameworkMethod 1.runReflectiveCall(FrameworkMethod.java:47) 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44) 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271) 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70) 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50) 	at org.junit.runners.ParentRunner 3.run(ParentRunner.java:238) 	at org.junit.runners.ParentRunner 1.schedule(ParentRunner.java:63) 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236) 	at org.junit.runners.ParentRunner.access 000(ParentRunner.java:53) 	at org.junit.runners.ParentRunner 2.evaluate(ParentRunner.java:229) 	at org.junit.runners.ParentRunner.run(ParentRunner.java:309) 	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:50) 	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197) {noformat}  Fix: either introduce a mapper, that "flattens" the {{SplitStreamType}} in to regular tuple type that is nested inside or provide a custom {{KeySelector}}.$$patch1-flink-ce68cbd9_Developer_PatchNaturalnessYe$$Added missing license header. Added storm topology builder .. Fix typo in FlinkTopologyBuilder. Fix typo in FlinkTopologyBuilder. Fix broken Grails debug .. $$1
flink-948b6e05$$CsvParser: Quotes cannot be escaped inside quoted fields$$We should allow users to escape the quote character inside a quoted field.  Quoting could be realized through the \ character like in: {{"This is an \"escaped\" quotation."}}  Mailing list thread: http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/jira-Created-FLINK-2567-CsvParser-Quotes-cannot-be-escaped-inside-quoted-fields-td7654.html$$patch1-flink-948b6e05_Developer_PatchNaturalnessYe$$Add back  backslash in GenericCsvInputFormat. don ' t search for backslash when field is quoted. Add back  backslash. Don ' t search for backslash when it is escaped. Add back  backslash. don ' t search for backslash when it is escaped. $$1
flink-a402002d$$Cannot cancel failing/restarting streaming job from the command line$$I cannot seem to be able to cancel a failing/restarting job from the command line client. The job cannot be rescheduled so it keeps failing:  The exception I get: 13:58:11,240 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Status of job 0c895d22c632de5dfe16c42a9ba818d5 (player-id) changed to RESTARTING. 13:58:25,234 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Trying to cancel job with ID 0c895d22c632de5dfe16c42a9ba818d5. 13:58:25,561 WARN  akka.remote.ReliableDeliverySupervisor                        - Association with remote system [akka.tcp://flink@127.0.0.1:42012] has failed, address is now gated for [5000] ms. Reason is: [Disassociated].$$patch1-flink-a402002d_Developer_PatchNaturalnessYe$$Don ' t rethrow FAILED state as well as restarting job. $$1
flink-dc78a747$$Kryo StackOverflowError due to disabled Kryo Reference tracking$$As discussed on the dev list,  In {{KryoSerializer.java}}  Kryo Reference tracking is disabled by default:  {code}     kryo.setReferences(false); {code}  This can causes  {{StackOverflowError}} Exceptions when serializing many objects that may contain recursive objects:  {code} java.lang.StackOverflowError 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:48) 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:495) 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:523) 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:61) 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:495) 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:523) 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:61) 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:495) 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:523) 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:61) 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:495) {code}  By enabling reference tracking, we can fix this problem.  [1]https://gist.github.com/andrewpalumbo/40c7422a5187a24cd03d7d81feb2a419$$patch1-flink-dc78a747_Developer_PatchNaturalnessYe$$enable reference tracking by default. $$1
flink-30761572$$PojoType fields not supported by field position keys$$Tuple fields which are Pojos (or any other non-tuple composite type) cannot be selected as keys by field position keys.  Something like   {code} DataSet<Tuple2<Integer, MyPojo>> data = ... data.groupBy(1).reduce(...) {code}  fails with an exception.$$patch1-flink-30761572_Developer_PatchNaturalnessYe$$Remove unused imports. Reorder fields in tuple type definition. Fix bug in Keys. $$1
flink-22c370d9$$POJO Type extractor bug with type variables$$The following program incorrectly states that there are duplicate getters/setters.  {code} 	public static class Vertex<K, V> { 		 		private K key1; 		private K key2; 		private V value; 		 		public Vertex() {} 		 		public Vertex(K key, V value) { 			this.key1 = key; 			this.key2 = key; 			this.value = value; 		} 		 		public Vertex(K key1, K key2, V value) { 			this.key1 = key1; 			this.key2 = key2; 			this.value = value; 		}  		public void setKey1(K key1) { 			this.key1 = key1; 		} 		 		public void setKey2(K key2) { 			this.key2 = key2; 		} 		 		public K getKey1() { 			return key1; 		} 		 		public K getKey2() { 			return key2; 		} 		 		public void setValue(V value) { 			this.value = value; 		} 		 		public V getValue() { 			return value; 		} 	} 	 	public static void main(String[] args) throws Exception { 		ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); 		 		DataSet<Vertex<Long, Double>> set = env.fromElements(new Vertex<Long, Double>(0L, 3.0), new Vertex<Long, Double>(1L, 1.0)); 		 		set.print(); 		 		env.execute(); 	} {code}  The exception is {code} Exception in thread "main" java.lang.IllegalStateException: Detected more than one getters 	at org.apache.flink.api.java.typeutils.TypeExtractor.isValidPojoField(TypeExtractor.java:981) 	at org.apache.flink.api.java.typeutils.TypeExtractor.analyzePojo(TypeExtractor.java:1025) 	at org.apache.flink.api.java.typeutils.TypeExtractor.privateGetForClass(TypeExtractor.java:937) 	at org.apache.flink.api.java.typeutils.TypeExtractor.privateGetForClass(TypeExtractor.java:863) 	at org.apache.flink.api.java.typeutils.TypeExtractor.privateGetForObject(TypeExtractor.java:1146) 	at org.apache.flink.api.java.typeutils.TypeExtractor.getForObject(TypeExtractor.java:1116) 	at org.apache.flink.api.java.ExecutionEnvironment.fromElements(ExecutionEnvironment.java:466) 	at test.Test.main(Test.java:74)  {code}$$patch1-flink-22c370d9_Developer_PatchNaturalnessYe$$Fix typo in TypeExtractor. Fix typo. $$1
flink-ccd574a4$$Failed task deployment causes NPE on input split assignment$$The input split assignment code is returning {null} if the Task has failed, which is causing a NPE.  We should improve our error handling / reporting in that situation.  {code} 13:12:31,002 INFO  org.apache.flink.yarn.ApplicationMaster  anonfun 2  anon 1    - Status of job c0b47ce41e9a85a628a628a3977705ef (Flink Java Job at Tue Apr 21 13:10:36 UTC 2015) changed to FAILING Cannot deploy task - TaskManager not responding.. .... 13:12:47,591 ERROR org.apache.flink.runtime.operators.RegularPactTask            - Error in task code:  CHAIN DataSource (at userMethod (org.apache.flink.api.java.io.AvroInputFormat)) -> FlatMap (FlatMap at main(UserClass.java:111)) (20/50) java.lang.RuntimeException: Requesting the next InputSplit failed. 	at org.apache.flink.runtime.taskmanager.TaskInputSplitProvider.getNextInputSplit(TaskInputSplitProvider.java:88) 	at org.apache.flink.runtime.operators.DataSourceTask 1.hasNext(DataSourceTask.java:337) 	at org.apache.flink.runtime.operators.DataSourceTask.invoke(DataSourceTask.java:136) 	at org.apache.flink.runtime.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:217) 	at java.lang.Thread.run(Thread.java:744) Caused by: java.lang.NullPointerException 	at java.io.ByteArrayInputStream.<init>(ByteArrayInputStream.java:106) 	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:301) 	at org.apache.flink.runtime.taskmanager.TaskInputSplitProvider.getNextInputSplit(TaskInputSplitProvider.java:83) 	... 4 more 13:12:47,595 INFO  org.apache.flink.runtime.taskmanager.Task                     - CHAIN DataSource (at SomeMethod (org.apache.flink.api.java.io.AvroInputFormat)) -> FlatMap (FlatMap at main(SomeClass.java:111)) (20/50) switched to FAILED : java.lang.RuntimeException: Requesting the next InputSplit failed. 	at org.apache.flink.runtime.taskmanager.TaskInputSplitProvider.getNextInputSplit(TaskInputSplitProvider.java:88) 	at org.apache.flink.runtime.operators.DataSourceTask 1.hasNext(DataSourceTask.java:337) 	at org.apache.flink.runtime.operators.DataSourceTask.invoke(DataSourceTask.java:136) 	at org.apache.flink.runtime.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:217) 	at java.lang.Thread.run(Thread.java:744) Caused by: java.lang.NullPointerException 	at java.io.ByteArrayInputStream.<init>(ByteArrayInputStream.java:106) 	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:301) 	at org.apache.flink.runtime.taskmanager.TaskInputSplitProvider.getNextInputSplit(TaskInputSplitProvider.java:83) 	... 4 more {code}$$patch1-flink-ccd574a4_Developer_PatchNaturalnessYe$$Fix finalization of Execution . getInstance ( ). Fix a message. Remove incorrect return type in TaskInputSplitProvider. Don ' t return null input splits if data is null. $$1
flink-39d526e6$$Bug in DoubleParser and FloatParser - empty String is not casted to 0$$Hi,  I found the bug, when I wanted to read a csv file, which had a line like: "||\n"  If I treat it as a Tuple2<Long,Long>, I get as expected a tuple (0L,0L).  But if I want to read it into a Double-Tuple or a Float-Tuple, I get the following error:  java.lang.AssertionError: Test failed due to a org.apache.flink.api.common.io.ParseException: Line could not be parsed: '||' ParserError NUMERIC_VALUE_FORMAT_ERROR   This error can be solved by adding an additional condition for empty strings in the FloatParser / DoubleParser.  We definitely need the CSVReader to be able to read "empty values".  I can fix it like described if there are no better ideas :)$$patch1-flink-39d526e6_Developer_PatchNaturalnessYe$$parseField ( ) didn ' t catch any number of whitespaces in the field value. Added parseField ( ) javadoc - since it is normally possible to parse text characters .. don ' t ignore whitespace in numeric field value. Remove unnecessary copy of delimiters. ignore whitespace in numeric field. Fix the oversampling of ShortParser . result. Don ' t setErrorState for empty field with only the sign. Fix checkstyle error in ShortParser. Remove an unnecessary check for parseField ( ). Don ' t throw exception if value is overflow / underflow .. workflow changes - Add error state back to the reusable field when it is not used .. Fix whitespace in FloatParser . parseField ( ) where the field is not reusable. don ' t ignore whitespace in numeric field value. Added parseField ( ). don ' t ignore whitespace in numeric field value. NPE on byte value parsing. Parser always setErrorState ( ) if we don ' t have a value , it will throw an. Remove unnecessary copy of delimiters. ignore whitespace in numeric field. Don ' t setErrorState for empty string fields. Don ' t over - parse NUMERIC_VALUE_OVERFLOW_UNDERFLOW .. Remove an obsolete parseField ( ) method. Don ' t throw NPE on empty field value .. Add whitespace in numeric fields as well as string parsing. Don ' t mark an empty field with the - sign. Don ' t setErrorState ( ) on empty string fields. Don ' t allow negative numbers in byte parsers. Remove erroneous parseField static utility note. Fix bug in byte parser. NPE in IntValueParser. remove unused local variable. move to the parser ' s own error state. fix a typo long parser would mark the end of text as long on the fly. Do not set parseField ( ) with - > long. Fix build error where negative long fields were read. $$1
flink-8fc7e7af$$Early cancel calls can cause Tasks to not cancel properly$$When a task receives the "cancel()" call before the operators are properly instantiated, it can be that the operator never receives a cancel call.  In certain cases, this causes the operator to hang.$$patch1-flink-8fc7e7af_Developer_PatchNaturalnessYe$$Add canceled flag to StreamTask. throw exception if canceled. Add a final check to exit early on running the task. @@ cancel is not enumerable in Javadoc. Added private method to StreamTask. $$1
flink-59685903$$Windowed fold operation fails because the initial value was not serialized$$The windowed fold operation currently fails because the initial value was not serialized. The reason for this is that the fold operation is realized as a {{WindowFunction}} within an {{AbstractUdfStreamOperator}} and does not get the output type information forwarded (which is necessary for the serialization).   The solution is to let the {{AbstractUdfStreamOperator}} forward the output type information to the {{WindowFunction}} if it implements the {{OutputTypeConfigurable}} interface.$$patch1-flink-59685903_Developer_PatchNaturalnessYe$$Remove unused import. Remove copy and paste error. Remove unnecessary code. Remove unused import. Remove copy and paste error. Remove unnecessary code. Add missing imports. Fix checkstyle. Fix setting of output type for UDF stream operators. $$1
flink-a17d4e82$$ReduceOnNeighborsWithExceptionITCase failure$$I noticed a build error due to failure on this case. It was on a branch of my fork, which didn't actually have anything to do with the failed test or the runtime system at all.  Here's the error log: https://s3.amazonaws.com/archive.travis-ci.org/jobs/73695554/log.txt$$patch1-flink-a17d4e82_Developer_PatchNaturalnessYe$$Fix volatile flag in SpillableSubpartition .. Added isReleased method. Fix build. Fix isReleased .. Fix compaction issue. Fix isReleased .. Fixes issue 732. Added methods on PipelinedSubpartition to check if isReleased .. Fix isReleased in AsyncIOView. $$1
flink-a5b05566$$Error while parsing job arguments passed by CLI$$Flink CLI treats job arguments provided in format "-<char>" as its own parameters, which results in errors in execution.  Example 1: call: >bin/flink info myJarFile.jar -f flink -i <filepath> -m 1 error: Unrecognized option: -f  Example 2: Job myJarFile.jar is uploaded to web submission client, flink parameter box is empty program arguments box: -f flink -i <filepath> -m 1 error:  An unexpected error occurred: Unrecognized option: -f org.apache.flink.client.cli.CliArgsException: Unrecognized option: -f 	at org.apache.flink.client.cli.CliFrontendParser.parseInfoCommand(CliFrontendParser.java:296) 	at org.apache.flink.client.CliFrontend.info(CliFrontend.java:376) 	at org.apache.flink.client.CliFrontend.parseParameters(CliFrontend.java:983) 	at org.apache.flink.client.web.JobSubmissionServlet.doGet(JobSubmissionServlet.java:171) 	at javax.servlet.http.HttpServlet.service(HttpServlet.java:734) 	at javax.servlet.http.HttpServlet.service(HttpServlet.java:847) 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:532) 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:453) 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:227) 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:965) 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:388) 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:187) 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:901) 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:117) 	at org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:47) 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:113) 	at org.eclipse.jetty.server.Server.handle(Server.java:348) 	at org.eclipse.jetty.server.HttpConnection.handleRequest(HttpConnection.java:596) 	at org.eclipse.jetty.server.HttpConnection RequestHandler.headerComplete(HttpConnection.java:1048) 	at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:549) 	at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:211) 	at org.eclipse.jetty.server.HttpConnection.handle(HttpConnection.java:425) 	at org.eclipse.jetty.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:489) 	at org.eclipse.jetty.util.thread.QueuedThreadPool 2.run(QueuedThreadPool.java:436) 	at java.lang.Thread.run(Thread.java:745)  Execution of  >bin/flink run myJarFile.jar -f flink -i <filepath> -m 1   works perfectly fine$$patch1-flink-a5b05566_Developer_PatchNaturalnessYe$$parse info command from args with spaces. $$1
flink-259f10c0$$CompilerException caused by NullPointerException$$Run into it during working on my code. Seems not caused by my plan, or anyway the compiler should have a NullPointer isssue:  org.apache.flink.compiler.CompilerException: An error occurred while translating the optimized plan to a nephele JobGraph: Error translating node 'Union "Union" : UNION [[ GlobalProperties [partitioning=HASH_PARTITIONED, on fields [0]] ]] [[ LocalProperties [ordering=null, grouped=null, unique=null] ]]': null 	at org.apache.flink.compiler.plantranslate.NepheleJobGraphGenerator.postVisit(NepheleJobGraphGenerator.java:543) 	at org.apache.flink.compiler.plantranslate.NepheleJobGraphGenerator.postVisit(NepheleJobGraphGenerator.java:95) 	at org.apache.flink.compiler.plan.DualInputPlanNode.accept(DualInputPlanNode.java:170) 	at org.apache.flink.compiler.plan.SingleInputPlanNode.accept(SingleInputPlanNode.java:196) 	at org.apache.flink.compiler.plan.SingleInputPlanNode.accept(SingleInputPlanNode.java:196) 	at org.apache.flink.compiler.plan.OptimizedPlan.accept(OptimizedPlan.java:165) 	at org.apache.flink.compiler.plantranslate.NepheleJobGraphGenerator.compileJobGraph(NepheleJobGraphGenerator.java:163) 	at org.apache.flink.client.program.Client.getJobGraph(Client.java:218) 	at org.apache.flink.client.program.Client.run(Client.java:290) 	at org.apache.flink.client.program.Client.run(Client.java:285) 	at org.apache.flink.client.program.Client.run(Client.java:230) 	at org.apache.flink.client.CliFrontend.executeProgram(CliFrontend.java:347) 	at org.apache.flink.client.CliFrontend.run(CliFrontend.java:334) 	at org.apache.flink.client.CliFrontend.parseParameters(CliFrontend.java:1001) 	at org.apache.flink.client.CliFrontend.main(CliFrontend.java:1025) Caused by: org.apache.flink.compiler.CompilerException: Error translating node 'Union "Union" : UNION [[ GlobalProperties [partitioning=HASH_PARTITIONED, on fields [0]] ]] [[ LocalProperties [ordering=null, grouped=null, unique=null] ]]': null 	at org.apache.flink.compiler.plantranslate.NepheleJobGraphGenerator.preVisit(NepheleJobGraphGenerator.java:338) 	at org.apache.flink.compiler.plantranslate.NepheleJobGraphGenerator.preVisit(NepheleJobGraphGenerator.java:95) 	at org.apache.flink.compiler.plan.DualInputPlanNode.accept(DualInputPlanNode.java:162) 	at org.apache.flink.compiler.plan.WorksetIterationPlanNode.acceptForStepFunction(WorksetIterationPlanNode.java:196) 	at org.apache.flink.compiler.plantranslate.NepheleJobGraphGenerator.postVisit(NepheleJobGraphGenerator.java:398) 	... 14 more Caused by: java.lang.NullPointerException 	at org.apache.flink.runtime.operators.util.TaskConfig.setDriver(TaskConfig.java:307) 	at org.apache.flink.compiler.plantranslate.NepheleJobGraphGenerator.createDualInputVertex(NepheleJobGraphGenerator.java:793) 	at org.apache.flink.compiler.plantranslate.NepheleJobGraphGenerator.preVisit(NepheleJobGraphGenerator.java:286) 	... 18 more$$patch1-flink-259f10c0_Developer_PatchNaturalnessYe$$Fix a bug in BulkIterationNode where the partial solution is a union node. Add a check for union / union node. add plandump header. Remove unused imports. Remove unused import. Fix JSON output of Channel . getLocalStrategy. $$1
flink-0cfa43d7$$Chained stream tasks share the same RuntimeContext$$Chained stream operators currently share the same runtimecontext, this will certainly lead to problems in the future.   We should create separate runtime contexts for each operator in the chain.$$patch1-flink-0cfa43d7_Developer_PatchNaturalnessYe$$Added runtime context for chained operator. Fix modifier order in StreamTask. Fix stream task registration. Added missing mandatory task parameter. $$1
flink-88a97768$$Watermark triggered operators cannot progress with cyclic flows$$The problem is that we can easily create a cyclic watermark (time) dependency in the stream graph which will result in a deadlock for watermark triggered operators such as  the `WindowOperator`.  A solution to this could be to emit a Long.MAX_VALUE watermark from the iteration sources.$$patch1-flink-88a97768_Developer_PatchNaturalnessYe$$Remove unused imports. Add an optimization to reduce the number of times that a stream item is emitted. Fix warning. remove warning suppression. $$1
flink-4dbf030a$$NullPointerException in vertex-centric iteration$$Hello to my Squirrels,  I came across this exception when having a vertex-centric iteration output followed by a group by.  I'm not sure if what is causing it, since I saw this error in a rather large pipeline, but I managed to reproduce it with [this code example | https://github.com/vasia/flink/commit/1b7bbca1a6130fbcfe98b4b9b43967eb4c61f309] and a sufficiently large dataset, e.g. [this one | http://snap.stanford.edu/data/com-DBLP.html] (I'm running this locally). It seems like a null Buffer in RecordWriter.  The exception message is the following:  Exception in thread "main" org.apache.flink.runtime.client.JobExecutionException: Job execution failed. at org.apache.flink.runtime.jobmanager.JobManager anonfun receiveWithLogMessages 1.applyOrElse(JobManager.scala:319) at scala.runtime.AbstractPartialFunction mcVL sp.apply mcVL sp(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:25) at org.apache.flink.runtime.ActorLogMessages anon 1.apply(ActorLogMessages.scala:37) at org.apache.flink.runtime.ActorLogMessages anon 1.apply(ActorLogMessages.scala:30) at scala.PartialFunction class.applyOrElse(PartialFunction.scala:118) at org.apache.flink.runtime.ActorLogMessages anon 1.applyOrElse(ActorLogMessages.scala:30) at akka.actor.Actor class.aroundReceive(Actor.scala:465) at org.apache.flink.runtime.jobmanager.JobManager.aroundReceive(JobManager.scala:94) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516) at akka.actor.ActorCell.invoke(ActorCell.scala:487) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:254) at akka.dispatch.Mailbox.run(Mailbox.scala:221) at akka.dispatch.Mailbox.exec(Mailbox.scala:231) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) Caused by: java.lang.NullPointerException at org.apache.flink.runtime.io.network.api.serialization.SpanningRecordSerializer.setNextBuffer(SpanningRecordSerializer.java:93) at org.apache.flink.runtime.io.network.api.writer.RecordWriter.emit(RecordWriter.java:92) at org.apache.flink.runtime.operators.shipping.OutputCollector.collect(OutputCollector.java:65) at org.apache.flink.runtime.iterative.task.IterationHeadPactTask.streamSolutionSetToFinalOutput(IterationHeadPactTask.java:405) at org.apache.flink.runtime.iterative.task.IterationHeadPactTask.run(IterationHeadPactTask.java:365) at org.apache.flink.runtime.operators.RegularPactTask.invoke(RegularPactTask.java:360) at org.apache.flink.runtime.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:221) at java.lang.Thread.run(Thread.java:745)$$patch1-flink-4dbf030a_Developer_PatchNaturalnessYe$$Add an exception if the BufferPool is destroyed .. $$1
flink-91f9bfc7$$Interfaces and abstract classes are not valid types$$I don't know whether this is by design or is a bug, but I am having trouble working with DataSet and traits in scala which is a major limitation.  A simple example is shown below.    Compile time warning is 'Type Main.SimpleTrait has no fields that are visible from Scala Type analysis. Falling back to Java Type Analysis...'  Run time error is 'Interfaces and abstract classes are not valid types: interface Main SimpleTrait'  Regards, John    val env = ExecutionEnvironment.getExecutionEnvironment    trait SimpleTrait {     def contains(x: String): Boolean   }    class SimpleClass extends SimpleTrait {     def contains(x: String) = true   }    val data: DataSet[Double] = env.fromElements(1.0, 2.0, 3.0, 4.0)    def f(data: DataSet[Double]): DataSet[SimpleTrait] = {      data.mapPartition(iterator => {       Iterator(new SimpleClass)     })   }     val g = f(data)   g.print()     env.execute("Simple example")$$patch1-flink-91f9bfc7_Developer_PatchNaturalnessYe$$Remove generics errors. Remove invalid check for non - primitives. Improve TypeExtractor for interfaces ( fix error in previous change ). Fixed log level for TypeExtractor . does not contain a getter / setter. Fixed log level. Fixed log level for TypeExtractor. Add logging for non - abstract / interface classes .. $$1
flink-21f47d9c$$Custom Kryo Serializer fails in itertation scenario$$When using iterations with a custom serializer for a domain object, the iteration will fail.  {code:java} org.apache.flink.runtime.client.JobExecutionException: com.esotericsoftware.kryo.KryoException: Buffer underflow 	at org.apache.flink.api.java.typeutils.runtime.NoFetchingInput.require(NoFetchingInput.java:76) 	at com.esotericsoftware.kryo.io.Input.readVarInt(Input.java:355) 	at com.esotericsoftware.kryo.util.DefaultClassResolver.readClass(DefaultClassResolver.java:109) 	at com.esotericsoftware.kryo.Kryo.readClass(Kryo.java:641) 	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:752) 	at org.apache.flink.api.java.typeutils.runtime.KryoSerializer.deserialize(KryoSerializer.java:198) 	at org.apache.flink.api.java.typeutils.runtime.KryoSerializer.deserialize(KryoSerializer.java:203) 	at org.apache.flink.runtime.io.disk.InputViewIterator.next(InputViewIterator.java:43) 	at org.apache.flink.runtime.iterative.task.IterationHeadPactTask.streamOutFinalOutputBulk(IterationHeadPactTask.java:404) 	at org.apache.flink.runtime.iterative.task.IterationHeadPactTask.run(IterationHeadPactTask.java:377) 	at org.apache.flink.runtime.operators.RegularPactTask.invoke(RegularPactTask.java:360) 	at org.apache.flink.runtime.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:204) 	at java.lang.Thread.run(Thread.java:745) {code}$$patch1-flink-21f47d9c_Developer_PatchNaturalnessYe$$Don ' t rethrow EOFExceptions , keep going .. Added missing import. Fix NoFetchingInput to throw an exception with the missing ' more bytes ' left in the. Add EOFException to NoFetchingInput .. $$1
flink-1f726e48$$Streaming iteration heads cannot be instantiated$$It looks that streaming jobs with iterations and dop > 1 do not work currently. From what I see, when the TaskManager tries to instantiate a new RuntimeEnvironment for the iteration head tasks it fails since the following exception is being thrown:  java.lang.Exception: Failed to deploy the task Map (2/8) - execution #0 to slot SimpleSlot (0)(1) - 0e39fcabcab3e8543cc2d8320f9de783 - ALLOCATED/ALIVE: java.lang.Exception: Error setting up runtime environment: java.lang.RuntimeException: Could not register the given element, broker slot is already occupied. 	at org.apache.flink.runtime.execution.RuntimeEnvironment.<init>(RuntimeEnvironment.java:174) 	at org.apache.flink.runtime.taskmanager.TaskManager.org apache flink runtime taskmanager TaskManager  submitTask(TaskManager.scala:432) ..... ..... Caused by: java.lang.RuntimeException: java.lang.RuntimeException: Could not register the given element, broker slot is already occupied. 	at org.apache.flink.streaming.api.streamvertex.StreamIterationHead.setInputsOutputs(StreamIterationHead.java:64) 	at org.apache.flink.streaming.api.streamvertex.StreamVertex.registerInputOutput(StreamVertex.java:86) 	at org.apache.flink.runtime.execution.RuntimeEnvironment.<init>(RuntimeEnvironment.java:171) 	... 20 more Caused by: java.lang.RuntimeException: Could not register the given element, broker slot is already occupied. 	at org.apache.flink.runtime.iterative.concurrent.Broker.handIn(Broker.java:39) 	at org.apache.flink.streaming.api.streamvertex.StreamIterationHead.setInputsOutputs(StreamIterationHead.java:62)  The IterateTest passed since it is using a dop of 1 but for higher parallelism it fails. Also, the IterateExample fails as well if you try to run it.   I will debug this once I find some time so any ideas of what could possible cause this are more than welcome.$$patch1-flink-1f726e48_Developer_PatchNaturalnessYe$$Added missing line. $$1
flink-03340919$$FileInputFormat.addFilesInDir miscalculates total size$$In FileInputFormat.addFilesInDir, the length variable should start from 0, because the return value is always used by adding it to the length (instead of just assigning). So with the current version, the length before the call will be seen twice in the result.  mvn verify caught this for me now. The reason why this hasn't been seen yet, is because testGetStatisticsMultipleNestedFiles catches this only if it gets the listings of the outer directory in a certain order. Concretely, if the inner directory is seen before the other file in the outer directory, then length is 0 at that point, so the bug doesn't show. But if the other file is seen first, then its size is added twice to the total result.$$patch1-flink-03340919_Developer_PatchNaturalnessYe$$Fix bug in FileInputFormat. Remove over - aggressive split over time .. Fix max log ignored files for FileInputFormat. $$1
flink-6968a57a$$ExecutionGraph gets stuck in state FAILING$$It is a bit of a rare case, but the following can currently happen:    1. Jobs runs for a while, some tasks are already finished.   2. Job fails, goes to state failing and restarting. Non-finished tasks fail or are canceled.   3. For the finished tasks, ask-futures from certain messages (for example for releasing intermediate result partitions) can fail (timeout) and cause the execution to go from FINISHED to FAILED   4. This triggers the execution graph to go to FAILING without ever going further into RESTARTING again   5. The job is stuck  It initially looks like this is mainly an issue for batch jobs (jobs where tasks do finish, rather than run infinitely).  The log that shows how this manifests: {code} -------------------------------------------------------------------------------- 17:19:19,782 INFO  akka.event.slf4j.Slf4jLogger                                  - Slf4jLogger started 17:19:19,844 INFO  Remoting                                                      - Starting remoting 17:19:20,065 INFO  Remoting                                                      - Remoting started; listening on addresses :[akka.tcp://flink@127.0.0.1:56722] 17:19:20,090 INFO  org.apache.flink.runtime.blob.BlobServer                      - Created BLOB server storage directory /tmp/blobStore-6766f51a-1c51-4a03-acfb-08c2c29c11f0 17:19:20,096 INFO  org.apache.flink.runtime.blob.BlobServer                      - Started BLOB server at 0.0.0.0:43327 - max concurrent requests: 50 - max backlog: 1000 17:19:20,113 INFO  org.apache.flink.runtime.jobmanager.MemoryArchivist           - Started memory archivist akka://flink/user/archive 17:19:20,115 INFO  org.apache.flink.runtime.checkpoint.SavepointStoreFactory     - No savepoint state backend configured. Using job manager savepoint state backend. 17:19:20,118 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Starting JobManager at akka.tcp://flink@127.0.0.1:56722/user/jobmanager. 17:19:20,123 INFO  org.apache.flink.runtime.jobmanager.JobManager                - JobManager akka.tcp://flink@127.0.0.1:56722/user/jobmanager was granted leadership with leader session ID None. 17:19:25,605 INFO  org.apache.flink.runtime.instance.InstanceManager             - Registered TaskManager at testing-worker-linux-docker-e6d6931f-3200-linux-4 (akka.tcp://flink@172.17.0.253:43702/user/taskmanager) as f213232054587f296a12140d56f63ed1. Current number of registered hosts is 1. Current number of alive task slots is 2. 17:19:26,758 INFO  org.apache.flink.runtime.instance.InstanceManager             - Registered TaskManager at testing-worker-linux-docker-e6d6931f-3200-linux-4 (akka.tcp://flink@172.17.0.253:43956/user/taskmanager) as f9e78baa14fb38c69517fb1bcf4f419c. Current number of registered hosts is 2. Current number of alive task slots is 4. 17:19:27,064 INFO  org.apache.flink.api.java.ExecutionEnvironment                - The job has 0 registered types and 0 default Kryo serializers 17:19:27,071 INFO  org.apache.flink.client.program.Client                        - Starting client actor system 17:19:27,072 INFO  org.apache.flink.runtime.client.JobClient                     - Starting JobClient actor system 17:19:27,110 INFO  akka.event.slf4j.Slf4jLogger                                  - Slf4jLogger started 17:19:27,121 INFO  Remoting                                                      - Starting remoting 17:19:27,143 INFO  org.apache.flink.runtime.client.JobClient                     - Started JobClient actor system at 127.0.0.1:51198 17:19:27,145 INFO  Remoting                                                      - Remoting started; listening on addresses :[akka.tcp://flink@127.0.0.1:51198] 17:19:27,325 INFO  org.apache.flink.runtime.client.JobClientActor                - Disconnect from JobManager null. 17:19:27,362 INFO  org.apache.flink.runtime.client.JobClientActor                - Received job Flink Java Job at Mon Jan 18 17:19:27 UTC 2016 (fa05fd25993a8742da09cc5023c1e38d). 17:19:27,362 INFO  org.apache.flink.runtime.client.JobClientActor                - Could not submit job Flink Java Job at Mon Jan 18 17:19:27 UTC 2016 (fa05fd25993a8742da09cc5023c1e38d), because there is no connection to a JobManager. 17:19:27,379 INFO  org.apache.flink.runtime.client.JobClientActor                - Connect to JobManager Actor[akka.tcp://flink@127.0.0.1:56722/user/jobmanager#-1489998809]. 17:19:27,379 INFO  org.apache.flink.runtime.client.JobClientActor                - Connected to new JobManager akka.tcp://flink@127.0.0.1:56722/user/jobmanager. 17:19:27,379 INFO  org.apache.flink.runtime.client.JobClientActor                - Sending message to JobManager akka.tcp://flink@127.0.0.1:56722/user/jobmanager to submit job Flink Java Job at Mon Jan 18 17:19:27 UTC 2016 (fa05fd25993a8742da09cc5023c1e38d) and wait for progress 17:19:27,380 INFO  org.apache.flink.runtime.client.JobClientActor                - Upload jar files to job manager akka.tcp://flink@127.0.0.1:56722/user/jobmanager. 17:19:27,380 INFO  org.apache.flink.runtime.client.JobClientActor                - Submit job to the job manager akka.tcp://flink@127.0.0.1:56722/user/jobmanager. 17:19:27,453 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Submitting job fa05fd25993a8742da09cc5023c1e38d (Flink Java Job at Mon Jan 18 17:19:27 UTC 2016). 17:19:27,591 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Scheduling job fa05fd25993a8742da09cc5023c1e38d (Flink Java Job at Mon Jan 18 17:19:27 UTC 2016). 17:19:27,592 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (1/4) (c79bf4381462c690f5999f2d1949ab50) switched from CREATED to SCHEDULED 17:19:27,596 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (1/4) (c79bf4381462c690f5999f2d1949ab50) switched from SCHEDULED to DEPLOYING 17:19:27,597 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Deploying DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (1/4) (attempt #0) to testing-worker-linux-docker-e6d6931f-3200-linux-4 17:19:27,606 INFO  org.apache.flink.runtime.client.JobClientActor                - Job was successfully submitted to the JobManager akka.tcp://flink@127.0.0.1:56722/user/jobmanager. 17:19:27,630 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Status of job fa05fd25993a8742da09cc5023c1e38d (Flink Java Job at Mon Jan 18 17:19:27 UTC 2016) changed to RUNNING. 17:19:27,637 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (2/4) (e73af91028cb76f7d3cd887cb6d66755) switched from CREATED to SCHEDULED 17:19:27,654 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:27	Job execution switched to status RUNNING. 17:19:27,655 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:27	DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(1/4) switched to SCHEDULED  17:19:27,656 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:27	DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(1/4) switched to DEPLOYING  17:19:27,666 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (2/4) (e73af91028cb76f7d3cd887cb6d66755) switched from SCHEDULED to DEPLOYING 17:19:27,667 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Deploying DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (2/4) (attempt #0) to testing-worker-linux-docker-e6d6931f-3200-linux-4 17:19:27,667 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:27	DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(2/4) switched to SCHEDULED  17:19:27,669 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:27	DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(2/4) switched to DEPLOYING  17:19:27,681 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (3/4) (807daf978da9dc347dca930822c78f8f) switched from CREATED to SCHEDULED 17:19:27,682 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (3/4) (807daf978da9dc347dca930822c78f8f) switched from SCHEDULED to DEPLOYING 17:19:27,682 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Deploying DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (3/4) (attempt #0) to testing-worker-linux-docker-e6d6931f-3200-linux-4 17:19:27,682 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (4/4) (ba45c37065b67fc8f5005a50d0e88fff) switched from CREATED to SCHEDULED 17:19:27,682 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (4/4) (ba45c37065b67fc8f5005a50d0e88fff) switched from SCHEDULED to DEPLOYING 17:19:27,685 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Deploying DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (4/4) (attempt #0) to testing-worker-linux-docker-e6d6931f-3200-linux-4 17:19:27,686 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:27	DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(3/4) switched to SCHEDULED  17:19:27,687 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:27	DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(3/4) switched to DEPLOYING  17:19:27,687 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:27	DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(4/4) switched to SCHEDULED  17:19:27,692 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:27	DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(4/4) switched to DEPLOYING  17:19:27,833 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (4/4) (ba45c37065b67fc8f5005a50d0e88fff) switched from DEPLOYING to RUNNING 17:19:27,839 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:27	DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(4/4) switched to RUNNING  17:19:27,840 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (2/4) (e73af91028cb76f7d3cd887cb6d66755) switched from DEPLOYING to RUNNING 17:19:27,852 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:27	DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(2/4) switched to RUNNING  17:19:27,896 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (1/4) (c79bf4381462c690f5999f2d1949ab50) switched from DEPLOYING to RUNNING 17:19:27,898 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (3/4) (807daf978da9dc347dca930822c78f8f) switched from DEPLOYING to RUNNING 17:19:27,901 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:27	DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(1/4) switched to RUNNING  17:19:27,905 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:27	DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(3/4) switched to RUNNING  17:19:28,114 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (3/4) (7997918330ecf2610b3298a8c8ef2852) switched from CREATED to SCHEDULED 17:19:28,126 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (1/4) (6421c8f88b191ea844619a40a523773b) switched from CREATED to SCHEDULED 17:19:28,134 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (1/4) (6421c8f88b191ea844619a40a523773b) switched from SCHEDULED to DEPLOYING 17:19:28,134 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Deploying CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (1/4) (attempt #0) to testing-worker-linux-docker-e6d6931f-3200-linux-4 17:19:28,126 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (2/4) (d0d011dc0a0823bcec5a57a369b334ed) switched from CREATED to SCHEDULED 17:19:28,139 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (2/4) (d0d011dc0a0823bcec5a57a369b334ed) switched from SCHEDULED to DEPLOYING 17:19:28,139 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Deploying CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (2/4) (attempt #0) to testing-worker-linux-docker-e6d6931f-3200-linux-4 17:19:28,117 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (4/4) (c928d19f73d700e80cdfad650689febb) switched from CREATED to SCHEDULED 17:19:28,134 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (3/4) (7997918330ecf2610b3298a8c8ef2852) switched from SCHEDULED to DEPLOYING 17:19:28,140 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Deploying CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (3/4) (attempt #0) to testing-worker-linux-docker-e6d6931f-3200-linux-4 17:19:28,140 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (4/4) (c928d19f73d700e80cdfad650689febb) switched from SCHEDULED to DEPLOYING 17:19:28,141 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Deploying CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (4/4) (attempt #0) to testing-worker-linux-docker-e6d6931f-3200-linux-4 17:19:28,147 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28	CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(3/4) switched to SCHEDULED  17:19:28,153 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28	CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(1/4) switched to SCHEDULED  17:19:28,153 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28	CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(1/4) switched to DEPLOYING  17:19:28,153 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28	CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(2/4) switched to SCHEDULED  17:19:28,153 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28	CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(2/4) switched to DEPLOYING  17:19:28,156 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28	CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(3/4) switched to DEPLOYING  17:19:28,158 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28	CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(4/4) switched to SCHEDULED  17:19:28,165 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28	CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(4/4) switched to DEPLOYING  17:19:28,238 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (2/4) (e73af91028cb76f7d3cd887cb6d66755) switched from RUNNING to FINISHED 17:19:28,242 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28	DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(2/4) switched to FINISHED  17:19:28,308 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (3/4) (807daf978da9dc347dca930822c78f8f) switched from RUNNING to FINISHED 17:19:28,315 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (1/4) (c79bf4381462c690f5999f2d1949ab50) switched from RUNNING to FINISHED 17:19:28,317 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28	DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(3/4) switched to FINISHED  17:19:28,318 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28	DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(1/4) switched to FINISHED  17:19:28,328 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (1/4) (6421c8f88b191ea844619a40a523773b) switched from DEPLOYING to RUNNING 17:19:28,336 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28	CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(1/4) switched to RUNNING  17:19:28,338 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (3/4) (7997918330ecf2610b3298a8c8ef2852) switched from DEPLOYING to RUNNING 17:19:28,341 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28	CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(3/4) switched to RUNNING  17:19:28,459 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (4/4) (ba45c37065b67fc8f5005a50d0e88fff) switched from RUNNING to FINISHED 17:19:28,463 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28	DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(4/4) switched to FINISHED  17:19:28,520 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (4/4) (c928d19f73d700e80cdfad650689febb) switched from DEPLOYING to RUNNING 17:19:28,529 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28	CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(4/4) switched to RUNNING  17:19:28,540 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (2/4) (d0d011dc0a0823bcec5a57a369b334ed) switched from DEPLOYING to RUNNING 17:19:28,545 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28	CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(2/4) switched to RUNNING  17:19:32,384 INFO  org.apache.flink.runtime.instance.InstanceManager             - Registered TaskManager at testing-worker-linux-docker-e6d6931f-3200-linux-4 (akka.tcp://flink@172.17.0.253:60852/user/taskmanager) as 5848d44035a164a0302da6c8701ff748. Current number of registered hosts is 3. Current number of alive task slots is 6. 17:19:32,598 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Reduce (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (1/1) (d0f8f69f9047c3154b860850955de20f) switched from CREATED to SCHEDULED 17:19:32,598 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Reduce (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (1/1) (d0f8f69f9047c3154b860850955de20f) switched from SCHEDULED to DEPLOYING 17:19:32,598 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Deploying Reduce (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (1/1) (attempt #0) to testing-worker-linux-docker-e6d6931f-3200-linux-4 17:19:32,605 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:32	Reduce (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(1/1) switched to SCHEDULED  17:19:32,605 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:32	Reduce (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(1/1) switched to DEPLOYING  17:19:32,611 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (4/4) (c928d19f73d700e80cdfad650689febb) switched from RUNNING to FINISHED 17:19:32,614 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:32	CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(4/4) switched to FINISHED  17:19:32,717 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (1/4) (6421c8f88b191ea844619a40a523773b) switched from RUNNING to FINISHED 17:19:32,719 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:32	CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(1/4) switched to FINISHED  17:19:32,724 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Reduce (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (1/1) (d0f8f69f9047c3154b860850955de20f) switched from DEPLOYING to RUNNING 17:19:32,726 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:32	Reduce (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(1/1) switched to RUNNING  17:19:32,843 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (2/4) (d0d011dc0a0823bcec5a57a369b334ed) switched from RUNNING to FINISHED 17:19:32,845 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:32	CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(2/4) switched to FINISHED  17:19:33,092 WARN  akka.remote.ReliableDeliverySupervisor                        - Association with remote system [akka.tcp://flink@172.17.0.253:43702] has failed, address is now gated for [5000] ms. Reason is: [Disassociated]. 17:19:39,111 WARN  Remoting                                                      - Tried to associate with unreachable remote address [akka.tcp://flink@172.17.0.253:43702]. Address is now gated for 5000 ms, all messages to this address will be delivered to dead letters. Reason: Connection refused: /172.17.0.253:43702 17:19:39,113 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Task manager akka.tcp://flink@172.17.0.253:43702/user/taskmanager terminated. 17:19:39,114 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (3/4) (7997918330ecf2610b3298a8c8ef2852) switched from RUNNING to FAILED 17:19:39,120 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:39	CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(3/4) switched to FAILED  java.lang.Exception: The slot in which the task was executed has been released. Probably loss of TaskManager f213232054587f296a12140d56f63ed1 @ testing-worker-linux-docker-e6d6931f-3200-linux-4 - 2 slots - URL: akka.tcp://flink@172.17.0.253:43702/user/taskmanager 	at org.apache.flink.runtime.instance.SimpleSlot.releaseSlot(SimpleSlot.java:151) 	at org.apache.flink.runtime.instance.SlotSharingGroupAssignment.releaseSharedSlot(SlotSharingGroupAssignment.java:547) 	at org.apache.flink.runtime.instance.SharedSlot.releaseSlot(SharedSlot.java:119) 	at org.apache.flink.runtime.instance.Instance.markDead(Instance.java:156) 	at org.apache.flink.runtime.instance.InstanceManager.unregisterTaskManager(InstanceManager.java:215) 	at org.apache.flink.runtime.jobmanager.JobManager  anonfun handleMessage 1.applyOrElse(JobManager.scala:792) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply mcVL sp(AbstractPartialFunction.scala:33) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:33) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:25) 	at org.apache.flink.runtime.LeaderSessionMessageFilter  anonfun receive 1.applyOrElse(LeaderSessionMessageFilter.scala:44) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply mcVL sp(AbstractPartialFunction.scala:33) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:33) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:25) 	at org.apache.flink.runtime.LogMessages  anon 1.apply(LogMessages.scala:33) 	at org.apache.flink.runtime.LogMessages  anon 1.apply(LogMessages.scala:28) 	at scala.PartialFunction class.applyOrElse(PartialFunction.scala:118) 	at org.apache.flink.runtime.LogMessages  anon 1.applyOrElse(LogMessages.scala:28) 	at akka.actor.Actor class.aroundReceive(Actor.scala:465) 	at org.apache.flink.runtime.jobmanager.JobManager.aroundReceive(JobManager.scala:100) 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516) 	at akka.actor.dungeon.DeathWatch class.receivedTerminated(DeathWatch.scala:46) 	at akka.actor.ActorCell.receivedTerminated(ActorCell.scala:369) 	at akka.actor.ActorCell.autoReceiveMessage(ActorCell.scala:501) 	at akka.actor.ActorCell.invoke(ActorCell.scala:486) 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:254) 	at akka.dispatch.Mailbox.run(Mailbox.scala:221) 	at akka.dispatch.Mailbox.exec(Mailbox.scala:231) 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) 	at scala.concurrent.forkjoin.ForkJoinPool WorkQueue.runTask(ForkJoinPool.java:1339) 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)  17:19:39,129 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Reduce (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (1/1) (d0f8f69f9047c3154b860850955de20f) switched from RUNNING to CANCELING 17:19:39,132 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSink (collect()) (1/1) (895e1ea552281a665ae390c966cdb3b7) switched from CREATED to CANCELED 17:19:39,149 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:39	Job execution switched to status FAILING. java.lang.Exception: The slot in which the task was executed has been released. Probably loss of TaskManager f213232054587f296a12140d56f63ed1 @ testing-worker-linux-docker-e6d6931f-3200-linux-4 - 2 slots - URL: akka.tcp://flink@172.17.0.253:43702/user/taskmanager 	at org.apache.flink.runtime.instance.SimpleSlot.releaseSlot(SimpleSlot.java:151) 	at org.apache.flink.runtime.instance.SlotSharingGroupAssignment.releaseSharedSlot(SlotSharingGroupAssignment.java:547) 	at org.apache.flink.runtime.instance.SharedSlot.releaseSlot(SharedSlot.java:119) 	at org.apache.flink.runtime.instance.Instance.markDead(Instance.java:156) 	at org.apache.flink.runtime.instance.InstanceManager.unregisterTaskManager(InstanceManager.java:215) 	at org.apache.flink.runtime.jobmanager.JobManager  anonfun handleMessage 1.applyOrElse(JobManager.scala:792) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply mcVL sp(AbstractPartialFunction.scala:33) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:33) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:25) 	at org.apache.flink.runtime.LeaderSessionMessageFilter  anonfun receive 1.applyOrElse(LeaderSessionMessageFilter.scala:44) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply mcVL sp(AbstractPartialFunction.scala:33) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:33) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:25) 	at org.apache.flink.runtime.LogMessages  anon 1.apply(LogMessages.scala:33) 	at org.apache.flink.runtime.LogMessages  anon 1.apply(LogMessages.scala:28) 	at scala.PartialFunction class.applyOrElse(PartialFunction.scala:118) 	at org.apache.flink.runtime.LogMessages  anon 1.applyOrElse(LogMessages.scala:28) 	at akka.actor.Actor class.aroundReceive(Actor.scala:465) 	at org.apache.flink.runtime.jobmanager.JobManager.aroundReceive(JobManager.scala:100) 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516) 	at akka.actor.dungeon.DeathWatch class.receivedTerminated(DeathWatch.scala:46) 	at akka.actor.ActorCell.receivedTerminated(ActorCell.scala:369) 	at akka.actor.ActorCell.autoReceiveMessage(ActorCell.scala:501) 	at akka.actor.ActorCell.invoke(ActorCell.scala:486) 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:254) 	at akka.dispatch.Mailbox.run(Mailbox.scala:221) 	at akka.dispatch.Mailbox.exec(Mailbox.scala:231) 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) 	at scala.concurrent.forkjoin.ForkJoinPool WorkQueue.runTask(ForkJoinPool.java:1339) 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) 17:19:39,173 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:39	Reduce (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(1/1) switched to CANCELING  17:19:39,173 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:39	DataSink (collect())(1/1) switched to CANCELED  17:19:39,174 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Reduce (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (1/1) (d0f8f69f9047c3154b860850955de20f) switched from CANCELING to FAILED 17:19:39,177 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:39	Reduce (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(1/1) switched to FAILED  java.lang.Exception: The slot in which the task was executed has been released. Probably loss of TaskManager f213232054587f296a12140d56f63ed1 @ testing-worker-linux-docker-e6d6931f-3200-linux-4 - 2 slots - URL: akka.tcp://flink@172.17.0.253:43702/user/taskmanager 	at org.apache.flink.runtime.instance.SimpleSlot.releaseSlot(SimpleSlot.java:151) 	at org.apache.flink.runtime.instance.SlotSharingGroupAssignment.releaseSharedSlot(SlotSharingGroupAssignment.java:547) 	at org.apache.flink.runtime.instance.SharedSlot.releaseSlot(SharedSlot.java:119) 	at org.apache.flink.runtime.instance.Instance.markDead(Instance.java:156) 	at org.apache.flink.runtime.instance.InstanceManager.unregisterTaskManager(InstanceManager.java:215) 	at org.apache.flink.runtime.jobmanager.JobManager  anonfun handleMessage 1.applyOrElse(JobManager.scala:792) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply mcVL sp(AbstractPartialFunction.scala:33) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:33) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:25) 	at org.apache.flink.runtime.LeaderSessionMessageFilter  anonfun receive 1.applyOrElse(LeaderSessionMessageFilter.scala:44) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply mcVL sp(AbstractPartialFunction.scala:33) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:33) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:25) 	at org.apache.flink.runtime.LogMessages  anon 1.apply(LogMessages.scala:33) 	at org.apache.flink.runtime.LogMessages  anon 1.apply(LogMessages.scala:28) 	at scala.PartialFunction class.applyOrElse(PartialFunction.scala:118) 	at org.apache.flink.runtime.LogMessages  anon 1.applyOrElse(LogMessages.scala:28) 	at akka.actor.Actor class.aroundReceive(Actor.scala:465) 	at org.apache.flink.runtime.jobmanager.JobManager.aroundReceive(JobManager.scala:100) 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516) 	at akka.actor.dungeon.DeathWatch class.receivedTerminated(DeathWatch.scala:46) 	at akka.actor.ActorCell.receivedTerminated(ActorCell.scala:369) 	at akka.actor.ActorCell.autoReceiveMessage(ActorCell.scala:501) 	at akka.actor.ActorCell.invoke(ActorCell.scala:486) 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:254) 	at akka.dispatch.Mailbox.run(Mailbox.scala:221) 	at akka.dispatch.Mailbox.exec(Mailbox.scala:231) 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) 	at scala.concurrent.forkjoin.ForkJoinPool WorkQueue.runTask(ForkJoinPool.java:1339) 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)  17:19:39,179 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:39	Job execution switched to status RESTARTING. 17:19:39,179 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Delaying retry of job execution for 10000 ms ... 17:19:39,179 INFO  org.apache.flink.runtime.instance.InstanceManager             - Unregistered task manager akka.tcp://flink@172.17.0.253:43702/user/taskmanager. Number of registered task managers 2. Number of available slots 4. 17:19:39,179 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Status of job fa05fd25993a8742da09cc5023c1e38d (Flink Java Job at Mon Jan 18 17:19:27 UTC 2016) changed to FAILING. java.lang.Exception: The slot in which the task was executed has been released. Probably loss of TaskManager f213232054587f296a12140d56f63ed1 @ testing-worker-linux-docker-e6d6931f-3200-linux-4 - 2 slots - URL: akka.tcp://flink@172.17.0.253:43702/user/taskmanager 	at org.apache.flink.runtime.instance.SimpleSlot.releaseSlot(SimpleSlot.java:151) 	at org.apache.flink.runtime.instance.SlotSharingGroupAssignment.releaseSharedSlot(SlotSharingGroupAssignment.java:547) 	at org.apache.flink.runtime.instance.SharedSlot.releaseSlot(SharedSlot.java:119) 	at org.apache.flink.runtime.instance.Instance.markDead(Instance.java:156) 	at org.apache.flink.runtime.instance.InstanceManager.unregisterTaskManager(InstanceManager.java:215) 	at org.apache.flink.runtime.jobmanager.JobManager  anonfun handleMessage 1.applyOrElse(JobManager.scala:792) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply mcVL sp(AbstractPartialFunction.scala:33) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:33) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:25) 	at org.apache.flink.runtime.LeaderSessionMessageFilter  anonfun receive 1.applyOrElse(LeaderSessionMessageFilter.scala:44) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply mcVL sp(AbstractPartialFunction.scala:33) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:33) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:25) 	at org.apache.flink.runtime.LogMessages  anon 1.apply(LogMessages.scala:33) 	at org.apache.flink.runtime.LogMessages  anon 1.apply(LogMessages.scala:28) 	at scala.PartialFunction class.applyOrElse(PartialFunction.scala:118) 	at org.apache.flink.runtime.LogMessages  anon 1.applyOrElse(LogMessages.scala:28) 	at akka.actor.Actor class.aroundReceive(Actor.scala:465) 	at org.apache.flink.runtime.jobmanager.JobManager.aroundReceive(JobManager.scala:100) 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516) 	at akka.actor.dungeon.DeathWatch class.receivedTerminated(DeathWatch.scala:46) 	at akka.actor.ActorCell.receivedTerminated(ActorCell.scala:369) 	at akka.actor.ActorCell.autoReceiveMessage(ActorCell.scala:501) 	at akka.actor.ActorCell.invoke(ActorCell.scala:486) 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:254) 	at akka.dispatch.Mailbox.run(Mailbox.scala:221) 	at akka.dispatch.Mailbox.exec(Mailbox.scala:231) 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) 	at scala.concurrent.forkjoin.ForkJoinPool WorkQueue.runTask(ForkJoinPool.java:1339) 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) 17:19:39,180 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Status of job fa05fd25993a8742da09cc5023c1e38d (Flink Java Job at Mon Jan 18 17:19:27 UTC 2016) changed to RESTARTING. 17:19:42,766 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (2/4) (d0d011dc0a0823bcec5a57a369b334ed) switched from FINISHED to FAILED 17:19:42,773 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:42	CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(2/4) switched to FAILED  java.lang.IllegalStateException: Update task on instance f213232054587f296a12140d56f63ed1 @ testing-worker-linux-docker-e6d6931f-3200-linux-4 - 2 slots - URL: akka.tcp://flink@172.17.0.253:43702/user/taskmanager failed due to: 	at org.apache.flink.runtime.executiongraph.Execution 5.onFailure(Execution.java:915) 	at akka.dispatch.OnFailure.internal(Future.scala:228) 	at akka.dispatch.OnFailure.internal(Future.scala:227) 	at akka.dispatch.japi CallbackBridge.apply(Future.scala:174) 	at akka.dispatch.japi CallbackBridge.apply(Future.scala:171) 	at scala.PartialFunction class.applyOrElse(PartialFunction.scala:118) 	at scala.runtime.AbstractPartialFunction.applyOrElse(AbstractPartialFunction.scala:25) 	at scala.concurrent.Future  anonfun onFailure 1.apply(Future.scala:136) 	at scala.concurrent.Future  anonfun onFailure 1.apply(Future.scala:134) 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32) 	at scala.concurrent.impl.ExecutionContextImpl  anon 3.exec(ExecutionContextImpl.scala:107) 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) 	at scala.concurrent.forkjoin.ForkJoinPool WorkQueue.runTask(ForkJoinPool.java:1339) 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka.tcp://flink@172.17.0.253:43702/user/taskmanager#-1712955384]] after [10000 ms] 	at akka.pattern.PromiseActorRef  anonfun 1.apply mcV sp(AskSupport.scala:333) 	at akka.actor.Scheduler  anon 7.run(Scheduler.scala:117) 	at scala.concurrent.Future InternalCallbackExecutor .scala concurrent Future InternalCallbackExecutor  unbatchedExecute(Future.scala:694) 	at scala.concurrent.Future InternalCallbackExecutor .execute(Future.scala:691) 	at akka.actor.LightArrayRevolverScheduler TaskHolder.executeTask(Scheduler.scala:467) 	at akka.actor.LightArrayRevolverScheduler  anon 8.executeBucket 1(Scheduler.scala:419) 	at akka.actor.LightArrayRevolverScheduler  anon 8.nextTick(Scheduler.scala:423) 	at akka.actor.LightArrayRevolverScheduler  anon 8.run(Scheduler.scala:375) 	at java.lang.Thread.run(Thread.java:745)  17:19:42,774 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Status of job fa05fd25993a8742da09cc5023c1e38d (Flink Java Job at Mon Jan 18 17:19:27 UTC 2016) changed to FAILING. java.lang.IllegalStateException: Update task on instance f213232054587f296a12140d56f63ed1 @ testing-worker-linux-docker-e6d6931f-3200-linux-4 - 2 slots - URL: akka.tcp://flink@172.17.0.253:43702/user/taskmanager failed due to: 	at org.apache.flink.runtime.executiongraph.Execution 5.onFailure(Execution.java:915) 	at akka.dispatch.OnFailure.internal(Future.scala:228) 	at akka.dispatch.OnFailure.internal(Future.scala:227) 	at akka.dispatch.japi CallbackBridge.apply(Future.scala:174) 	at akka.dispatch.japi CallbackBridge.apply(Future.scala:171) 	at scala.PartialFunction class.applyOrElse(PartialFunction.scala:118) 	at scala.runtime.AbstractPartialFunction.applyOrElse(AbstractPartialFunction.scala:25) 	at scala.concurrent.Future  anonfun onFailure 1.apply(Future.scala:136) 	at scala.concurrent.Future  anonfun onFailure 1.apply(Future.scala:134) 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32) 	at scala.concurrent.impl.ExecutionContextImpl  anon 3.exec(ExecutionContextImpl.scala:107) 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) 	at scala.concurrent.forkjoin.ForkJoinPool WorkQueue.runTask(ForkJoinPool.java:1339) 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka.tcp://flink@172.17.0.253:43702/user/taskmanager#-1712955384]] after [10000 ms] 	at akka.pattern.PromiseActorRef  anonfun 1.apply mcV sp(AskSupport.scala:333) 	at akka.actor.Scheduler  anon 7.run(Scheduler.scala:117) 	at scala.concurrent.Future InternalCallbackExecutor .scala concurrent Future InternalCallbackExecutor  unbatchedExecute(Future.scala:694) 	at scala.concurrent.Future InternalCallbackExecutor .execute(Future.scala:691) 	at akka.actor.LightArrayRevolverScheduler TaskHolder.executeTask(Scheduler.scala:467) 	at akka.actor.LightArrayRevolverScheduler  anon 8.executeBucket 1(Scheduler.scala:419) 	at akka.actor.LightArrayRevolverScheduler  anon 8.nextTick(Scheduler.scala:423) 	at akka.actor.LightArrayRevolverScheduler  anon 8.run(Scheduler.scala:375) 	at java.lang.Thread.run(Thread.java:745) 17:19:42,780 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:42	Job execution switched to status FAILING. java.lang.IllegalStateException: Update task on instance f213232054587f296a12140d56f63ed1 @ testing-worker-linux-docker-e6d6931f-3200-linux-4 - 2 slots - URL: akka.tcp://flink@172.17.0.253:43702/user/taskmanager failed due to: 	at org.apache.flink.runtime.executiongraph.Execution 5.onFailure(Execution.java:915) 	at akka.dispatch.OnFailure.internal(Future.scala:228) 	at akka.dispatch.OnFailure.internal(Future.scala:227) 	at akka.dispatch.japi CallbackBridge.apply(Future.scala:174) 	at akka.dispatch.japi CallbackBridge.apply(Future.scala:171) 	at scala.PartialFunction class.applyOrElse(PartialFunction.scala:118) 	at scala.runtime.AbstractPartialFunction.applyOrElse(AbstractPartialFunction.scala:25) 	at scala.concurrent.Future  anonfun onFailure 1.apply(Future.scala:136) 	at scala.concurrent.Future  anonfun onFailure 1.apply(Future.scala:134) 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32) 	at scala.concurrent.impl.ExecutionContextImpl  anon 3.exec(ExecutionContextImpl.scala:107) 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) 	at scala.concurrent.forkjoin.ForkJoinPool WorkQueue.runTask(ForkJoinPool.java:1339) 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka.tcp://flink@172.17.0.253:43702/user/taskmanager#-1712955384]] after [10000 ms] 	at akka.pattern.PromiseActorRef  anonfun 1.apply mcV sp(AskSupport.scala:333) 	at akka.actor.Scheduler  anon 7.run(Scheduler.scala:117) 	at scala.concurrent.Future InternalCallbackExecutor .scala concurrent Future InternalCallbackExecutor  unbatchedExecute(Future.scala:694) 	at scala.concurrent.Future InternalCallbackExecutor .execute(Future.scala:691) 	at akka.actor.LightArrayRevolverScheduler TaskHolder.executeTask(Scheduler.scala:467) 	at akka.actor.LightArrayRevolverScheduler  anon 8.executeBucket 1(Scheduler.scala:419) 	at akka.actor.LightArrayRevolverScheduler  anon 8.nextTick(Scheduler.scala:423) 	at akka.actor.LightArrayRevolverScheduler  anon 8.run(Scheduler.scala:375) 	at java.lang.Thread.run(Thread.java:745) 17:19:49,152 WARN  Remoting                                                      - Tried to associate with unreachable remote address [akka.tcp://flink@172.17.0.253:43702]. Address is now gated for 5000 ms, all messages to this address will be delivered to dead letters. Reason: Connection refused: /172.17.0.253:43702 17:19:59,172 WARN  Remoting                                                      - Tried to associate with unreachable remote address [akka.tcp://flink@172.17.0.253:43702]. Address is now gated for 5000 ms, all messages to this address will be delivered to dead letters. Reason: Connection refused: /172.17.0.253:43702 17:20:09,191 WARN  Remoting                                                      - Tried to associate with unreachable remote address [akka.tcp://flink@172.17.0.253:43702]. Address is now gated for 5000 ms, all messages to this address will be delivered to dead letters. Reason: Connection refused: /172.17.0.253:43702 17:24:32,423 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Stopping JobManager akka.tcp://flink@127.0.0.1:56722/user/jobmanager. 17:24:32,440 ERROR org.apache.flink.test.recovery.TaskManagerProcessFailureBatchRecoveryITCase  -  -------------------------------------------------------------------------------- Test testTaskManagerProcessFailure[0](org.apache.flink.test.recovery.TaskManagerProcessFailureBatchRecoveryITCase) failed with: java.lang.AssertionError: The program did not finish in time 	at org.junit.Assert.fail(Assert.java:88) 	at org.junit.Assert.assertTrue(Assert.java:41) 	at org.junit.Assert.assertFalse(Assert.java:64) 	at org.apache.flink.test.recovery.AbstractTaskManagerProcessFailureRecoveryTest.testTaskManagerProcessFailure(AbstractTaskManagerProcessFailureRecoveryTest.java:212) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.lang.reflect.Method.invoke(Method.java:606) 	at org.junit.runners.model.FrameworkMethod 1.runReflectiveCall(FrameworkMethod.java:47) 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44) 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) 	at org.junit.rules.TestWatcher 1.evaluate(TestWatcher.java:55) 	at org.junit.rules.RunRules.evaluate(RunRules.java:20) 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271) 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70) 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50) 	at org.junit.runners.ParentRunner 3.run(ParentRunner.java:238) 	at org.junit.runners.ParentRunner 1.schedule(ParentRunner.java:63) 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236) 	at org.junit.runners.ParentRunner.access 000(ParentRunner.java:53) 	at org.junit.runners.ParentRunner 2.evaluate(ParentRunner.java:229) 	at org.junit.runners.ParentRunner.run(ParentRunner.java:309) 	at org.junit.runners.Suite.runChild(Suite.java:127) 	at org.junit.runners.Suite.runChild(Suite.java:26) 	at org.junit.runners.ParentRunner 3.run(ParentRunner.java:238) 	at org.junit.runners.ParentRunner 1.schedule(ParentRunner.java:63) 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236) 	at org.junit.runners.ParentRunner.access 000(ParentRunner.java:53) 	at org.junit.runners.ParentRunner 2.evaluate(ParentRunner.java:229) 	at org.junit.runners.ParentRunner.run(ParentRunner.java:309) 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:283) 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:173) 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153) 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:128) {code}$$patch1-flink-6968a57a_Developer_PatchNaturalnessYe$$Fix typo in comment. Log better if we are aborting or are already finished. Added a sanity check to make sure that transition is possible .. $$1
flink-adb321d6$$NullPointerException in DeltaIteration when no ForwardedFileds$$The following exception is thrown by the Connected Components example, if the @ForwardedFieldsFirst("*") annotation from the ComponentIdFilter join is removed:  Caused by: java.lang.NullPointerException 	at org.apache.flink.examples.java.graph.ConnectedComponents ComponentIdFilter.join(ConnectedComponents.java:186) 	at org.apache.flink.examples.java.graph.ConnectedComponents ComponentIdFilter.join(ConnectedComponents.java:1) 	at org.apache.flink.runtime.operators.JoinWithSolutionSetSecondDriver.run(JoinWithSolutionSetSecondDriver.java:198) 	at org.apache.flink.runtime.operators.RegularPactTask.run(RegularPactTask.java:496) 	at org.apache.flink.runtime.iterative.task.AbstractIterativePactTask.run(AbstractIterativePactTask.java:139) 	at org.apache.flink.runtime.iterative.task.IterationIntermediatePactTask.run(IterationIntermediatePactTask.java:92) 	at org.apache.flink.runtime.operators.RegularPactTask.invoke(RegularPactTask.java:362) 	at org.apache.flink.runtime.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:217) 	at java.lang.Thread.run(Thread.java:745)  [Code | https://github.com/vasia/flink/tree/cc-test] and [dataset | http://snap.stanford.edu/data/com-DBLP.html] to reproduce.$$patch1-flink-adb321d6_Developer_PatchNaturalnessYe$$Don ' t add a pipeline breaker if the channel is within an iteration .. $$1
flink-63d9800e$$Custom StateCheckpointers should be included in the snapshots$$Currently the restoreInitialState call fails when the user uses a custom StateCheckpointer to create the snapshot, because the state is restored before the StateCheckpointer is set for the StreamOperatorState. (because the restoreInitialState() call precedes the open() call)  To avoid this issue, the custom StateCheckpointer instance should be stored within the snapshot and should be set in the StreamOperatorState before calling restoreState(..).  To reduce the overhead induced by this we can do 2 optimizations:  - We only include custom StateCheckpointers (the default java serializer one is always available)  - We only serialize the checkpointer once and store the byte array in the snapshot$$patch1-flink-63d9800e_Developer_PatchNaturalnessYe$$Restored private boolean flag. Restore state if it is not restore yet. Restored state before setting a new one. Fix checkpointing state correctly. Restored private boolean in PartitionedStreamOperatorState. Restore state if it is not restored yet. Restore state if it is not restored yet. PartitionedStreamOperatorState should set the checkpointer before the checkpoint is taken out of the way. $$1
flink-d594d024$$CancelTaskException leads to FAILED task state$$The {{CancelTaskException}} is thrown to trigger canceling of the executing task. It is intended to cause a cancelled status, rather than a failed status.  Currently, it leads to a {{FAILED}} state instead of the expected {{CANCELED}} state.$$patch1-flink-d594d024_Developer_PatchNaturalnessYe$$make task inner implementation accessible to the cancel ( ) method by the time we switched to running .. Don ' t log cancel ( ) on task fail ( ). Fix typo. Log task cancel in Controller .. $$1
flink-a41bc8cc$$TypeExtractor.analyzePojo has some problems around the default constructor detection$$If a class does have a default constructor, but the user forgot to make it public, then TypeExtractor.analyzePojo still thinks everything is OK, so it creates a PojoTypeInfo. Then PojoSerializer.createInstance blows up.  Furthermore, a "return null" seems to be missing from the then case of the if after catching the NoSuchMethodException which would also cause a headache for PojoSerializer.  An additional minor issue is that the word "class" is printed twice in several places, because class.toString also prepends it to the class name.$$patch1-flink-a41bc8cc_Developer_PatchNaturalnessYe$$Add missing import. Fixed log level for TypeExtractor . does not contain a getter / setter. Fixed log level. Remove debug statements. Improve logging .. Fix log message. $$1
flink-937963e3$$ZooKeeperCheckpointIDCounter.start() can block JobManager actor$$In HA mode, the job manager enables checkpoints during submission of streaming programs.  This leads to call to ZooKeeperCheckpointIDCounter.start(), which communicates with ZooKeeper. This can block the job manager actor.  A solution is to start the counter later instead of the CheckpointCoordinator constructor.$$patch1-flink-937963e3_Developer_PatchNaturalnessYe$$SavepointCoordinator should start the checkpoint id counter thread. $$1
flink-5dfc897b$$FileMonitoring function throws NPE when location is empty$${{StreamExecutionEnvironment.readFileStream()}} does not handle a missing location properly. I would suggest to log that the location is empty and continue running the job.  A test covering the correct behavior is also needed.$$patch1-flink-5dfc897b_Developer_PatchNaturalnessYe$$Remove unused imports. Add missing imports. Added missing warning. $$1
flink-5a86a0a1$$Cannot cancel failing/restarting streaming job from the command line$$I cannot seem to be able to cancel a failing/restarting job from the command line client. The job cannot be rescheduled so it keeps failing:  The exception I get: 13:58:11,240 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Status of job 0c895d22c632de5dfe16c42a9ba818d5 (player-id) changed to RESTARTING. 13:58:25,234 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Trying to cancel job with ID 0c895d22c632de5dfe16c42a9ba818d5. 13:58:25,561 WARN  akka.remote.ReliableDeliverySupervisor                        - Association with remote system [akka.tcp://flink@127.0.0.1:42012] has failed, address is now gated for [5000] ms. Reason is: [Disassociated].$$patch1-flink-5a86a0a1_Developer_PatchNaturalnessYe$$Added special note for failing workflows. Don ' t log canceled job in restart of job state .. $$1
flink-94c8e3fa$$Auxiliary nodes in iterations are not correctly identified as "dynamic" or "static"$$The static/dynamic path tagger starts on the original roots of the step functions, ignoring possible auxiliary nodes that we need to attach to the root (such as NoOps, when the root is a union)$$patch1-flink-94c8e3fa_Developer_PatchNaturalnessYe$$Fix null termination criterion in workset iterations. Fix potential NPE in data flow iterators. $$1
flink-63ef8e86$$Getter/Setter recognition for POJO fields with generics is not working$$Fields like {code} private List<Contributors> contributors; {code}  Are not recognized correctly, even if they have getters and setters. Workaround: make them public.$$patch1-flink-63ef8e86_Developer_PatchNaturalnessYe$$Fix TypeExtractor . equals for boolean fields. Fix TypeExtractor for setters. $$1
flink-d90672fd$$Fix interplay of automatic Operator UID and Changing name of WindowOperator$$WindowOperator can have a changing name because it has the TypeSerializer .toString() output in it's name. For some type serializers that don't implement toString() this means that the name changes.  This means that savepoint restore does not work for the automatically generated UID.$$patch1-flink-d90672fd_Developer_PatchNaturalnessYe$$Add udfOperatorName to hasher. $$1
flink-17e7b423$$Certain Avro generated getters/setters not recognized$$For Avro schemas where value null is not allowed, the field is unboxed e.g. int but the getter/setter methods provide the boxed Integer as interface:  {code} {  "fields": [   {    "type": "double",     "name": "time"   },  } {code}  This results in Java  {code}   private double time;    public java.lang.Double getTime() {     return time;   }    public void setTime(java.lang.Double value) {     this.time = value;   } {code}  There is also a problem when there is an underscore in the Avro schema, e.g.:  {code}   {    "default": null,     "type": [     "null",      "long"    ],     "name": "conn_id"   },  {code}  This results in Java:  {code} private java.lang.Long conn_id;    public java.lang.Long getConnId() {     return conn_id;   }    public void setConnId(java.lang.Long value) {     this.conn_id = value;   } {code}$$patch1-flink-17e7b423_Developer_PatchNaturalnessYe$$Add missing import. Fix TypeExtractor . equals for boolean methods. Fix TypeExtractor . equals for setters. $$1
flink-0078c44e$$POJO serialization NPE$$NullPointer on serialization of a Date field:  Caused by: java.lang.RuntimeException: Error obtaining the sorted input: Thread 'SortMerger Reading Thread' terminated due to an exception: null 	at org.apache.flink.runtime.operators.sort.UnilateralSortMerger.getIterator(UnilateralSortMerger.java:607) 	at org.apache.flink.runtime.operators.RegularPactTask.getInput(RegularPactTask.java:1132) 	at org.apache.flink.runtime.operators.CoGroupDriver.prepare(CoGroupDriver.java:98) 	at org.apache.flink.runtime.operators.RegularPactTask.run(RegularPactTask.java:464) 	... 3 more Caused by: java.io.IOException: Thread 'SortMerger Reading Thread' terminated due to an exception: null 	at org.apache.flink.runtime.operators.sort.UnilateralSortMerger ThreadBase.run(UnilateralSortMerger.java:784) Caused by: java.lang.NullPointerException 	at org.apache.flink.api.common.typeutils.base.DateSerializer.deserialize(DateSerializer.java:72) 	at org.apache.flink.api.common.typeutils.base.DateSerializer.deserialize(DateSerializer.java:1) 	at org.apache.flink.api.java.typeutils.runtime.PojoSerializer.deserialize(PojoSerializer.java:487) 	at org.apache.flink.api.java.typeutils.runtime.TupleSerializer.deserialize(TupleSerializer.java:136) 	at org.apache.flink.api.java.typeutils.runtime.TupleSerializer.deserialize(TupleSerializer.java:30) 	at org.apache.flink.runtime.plugable.ReusingDeserializationDelegate.read(ReusingDeserializationDelegate.java:57) 	at org.apache.flink.runtime.io.network.api.serialization.SpillingAdaptiveSpanningRecordDeserializer.getNextRecord(SpillingAdaptiveSpanningRecordDeserializer.java:111) 	at org.apache.flink.runtime.io.network.api.reader.AbstractRecordReader.getNextRecord(AbstractRecordReader.java:64) 	at org.apache.flink.runtime.io.network.api.reader.MutableRecordReader.next(MutableRecordReader.java:34) 	at org.apache.flink.runtime.operators.util.ReaderIterator.next(ReaderIterator.java:59) 	at org.apache.flink.runtime.operators.sort.UnilateralSortMerger ReadingThread.go(UnilateralSortMerger.java:958) 	at org.apache.flink.runtime.operators.sort.UnilateralSortMerger ThreadBase.run(UnilateralSortMerger.java:781)$$patch1-flink-0078c44e_Developer_PatchNaturalnessYe$$fix merge conflict resolution. fix cql3 error. $$1
flink-434e88fd$$Input type validation often fails on custom TypeInfo implementations$$Input type validation often fails when used with custom type infos. One example of this behaviour can be reproduced by creating a custom type info with our own field type:  StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();  env.generateSequence(1, 10).map(new MapFunction<Long, Tuple1<Optional<Long>>>() { 			@Override 			public Tuple1<Optional<Long>> map(Long value) throws Exception { 				return Tuple1.of(Optional.of(value)); 			} 		}).returns(new TupleTypeInfo<>(new OptionTypeInfo<Long>(BasicTypeInfo.LONG_TYPE_INFO))) 				.keyBy(new KeySelector<Tuple1<Optional<Long>>, Optional<Long>>() {  					@Override 					public Optional<Long> getKey(Tuple1<Optional<Long>> value) throws Exception { 						return value.f0; 					} 				});  This will fail on Input type validation at the KeySelector (or any other function for example a mapper) with the following exception:  Input mismatch: Basic type expected.$$patch1-flink-434e88fd_Developer_PatchNaturalnessYe$$Fixed TypeExtractor . java. Fix TypeExtractor for Java Tuples. Fix generic object type in TypeExtractor. $$1
flink-06e2da35$$CheckpointCoordinator triggers checkpoints even if not all sources are running any more$$When some sources finish early, they will not emit checkpoint barriers any more. That means that pending checkpoint alignments will never be able to complete, locking the flow.$$patch1-flink-06e2da35_Developer_PatchNaturalnessYe$$add missing import. fix a warning. $$1
flink-ed3810b1$$Disable reference tracking in Kryo fallback serializer$$Kryo runs extra logic to track and resolve repeated references to the same object (similar as JavaSerialization)  We should disable reference tracking   - reference tracking is costly   - it is virtually always unnecessary in the datatypes used in Flink   - most importantly, it is inconsistent with Flink's own serialization (which does not do reference tracking)   - It may have problems if elements are read in a different order than they are written.$$patch1-flink-ed3810b1_Developer_PatchNaturalnessYe$$disable reference tracking. $$1
flink-494212b3$$Fix StateDescriptor.readObject$$The readObject method of StateDescriptor uses uses {{ObjectInputStream.read()}}. For very large serialized default values this will not necessarily read all data in one go. We need a loop that reads it in several steps.$$patch1-flink-494212b3_Developer_PatchNaturalnessYe$$Added missing comment. Fix checkstyle. Fix compile error. Add missing /** * *. Added missing javadoc. Add missing @ param javadoc. Fix build .. Fix build. Fix toString ( ) of StateDescriptor. Fix checkstyle. Remove check for data input error. $$1
flink-7164b2b6$$Paths containing a Windows drive letter cannot be used in FileOutputFormats$$Paths that contain a Windows drive letter such as {{file:///c:/my/directory}} cannot be used as output path for {{FileOutputFormat}}.  If done, the following exception is thrown:  {code} Caused by: java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: file:c:         at org.apache.flink.core.fs.Path.initialize(Path.java:242)         at org.apache.flink.core.fs.Path.<init>(Path.java:225)         at org.apache.flink.core.fs.Path.<init>(Path.java:138)         at org.apache.flink.core.fs.local.LocalFileSystem.pathToFile(LocalFileSystem.java:147)         at org.apache.flink.core.fs.local.LocalFileSystem.mkdirs(LocalFileSystem.java:232)         at org.apache.flink.core.fs.local.LocalFileSystem.mkdirs(LocalFileSystem.java:233)         at org.apache.flink.core.fs.local.LocalFileSystem.mkdirs(LocalFileSystem.java:233)         at org.apache.flink.core.fs.local.LocalFileSystem.mkdirs(LocalFileSystem.java:233)         at org.apache.flink.core.fs.local.LocalFileSystem.mkdirs(LocalFileSystem.java:233)         at org.apache.flink.core.fs.FileSystem.initOutPathLocalFS(FileSystem.java:603)         at org.apache.flink.api.common.io.FileOutputFormat.open(FileOutputFormat.java:233)         at org.apache.flink.api.java.io.CsvOutputFormat.open(CsvOutputFormat.java:158)         at org.apache.flink.runtime.operators.DataSinkTask.invoke(DataSinkTask.java:183)         at org.apache.flink.runtime.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:217)         at java.lang.Thread.run(Unknown Source) Caused by: java.net.URISyntaxException: Relative path in absolute URI: file:c:         at java.net.URI.checkPath(Unknown Source)         at java.net.URI.<init>(Unknown Source)         at org.apache.flink.core.fs.Path.initialize(Path.java:240)         ... 14 more {code}$$patch1-flink-7164b2b6_Developer_PatchNaturalnessYe$$Don ' t remove backslash on win32 path separators. Fix ` mkdirs ( ) ` method. $$1
flink-5546a1ef$$TypeExtractor returns wrong type info when a Tuple has two fields of the same POJO type$$Consider the following code:  DataSet<FooBarPojo> d1 = env.fromElements(new FooBarPojo()); 		DataSet<Tuple2<FooBarPojo, FooBarPojo>> d2 = d1.map(new MapFunction<FooBarPojo, Tuple2<FooBarPojo, FooBarPojo>>() { 			@Override 			public Tuple2<FooBarPojo, FooBarPojo> map(FooBarPojo value) throws Exception { 				return null; 			} 		});  where FooBarPojo is the following type: public class FooBarPojo { 	public int foo, bar; 	public FooBarPojo() {} }  This should print a tuple type with two identical fields: Java Tuple2<PojoType<FooBarPojo, fields = [bar: Integer, foo: Integer]>, PojoType<FooBarPojo, fields = [bar: Integer, foo: Integer]>>  But it prints the following instead: Java Tuple2<PojoType<FooBarPojo, fields = [bar: Integer, foo: Integer]>, GenericType<FooBarPojo>>  Note, that this problem causes some co-groups in Gelly to crash with "org.apache.flink.api.common.InvalidProgramException: The pair of co-group keys are not compatible with each other" when the vertex ID type is a POJO, because the second field of the Edge type gets to be a generic type, but the POJO gets recognized in the Vertex type, and getNumberOfKeyFields returns different numbers for the POJO and the generic type.  The source of the problem is the mechanism in TypeExtractor that would detect recursive types (see the "alreadySeen" field in TypeExtractor), as it mistakes the second appearance of FooBarPojo with a recursive field.  Specifically the following happens: createTypeInfoWithTypeHierarchy starts to process the Tuple2<FooBarPojo, FooBarPojo> type, and in line 434 it calls itself for the first field, which proceeds into the privateGetForClass case which correctly detects that it is a POJO, and correctly returns a PojoTypeInfo; but in the meantime in line 1191, privateGetForClass adds PojoTypeInfo to "alreadySeen". Then the outer createTypeInfoWithTypeHierarchy approaches the second field, goes into privateGetForClass, which mistakenly returns a GenericTypeInfo, as it thinks in line 1187, that a recursive type is being processed.  (Note, that if we comment out the recursive type detection (the lines that do their thing with the alreadySeen field), then the output is correct.)$$patch1-flink-5546a1ef_Developer_PatchNaturalnessYe$$Remove unused imports. TypeExtractor now has some useful info. Fix TypeExtractor . java. Fix TypeExtractor. Added private method to TypeExtractor . countTypeInHierarchy ( ). Improved TypeExtractor for POJO where already seen by TypeExtractor. $$1
flink-f2f5bd5b$$Session Window State is Not Checkpointed$$The merging window state in the {{WindowOperator}} is not checkpointed. This means that programs containing session windows will fail upon restore after a failure.  I propose adding a simulated snapshot/restore cycle to the tests in {{WindowOperatorTest}} to catch these problems in the future.$$patch1-flink-f2f5bd5b_Developer_PatchNaturalnessYe$$Add missing import. update missing import. Add a dispose method. Fixing workflow errors in window operator. Fixing merge operator. Fix trigger count bug. Fix warning. Fixing window operator initialization. Fix trigger arguments for window operator .. Fix trigger arguments for window operator .. restore accidentally - deleted merge state in WindowOperator. Add missing import. Add missing persist method to MergingWindowSet .. Fix synchronization issue. Fix context for merge ( ). Fix EvictingWindowOperator processTriggerResult ( ) with the wrong key being passed in the. Fix EvictingWindowOperator processTriggerResult ( ). Fix window evicting bug. $$1
wicket-f539c18c$$PropertyResolver does not scan for NotNull in annotation tree$$When annotating a field of a bean with e.g. org.hibernate.validator.constraints.NotEmpty, this implies javax.validation.constraints.NotNull, but PropertyValidator only checks for the annotations immediately on the filed not the tree of annotations. As a result Wicket does not mark the field as required in the UI, which it should.  Also PropertyResolver.findNotNullConstraints() is not even protected, so cannot be patched in a simple way.  So as a solution I suggest changing findNotNullConstraints() to be protected and rather be something like findConstraints(filter), or findConstraints(clazz), and then in that method method recursively invoking getComposingConstraints to get all constraints, but collecting only those of interest. Possibly some care needs to be taken to prevent infinite recursion where constraints are composed of each other (if that compiles).$$patch1-wicket-f539c18c_Developer_PatchNaturalnessYe$$Added missing import. Added translation to wicket - bean - validation error message. Fixed a typo in PropertyValidator. Allow property validator to apply to default group .. Allow constraint to be applied to default group .. $$1
wicket-8c83c5c5$$NPE in FormComponent#updateCollectionModel in case of no converted input and unmodifiable collection$$There are 2 issues with FormComponent#updateCollectionModel. 1) converted input is not checked for null before wrapping it to ArrayList 2) converted input is not checked for null then model returns unmodifiable collection. The both issues causes NPE.$$patch1-wicket-8c83c5c5_Developer_PatchNaturalnessYe$$reverted changes since they can break backwards compatibility. remove null check. kept old error message for backwards compatibility. $$1
wicket-2d9ebf9a$$Parsing of ChinUnionPay credit card should use the first 6 characters$$User report:  A China UnionPay number has to start with 622 (622126-622925) and has to have a length between 16 and 19. The source code of CreditCardValidator is:    220   	private boolean isChinaUnionPay(String creditCardNumber)   221   	{   222   		cardId = CreditCardValidator.INVALID;   223   		boolean returnValue = false;   224      225   		if ((creditCardNumber.length() >= 16 && creditCardNumber.length() <= 19) &&   226   			(creditCardNumber.startsWith("622")))   227   		{   228   			int firstDigits = Integer.parseInt(creditCardNumber.substring(0, 5));   229   			if (firstDigits >= 622126 && firstDigits <= 622925)   230   			{   231   				cardId = CreditCardValidator.CHINA_UNIONPAY;   232   				returnValue = true;   233   			}   234   		}   235      236   		return returnValue;   237   	} The problem is on the line 228 because the substring returns the first 5 digits and it is compared to 6 digits, so "firstDigits" is always < than 622126. The fix is to do #substring(0, 6).$$patch1-wicket-2d9ebf9a_Developer_PatchNaturalnessYe$$Fixed error .. $$1
wicket-a4caaa57$$AbstractTextComponent not escaping html data by default therefore user text is not redisplayed correctly$$User input is not escaped in all text fields by default (and the default is not configurable).  This leads to user entered text not being redisplayed correctly.  * You can replicate using the project from WICKET-3330. * Just enter the text my&frac12;companyname and press enter * The field will not redisplay the text entered properly$$patch1-wicket-a4caaa57_Developer_PatchNaturalnessYe$$don ' t unescape HTML by default.  wicket - util . string . Strings. don ' t unescape HTML entities when value is not a string. $$1
wicket-b224bad8$$Fixing AjaxTimerBehaviorTest$$This is an attempt to fix failing testcase:  target/surefire-reports/wicket.ajax.AjaxTimerBehaviorTest.txt ------------------------------------------------------------------------------- Test set: wicket.ajax.AjaxTimerBehaviorTest ------------------------------------------------------------------------------- Tests run: 2, Failures: 2, Errors: 0, Skipped: 0, Time elapsed: 0.113 sec <<< FAILURE! testAddToAjaxUpdate(wicket.ajax.AjaxTimerBehaviorTest)  Time elapsed: 0.063 sec  <<< FAILURE! junit.framework.AssertionFailedError: There should be 1 and only 1 script in the markup for this behavior,but 0 were found exp ected:<1> but was:<0>         at junit.framework.Assert.fail(Assert.java:47)         at junit.framework.Assert.failNotEquals(Assert.java:282)         at junit.framework.Assert.assertEquals(Assert.java:64)         at junit.framework.Assert.assertEquals(Assert.java:201)         at wicket.ajax.AjaxTimerBehaviorTest.validateTimerScript(AjaxTimerBehaviorTest.java:178)         at wicket.ajax.AjaxTimerBehaviorTest.validate(AjaxTimerBehaviorTest.java:143)         at wicket.ajax.AjaxTimerBehaviorTest.testAddToAjaxUpdate(AjaxTimerBehaviorTest.java:99)  testAddToWebPage(wicket.ajax.AjaxTimerBehaviorTest)  Time elapsed: 0.026 sec  <<< FAILURE! junit.framework.AssertionFailedError: There should be 1 and only 1 script in the markup for this behavior,but 0 were found exp ected:<1> but was:<0>         at junit.framework.Assert.fail(Assert.java:47)         at junit.framework.Assert.failNotEquals(Assert.java:282)         at junit.framework.Assert.assertEquals(Assert.java:64)         at junit.framework.Assert.assertEquals(Assert.java:201)         at wicket.ajax.AjaxTimerBehaviorTest.validateTimerScript(AjaxTimerBehaviorTest.java:178)         at wicket.ajax.AjaxTimerBehaviorTest.validate(AjaxTimerBehaviorTest.java:155)         at wicket.ajax.AjaxTimerBehaviorTest.testAddToWebPage(AjaxTimerBehaviorTest.java:127)  The attached patch properly handles the case when the callback script is added in body onload.  Also, AbstractAjaxTimerBehavior needs to handle AjaxRequestTarget properly, because adding a body onload has no effect in an ajax request.$$patch1-wicket-b224bad8_Developer_PatchNaturalnessYe$$Add missing import. render delay in < button > element on load. Add callback script for IE8 ( # 77 ). Add js timeout call to target element. $$1
wicket-8967eb2b$$WizardStep FormValidatorWrapper.isActiveStep(WizardStep.java) causes NullPointerException$$Using the Wizard with a nested WizardModel (see RecursiveWizardModel implementation at the attached quickstart application) causes a NullPointerException. I was able to run the same test application with wicket 1.4.18 without any problems.  Caused by: java.lang.NullPointerException         at org.apache.wicket.extensions.wizard.WizardStep FormValidatorWrapper.isActiveStep(WizardStep.java:145)         at org.apache.wicket.extensions.wizard.WizardStep FormValidatorWrapper.getDependentFormComponents(WizardStep.java:109)         at org.apache.wicket.markup.html.form.validation.FormValidatorAdapter.getDependentFormComponents(FormValidatorAdapter.java:47)         at org.apache.wicket.markup.html.form.Form.validateFormValidator(Form.java:1782)         at org.apache.wicket.markup.html.form.Form.validateFormValidators(Form.java:1828)         at org.apache.wicket.markup.html.form.Form.validate(Form.java:1706)         at org.apache.wicket.markup.html.form.Form.process(Form.java:773)         at org.apache.wicket.markup.html.form.Form.onFormSubmitted(Form.java:728)         at org.apache.wicket.markup.html.form.Form.onFormSubmitted(Form.java:670)$$patch1-wicket-8967eb2b_Developer_PatchNaturalnessYe$$remove unused imports. remove unused code. Added init method to each step .. $$1
wicket-087c0a26$$WebSocketRequestHandler is not set as a scheduled and thus RequestCycle#find(AjaxRequestTarget.class) doesn't work$$As discussed at https://groups.google.com/d/topic/wicket-jquery-ui/fw6TdyO5o18/discussion AbstractWebSocketProcessor doesn't schedules the WebSocketRequestHandler in the request cycle and thus it is not reachable for user code via RequestCycle#find(Class) API.  Additionally the configured application RequestCycle listeners are not notified.$$patch1-wicket-087c0a26_Developer_PatchNaturalnessYe$$Fixing license header in WebSocketRequestMapper. Remove unused imports. Remove unused imports. Fix whitespace. Remove duplicated code. Fixed try / catch in AbstractWebSocketProcessor. Fix possible NPE in sendPayload ( ). Adding missing constructor .. $$1
wicket-3cc3fe95$$Component queuing breaks with html tags that don't require close tag.$$Component queuing try to skip to close tag also for those tags that don't have one. This leads to a EmptyStackException (see ArrayListStack#peek).$$patch1-wicket-3cc3fe95_Developer_PatchNaturalnessYe$$add detach operation onMarkupContainer. Allow tag to be dequeueed after having children. $$1
wicket-1ac05533$$PageParameters#mergeWith may loose values of the 'other' PP$$The code at org.apache.wicket.request.mapper.parameter.PageParameters#mergeWith() looks like:  for (NamedPair curNamed : other.getAllNamed()) 		set(curNamed.getKey(), curNamed.getValue());  may loose some values if 'other' has a named parameter with several values.With the current code only the last name/value pair is preserved.$$patch1-wicket-1ac05533_Developer_PatchNaturalnessYe$$added missing non - null values in PageParameters. $$1
wicket-d906576c$$MiniMap.iterator().next() should throw NoSuchElementException$$The wicket.util.collections.MiniMap.iterator().next() should throw NoSuchElementException when there are no more elements to return (line 235), please add: if(i >= size)     throw new NoSuchElementException();$$patch1-wicket-d906576c_Developer_PatchNaturalnessYe$$Add missing import. Added a throw if the map is not empty .. throw exception if index > size ( ) - 1. Added a throw if the map is not empty ( and we ' re about to do it. $$1
wicket-581c7306$$InlineEnclosure are piling up on each render$$InlineEnclosureHandler#resolve() uses an auto-incremented id for its resolved InlineEnclosure,   On the next render, a new instance will be resolved, since the id of the already resolved InlineEnclosure does not match the id in the markup.  But InlineEnclosures are not removed after render as other auto-components, thus all instances pile up in the owning container of the markup.$$patch1-wicket-581c7306_Developer_PatchNaturalnessYe$$Removed unused imports. wicket - enhancer - 1 . 6 - remove inlineEnclosure . java. Remove obsolete code. wicket - core - 1 . 8 - removed inlineEnclosure . getMarkup ( ). add counter to inline enclosures. wicket wicket inline enclosure handler should increment the counter. Removed an unnecessary check for inline enclosures tag id .. $$1
wicket-09166ea8$$onBeforeRender() is called on components that are not allowed to render$$pasted from the list:  Hi,  I ran into an odd problem this week. A model fed to a ListView was calling service methods the current user wasn't allowed to use, and I was wondering how that could happen. A panel far above this ListView in the hierarchy had been secured (using Shiro annotations, but that turns out to not matter at all) and was not supposed to be rendered for this user. From this I had expected the ListView not to be rendered either, but here it was trying to assemble itself in onBeforeRender and thus calling the "forbidden" service methods.  I investigated Component and friends for a bit, and have found a potential problem.  internalBeforeRender() checks determineVisibility() before doing anything. So far so good. determineVisibility() then checks isRenderAllowed() so the application's IAuthorizationStrategy can block certain components. This is where it goes wrong though: isRenderAllowed() only looks at FLAG_IS_RENDER_ALLOWED for performance reasons, and that flag hasn't been set yet! internalPrepareForRender() only calls setRenderAllowed() *after* beforeRender().  Due to this, the supposedly secure panel was going through its own beforeRender and thus calling that ListView's beforeRender.  I think this can be a serious problem, such as in my case described above. I'd expect that if isActionAuthorized(RENDER) is false, the secured component should basically never get to the beforeRender phase. My questions are now:  - Is this intentional? If yes, please explain the reasoning behind it,  because it isn't obvious to me.  - If not, can we fix it? My intuitive suggestion would be to simply  move the call to setRenderAllowed() from the end of  internalBeforeRender() (prepareForRender in 1.4) to the beginning of  that method, so beforeRender() can reliably look at that flag.  - If we can fix it, when and where do we fix it? This hit me in 1.4,  and looking at the code it's still there in 1.5. I'd *really* like it  fixed in the last 1.4 release, and certainly in 1.5, given that this  has the potential to impact security.   It's not an API break, but I'm not sure whether the implications for  application behavior are tolerable for all existing applications. On  the other hand, it seems to be a real security problem, so maybe the  change is justified. I'd like some core dev opinions please :-)  If this is in fact a bug, I'm of course willing to provide a ticket and a patch :-)  Thanks!  Carl-Eric www.wicketbuch.de$$patch1-wicket-09166ea8_Developer_PatchNaturalnessYe$$Allow access to be overriden by workflow / emulator ( IDEADEV - 9314 ). removeAuthorizationException. $$1
wicket-2f1ece4b$$JavaScriptStripper fails with single line comments$$The valid input x++ // x++  gets transformed to x++ x++  which is syntactically invalid. This breaks the unminified version of bootstrap 2.1.1.  The problem doesn't occur with multiline comments because the linebreaks are preserved there.$$patch1-wicket-2f1ece4b_Developer_PatchNaturalnessYe$$Allow backslash to be passed in JavaScriptStripper. $$1
wicket-87ae870f$$"'NEW VALUE' is not a valid Serializable" error during ajax form submission$$I attached a quickstart with a test in TestHomePage#formSubmitsSuccessfully.  The test throws "'NEW VALUE' is not a valid Serializable" error when "NEW VALUE" string in "value" textField is submitted as a part of myForm ajax submission.  The problem is that a call to Objects#convertValue(nonNullNonArrayValue, Object.class) will always return null if nonNullNonArrayValue is a value that is not null and not an array! Shouldn't it always return the first parameter when the second parameter is Object.class?  Sven on Wicket forum suggested to fix this as by adding another if-statement in Objects#convertValue() if (toType.isInstance(value)) {   result = toType.cast(value); }  See the following forum thread for more information http://apache-wicket.1842946.n4.nabble.com/Issues-with-default-type-conversion-in-1-5-td4651857.html$$patch1-wicket-87ae870f_Developer_PatchNaturalnessYe$$Handle case when trying to convert from Java 1 . 5 to 2 .. $$1
wicket-cd414fa5$$Dequeueing problem when there is TransparentWebMarkupContainer around <wicket:child/>$$While testing 7.0.0-M1 release I've found an issue with wicket-bootstrap's sample application.  Here is a minified version of it that reproduces the problem. The two important things are: - a TransparentWebMarkupContainer (TWMC) is attached to <html> tag in the base page - the sub page is requested  It appears that dequeueing logic cannot find the closing tag of the TWMC and thinks that </wicket:child> is the closing tag.$$patch1-wicket-cd414fa5_Developer_PatchNaturalnessYe$$Allow null dequeue action for inline style tags. $$1
wicket-c849f986$$Ajax buttons inside ModalWindows don't submit properly$$I have a ModalWindow that contains an IndicatingAjaxButton. When I click the button, I get a big Java error complaining that the form submit wasn't multipart.  Digging into the javascript in wicket-ajax.js, I found this from line 1102 in the method handleMultipart  {code} multipart=multipart||form.enctype=="multipart/form-data";  if (multipart==false) {      // nothing to handle     return false;  } {code}  When this executed, multipart was false, and enctype was "" and therefore the submit aborted. This may be the cause.  Here's the Java stacktrace  {noformat} java.lang.IllegalStateException: ServletRequest does not contain multipart content 	at org.apache.wicket.protocol.http.servlet.MultipartServletWebRequest.<init>(MultipartServletWebRequest.java:113) 	at org.apache.wicket.protocol.http.servlet.MultipartServletWebRequest.<init>(MultipartServletWebRequest.java:83) 	at org.apache.wicket.extensions.ajax.markup.html.form.upload.MultipartRequest.<init>(MultipartRequest.java:41) 	at org.apache.wicket.extensions.ajax.markup.html.form.upload.UploadWebRequest.newMultipartWebRequest(UploadWebRequest.java:66) 	at org.apache.wicket.markup.html.form.Form.handleMultiPart(Form.java:1651) 	at org.apache.wicket.markup.html.form.Form.onFormSubmitted(Form.java:850) 	at org.apache.wicket.ajax.form.AjaxFormSubmitBehavior.onEvent(AjaxFormSubmitBehavior.java:135) 	at org.apache.wicket.ajax.AjaxEventBehavior.respond(AjaxEventBehavior.java:177) 	at org.apache.wicket.ajax.AbstractDefaultAjaxBehavior.onRequest(AbstractDefaultAjaxBehavior.java:299) 	at org.apache.wicket.request.target.component.listener.BehaviorRequestTarget.processEvents(BehaviorRequestTarget.java:119) 	at org.apache.wicket.request.AbstractRequestCycleProcessor.processEvents(AbstractRequestCycleProcessor.java:92) 	at org.apache.wicket.RequestCycle.processEventsAndRespond(RequestCycle.java:1250) 	at org.apache.wicket.RequestCycle.step(RequestCycle.java:1329) 	at org.apache.wicket.RequestCycle.steps(RequestCycle.java:1428) 	at org.apache.wicket.RequestCycle.request(RequestCycle.java:545) 	at org.apache.wicket.protocol.http.WicketFilter.doGet(WicketFilter.java:479) 	at org.apache.wicket.protocol.http.WicketFilter.doFilter(WicketFilter.java:312) 	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235) {noformat}$$patch1-wicket-c849f986_Developer_PatchNaturalnessYe$$add isMultiPart ( ). Added statics for multiPart .. LPS - 10541 Only set the multiPart flag at runtime when it is set to. Fixed a bug where form is not multiPart. removed unnecessary check for multiPart. Remove multiPart hint on render. setMultiPart ( true ) doesn ' t set multiPart hint value so it won ' t. Added a useful exception message. $$1
wicket-faaae8d3$$404 Error on Nested ModalWindows in IE7 and IE8$$When opening a ModalWindow inside a ModalWindow, the inner ModalWindow generates a 404 error.  Both windows use a PageCreator for content.  To replicate, you must use an actual IE 7 or IE 8 browser, as this does not replicate using developer tools and setting the document and brower to IE 7.  The problem can be seen at http://www.wicket-library.com/wicket-examples/ajax/modal-window.  I will attach a Quickstart as well.$$patch1-wicket-faaae8d3_Developer_PatchNaturalnessYe$$add marker for url in bookmarkable form. wicket - core - request . getClientUrl ( ) + urlStartsWith ( baseUrl , namespace. $$1
wicket-8c827e33$$Header can not be set from IRequestCycleListener#onEndRequest()$$Due to HeaderBufferingWebResponse a header can no longer be set from IRequestCycleListener#onEndRequest().  In 1.4.x this was possible because BufferedWebResponse just passed through all headers to HttpServletResponse.$$patch1-wicket-8c827e33_Developer_PatchNaturalnessYe$$Fixed bug. use the same write method as the other write methods in the same response .. Don ' t reset the buffer of the original response if we are still buffering .. $$1
wicket-1858bc18$$The Url's query parameters are not properly URL encoded$$If page parameter has a value with special characters like ' then it is rendered as it is in the produced markup and is only XML encoded but never URL encoded. This causes broken html for example in the case when a Link is attached to a non- a|area|link tag:   <html><body><span wicket:id="link" onclick="var win = this.ownerDocument.defaultView || this.ownerDocument.parentWindow; if (win == window) { window.location.href=&#039;bookmarkable/org.apache.wicket.MockPageWithLink?urlEscapeNeeded=someone&#039;s+ba+parameter&#039;; } ;return false"></span></body></html>  Notice that &#039; after 'someone' closes the location.href string too early and breaks the app.$$patch1-wicket-1858bc18_Developer_PatchNaturalnessYe$$wicket - request doesn ' t encode query strings by default. $$1
wicket-ffdd0864$$StackOverflowError after form submit with a validation error$$I was not able to find a cause or make a small quickstart, but it has something to do with a form validation, my workaround was to setDefaultFormProcessing(false) or not use required TextFields.  It can be reproduced on https://github.com/krasa/DevSupportApp/tree/stackOverflow  1) run StartVojtitkoDummy 2) go to http://localhost:8765/wicket/bookmarkable/krasa.release.frontend.TokenizationPage 3) click on "Generate Release json" button  - instead of SOE, it should give a validation error, probably even on fields which I would not want to validate, but that's just because I've made the page badly...     {code} java.lang.StackOverflowError: null ... 	at org.apache.wicket.Component.getMarkup(Component.java:755) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:81) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:74) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:66) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:144) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:123) 	at org.apache.wicket.MarkupContainer.visitChildren(MarkupContainer.java:862) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy.searchMarkupInTransparentResolvers(AbstractMarkupSourcingStrategy.java:65) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:99) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.Component.getMarkup(Component.java:755) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:81) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:74) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:66) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:144) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:123) 	at org.apache.wicket.MarkupContainer.visitChildren(MarkupContainer.java:862) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy.searchMarkupInTransparentResolvers(AbstractMarkupSourcingStrategy.java:65) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:99) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.Component.getMarkup(Component.java:755) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:81) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:74) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:66) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:144) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:123) 	at org.apache.wicket.MarkupContainer.visitChildren(MarkupContainer.java:862) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy.searchMarkupInTransparentResolvers(AbstractMarkupSourcingStrategy.java:65) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:99) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.Component.getMarkup(Component.java:755) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:81) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:74) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:66) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:144) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:123) 	at org.apache.wicket.MarkupContainer.visitChildren(MarkupContainer.java:862) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy.searchMarkupInTransparentResolvers(AbstractMarkupSourcingStrategy.java:65) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:99) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.Component.getMarkup(Component.java:755) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:81) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:74) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:66) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:144) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:123) 	at org.apache.wicket.MarkupContainer.visitChildren(MarkupContainer.java:862) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy.searchMarkupInTransparentResolvers(AbstractMarkupSourcingStrategy.java:65) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:99) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.Component.getMarkup(Component.java:755) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:81) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:74) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:66) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:144) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:123) 	at org.apache.wicket.MarkupContainer.visitChildren(MarkupContainer.java:862) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy.searchMarkupInTransparentResolvers(AbstractMarkupSourcingStrategy.java:65) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:99) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.Component.getMarkup(Component.java:755) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:81) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:74) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:66) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:144) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:123) 	at org.apache.wicket.MarkupContainer.visitChildren(MarkupContainer.java:862) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy.searchMarkupInTransparentResolvers(AbstractMarkupSourcingStrategy.java:65) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:99) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.Component.getMarkup(Component.java:755) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:81) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:74) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:66) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:144) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:123) 	at org.apache.wicket.MarkupContainer.visitChildren(MarkupContainer.java:862) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy.searchMarkupInTransparentResolvers(AbstractMarkupSourcingStrategy.java:65) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:99) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.Component.getMarkup(Component.java:755) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:81) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:74) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:66) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:144) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:162) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:162) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:162) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:123) 	at org.apache.wicket.MarkupContainer.visitChildren(MarkupContainer.java:862) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy.searchMarkupInTransparentResolvers(AbstractMarkupSourcingStrategy.java:65) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:99) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.Component.getMarkup(Component.java:755) 	at org.apache.wicket.Component.internalRender(Component.java:2344) 	at org.apache.wicket.Component.render(Component.java:2307) 	at org.apache.wicket.ajax.XmlAjaxResponse.writeComponent(XmlAjaxResponse.java:128) 	at org.apache.wicket.ajax.AbstractAjaxResponse.writeComponents(AbstractAjaxResponse.java:218) 	at org.apache.wicket.ajax.AbstractAjaxResponse.writeTo(AbstractAjaxResponse.java:150) 	at org.apache.wicket.ajax.AjaxRequestHandler.respond(AjaxRequestHandler.java:359) 	at org.apache.wicket.request.cycle.RequestCycle HandlerExecutor.respond(RequestCycle.java:865) 	at org.apache.wicket.request.RequestHandlerStack.execute(RequestHandlerStack.java:64) 	at org.apache.wicket.request.RequestHandlerStack.execute(RequestHandlerStack.java:97) 	at org.apache.wicket.request.cycle.RequestCycle.execute(RequestCycle.java:265) 	at org.apache.wicket.request.cycle.RequestCycle.processRequest(RequestCycle.java:222) 	at org.apache.wicket.request.cycle.RequestCycle.processRequestAndDetach(RequestCycle.java:293) 	at org.apache.wicket.protocol.ws.AbstractUpgradeFilter.processRequestCycle(AbstractUpgradeFilter.java:59) 	at org.apache.wicket.protocol.http.WicketFilter.processRequest(WicketFilter.java:203) 	at org.apache.wicket.protocol.http.WicketFilter.doFilter(WicketFilter.java:284) 	at org.eclipse.jetty.servlet.ServletHandler CachedChain.doFilter(ServletHandler.java:1652) 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:585) 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143) 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:577) 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:223) 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1125) 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:515) 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185) 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1059) 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141) 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:97) 	at org.eclipse.jetty.server.Server.handle(Server.java:497) 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:310) 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:248) 	at org.eclipse.jetty.io.AbstractConnection 2.run(AbstractConnection.java:540) 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:620) 	at org.eclipse.jetty.util.thread.QueuedThreadPool 3.ru {code}$$patch1-wicket-ffdd0864_Developer_PatchNaturalnessYe$$added searchMarkupInTransparentResolvers. Updating imports. Removed unused imports. fixed a warning. Use the transparent resolvers for child components. $$1
wicket-cf6172bd$$PageParametersEncoder should not decode parameters with no name$$From dev@ mailing list: http://markmail.org/message/khuc2v37aakzyfth  PageParametersEncoder should ignore query parameters like "&=&" and "&=value" because they make no sene and lead to exceptions later at PageParameters#add() call.$$patch1-wicket-cf6172bd_Developer_PatchNaturalnessYe$$add missing import. ignore empty query param. $$1
wicket-87fa630f$$BOM in UTF markup file breaks encoding detection$$I have project with internationalization and experienced this problem with one of the pages with non-english content. Page had UTF-8 encoding, but my JVM encoding is different. I always use "<?xml encoding ... ?>" to specify encoding for markup pages (and "MarkupSettings.defaultMarkupEncoding" is not set).  Unexpectedly I got problem with bad encoding on page. After several hours of debugging I found what source of this issue was UTF BOM (Byte order mark) at the beggining of file and inability of "XmlReader" to process it. "XmlReader.getXmlDeclaration" tries to match xml declaration with regular expression, but fails because of BOM. After that encoding defaults to JVM encoding.  It's possible to use "org.apache.commons.io.input.BOMInputStream" to handle BOM or you could handle it manually inside "XmlReader".  PS: issue found with Wicket 1.5.10 and I see same code in 6.12.0 without BOM handling, so I added it to "Affects Version/s", but no proof-in-code available from me at this moment.$$patch1-wicket-87fa630f_Developer_PatchNaturalnessYe$$Added getter for ByteOrderMark. Updated comments. Add better implementation for Changelog. use BOMInputStream ( this . inputStream ) instead of BufferedInputStream ( this . inputStream ) ;. $$1
wicket-a9e56e1e$$Url can't parse urls with username and password$$Url tries to parse the password as the portnumber, because it's after the :, resulting in the following exception: java.lang.NumberFormatException: For input string: "23dc429c-4ffa-4e99-8e24-984571f4c3b6@digdag-rest-dev2.topicusonderwijs.nl" 	java.lang.NumberFormatException.forInputString(NumberFormatException.java:65) 	java.lang.Integer.parseInt(Integer.java:492) 	java.lang.Integer.parseInt(Integer.java:527) 	org.apache.wicket.request.Url.parse(Url.java:276) 	org.apache.wicket.request.Url.parse(Url.java:192) 	org.apache.wicket.protocol.http.servlet.ServletWebResponse.encodeRedirectURL(ServletWebResponse.java:212) 	org.apache.wicket.protocol.http.servlet.ServletWebResponse.sendRedirect(ServletWebResponse.java:236) 	org.apache.wicket.protocol.http.BufferedWebResponse SendRedirectAction.invoke(BufferedWebResponse.java:400) 	org.apache.wicket.protocol.http.BufferedWebResponse.writeTo(BufferedWebResponse.java:588) 	org.apache.wicket.protocol.http.HeaderBufferingWebResponse.stopBuffering(HeaderBufferingWebResponse.java:60) 	org.apache.wicket.protocol.http.HeaderBufferingWebResponse.flush(HeaderBufferingWebResponse.java:97) 	org.apache.wicket.protocol.http.WicketFilter.processRequestCycle(WicketFilter.java:269) 	org.apache.wicket.protocol.http.WicketFilter.processRequest(WicketFilter.java:201) 	org.apache.wicket.protocol.http.WicketFilter.doFilter(WicketFilter.java:282)$$patch1-wicket-a9e56e1e_Developer_PatchNaturalnessYe$$Remove @ sign from end of hostAndPort. Add credentialsAt + 1 to Url . getHostAndPort ( ). $$1
wicket-b672cb2d$$Spaces in path cause ModifcationWatcher to fail$$The ModificationWatcher isn't able to reload resource files if there's a space in the path.  The problem is that Files#getLocalFileFromUrl(String) receives an URL encoded String in which spaces are encoded to %20. They are never decoded and passed to File(). The fix is not to use the external representation of an URL but the file representation.$$patch1-wicket-b672cb2d_Developer_PatchNaturalnessYe$$add missing import. try java 8 on file names with spaces ( case insensitive ). $$1
wicket-2293764f$$Base url is incorrect for error dispatched pages$$The fix for https://issues.apache.org/jira/browse/WICKET-4387 includes the following code in org.apache.wicket.protocol.http.servlet.ServletWebRequest#ServletWebRequest(HttpServletRequest httpServletRequest, String filterPrefix, Url url):  if (forwardAttributes != null || errorAttributes != null) 		{ 			if (LOG.isDebugEnabled()) 			{ 				LOG.debug("Setting filterPrefix('{}') to '' because there is either an error or a forward. {}, {}", 						new Object[] {filterPrefix, forwardAttributes, errorAttributes}); 			} 			// the filter prefix is not needed when the current request is internal 			// see WICKET-4387 			this.filterPrefix = "";  The filterPrefix is actually needed later when a request is made due to an error (e.g. 404):  public Url getClientUrl() 	{ 		if (errorAttributes != null && !Strings.isEmpty(errorAttributes.getRequestUri())) 		{ 			String problematicURI = Url.parse(errorAttributes.getRequestUri(), getCharset()) 				.toString(); 			return getContextRelativeUrl(problematicURI, filterPrefix);  With filterPrefix=="" the urls for any resources in the error page are wrong.$$patch1-wicket-2293764f_Developer_PatchNaturalnessYe$$Remove debug code. $$1
wicket-204849bc$$PackageMapper - Could not resolve class$$It seems that the PackageMapper try to resolve much more than it is supposed to do, for instance if I've 2 pages test1/TestPage1 and test2/TestPage2 then it tries to resolve test2/TestPage1 when I reach the page1...   WARN  - WicketObjects              - Could not resolve class [com.mycompany.test2.TestPage1] java.lang.ClassNotFoundException: com.mycompany.test2.TestPage1     at org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.java:1714)     at org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.java:1559)     at java.lang.Class.forName0(Native Method)     at java.lang.Class.forName(Class.java:270)     at org.apache.wicket.application.AbstractClassResolver.resolveClass(AbstractClassResolver.java:108)     at org.apache.wicket.core.util.lang.WicketObjects.resolveClass(WicketObjects.java:71)     at org.apache.wicket.core.request.mapper.AbstractComponentMapper.getPageClass(AbstractComponentMapper.java:134)     at org.apache.wicket.core.request.mapper.PackageMapper.parseRequest(PackageMapper.java:152)     at org.apache.wicket.core.request.mapper.AbstractBookmarkableMapper.mapRequest(AbstractBookmarkableMapper.java:322)     at org.apache.wicket.request.mapper.CompoundRequestMapper.mapRequest(CompoundRequestMapper.java:152)     at org.apache.wicket.request.cycle.RequestCycle.resolveRequestHandler(RequestCycle.java:189)     at org.apache.wicket.request.cycle.RequestCycle.processRequest(RequestCycle.java:219)     at org.apache.wicket.request.cycle.RequestCycle.processRequestAndDetach(RequestCycle.java:293)     at org.apache.wicket.protocol.http.WicketFilter.processRequestCycle(WicketFilter.java:261)     at org.apache.wicket.protocol.http.WicketFilter.processRequest(WicketFilter.java:203)     at org.apache.wicket.protocol.http.WicketFilter.doFilter(WicketFilter.java:284)$$patch1-wicket-204849bc_Developer_PatchNaturalnessYe$$score is always 0 if url starts with any path segment. Removed backwards incompatible method. wicket - 5056 - modify getCompatibilityScore ( Request ) to return 0 if url starts with. $$1
wicket-ce172da8$$Return error code 400 when an Ajax request has no base url set in header/request parameters.$$Hello,  currently we've got a problem with faked ajax requests. these ajax  requests misses some parameters, but the wicket-ajax header flag is set.  So ServletWebRequest throws an exception:  java.lang.IllegalStateException: Current ajax request is missing the base url header or parameter          at org.apache.wicket.util.lang.Checks.notNull(Checks.java:38)          at org.apache.wicket.protocol.http.servlet.ServletWebRequest.getClientUrl(ServletWebRequest.java:171)          at org.apache.wicket.request.UrlRenderer.<init>(UrlRenderer.java:59)   These faked requests are so massive, that our application is no longer  monitorable. Our workaround rejects these requests via apache config.   Instead of logging an exception, in deployment mode wicket should log a warning and reject the request$$patch1-wicket-ce172da8_Developer_PatchNaturalnessYe$$add missing import. Remove unused import. Adding a throw if the base url is not found .. $$1
wicket-def03add$$StackOverflowError when calling getObject() from load() in LDM$$The fix for WICKET-5772 caused an infinite loop when calling getObject() from inside load() in LoadableDetachableModel. While of course unwise to do so and nobody in their right mind would do so directly, such a cycle can be triggered through a series of unrelated calls emanating from load().$$patch1-wicket-def03add_Developer_PatchNaturalnessYe$$Fixed a typo .. Added toString ( ) to LoadableDetachableModel. Allow model to be attached .. Fixed a regression in how - to - detach. Allow detaching of transient object after detaching ( IDEADEV - 12901 ). Prevent infinite loop on attached state changes. Set attaching state to ATTACHED in onAttach. Fixed isAttached ( ). always append the transient model object to the toString ( ). Allow the model to be attached to another thread. $$1
wicket-b19a3d69$$WicketTester.assertRedirectUrl always fails because it always thinks the redirect was null$$I have a page which always redirects.  When I write a test for this page, tester.assertRedirectUrl(...) always fails with the assertion failure showing that the redirect URL was null.  The page does redirect when running the application for real and I have stepped through in the debugger when running the test and it goes all the way to the HttpServletResponse.sendRedirect call.  However, in the same debugging session, tester.getLastResponse().getRedirectLocation() == null  Cut-down example follows.   public class AlwaysRedirectPage extends WebPage {     public AlwaysRedirectPage()     {         // redirects to another web server on the same computer         throw new RedirectToUrlException("http://localhost:4333/");     } }  public class TestAlwaysRedirectPage {     @Test     public void test()     {         WicketTester tester = new WicketTester();         tester.startPage(AlwaysRedirectPage.class);         tester.assertRedirectUrl("http://localhost:4333/");     } }$$patch1-wicket-b19a3d69_Developer_PatchNaturalnessYe$$added missing status code. $$1
wicket-f45ce896$$WebApplication#unmount() unmounts the whole compound mapper if some of its inner ones matches$$From dev@ mailing lists: http://markmail.org/message/wmdgbrhvrvaeygvr  WebApplication.unmount() calls getRootRequestMapperAsCompound(), and  calls unmount() on that.  getRootRequestMapperAsCompound() checks if the root request mapper is a  compound, if not, wraps it in a compound, sets the compound as root and  returns the compound.  CompoundRequestMapper.unmount() identifies which of the mappers added  directly to the compound handle the url, and removes them.  The problem: If the original root mapper was a single wrapper, or layer of wrappers,  with the actual mounted mapper wrapped some levels down, then the whole  wrapper is removed, not just the specific MountedMapper that is wrapped.  This has the effect of removing every single mapper, leaving root mapper  as an empty compound.  I would like to attempt to provide a patch to fix this, but would like  guidance on the approach. I have come up with three approaches:  1. Introduce interface IWrappedRequestMapper. This will be an interface  which has one method: IRequestMapper getWrappedRequestMapper(). We can  then have all wrapper mappers implement this and work down the tree to  find the correct MountedMapper (wicket 6) to remove.  2. Have WebApplication hold a reference to a specific  CompoundRequestMapper, and have all mount()/unmount() operations add and  remove from this mapper. This compound would need to be added to the  default list during init. This makes it complicated to work out how to  do things like have CryptoMapper not apply to mounted pages.  3. Add method unmount() to IRequestMapper, so that wrappers can  delegate. This obviously can only be done in wicket 7, but we're making  mounting a problem of every single request mapper, when not even  Application cares about mounting.$$patch1-wicket-f45ce896_Developer_PatchNaturalnessYe$$update compound request mapper. $$1
wicket-7ae109a6$$Using render strategy ONE_PASS_RENDER fails for Ajax requests$$I have an application which has two pages. Page A has an AjaxLink which makes some checks and either sets some error feedback and stays on the same page (e.g. login page with "Invalid user" error) or if everything is OK then redirects to page B (via setResponsePage(B.class)). The problem comes when the current render strategy is ONE_PASS_RENDER. In this case no matter that fromUrl and toUrl are different and the request is Ajax the current code directly writes the page markup to the response. I think it should trigger a redirect instead. I am not sure whether it should be redirect to render or to buffer ...$$patch1-wicket-7ae109a6_Developer_PatchNaturalnessYe$$fix broken wicket. $$1
wicket-d110e307$$wicket-bean-validation: Bean validation PropertyValidator only works with direct field access$$There's a substring indexing bug in the wicket-bean-validation module in org.apache.wicket.bean.validation.DefaultPropertyResolver that causes it to only work with direct field access and fail when field is missing and getter method should be used.  The problem is on this line:      String name = getter.getName().substring(3, 1).toLowerCase() + getter.getName().substring(4);  Which should be:      String name = getter.getName().substring(3, 4).toLowerCase() + getter.getName().substring(4);  (or simply a single character access)$$patch1-wicket-d110e307_Developer_PatchNaturalnessYe$$Added missing import. Extend default property resolver for property resolved models. Fixed issue with bean - validation plugin. $$1
wicket-55eb5212$$NPE when using ComponentRenderer.renderComponent on a panel with <wicket:enclosure>$$Hi,  Consider this example: <wicket:panel> 	<wicket:enclosure child="externalLink"> 		<a wicket:id="externalLink">Link</a> 	</wicket:enclosure> </wicket:panel>  When trying to render such a panel with ComponentRenderer.renderComponent, a NPE is thrown because Wicket try to render Enclosure without initializing it.  Root cause: java.lang.NullPointerException 	at org.apache.wicket.markup.html.internal.Enclosure.isVisible(Enclosure.java:143) 	at org.apache.wicket.Component.determineVisibility(Component.java:4363) 	at org.apache.wicket.Component.internalBeforeRender(Component.java:916) 	at org.apache.wicket.Component.beforeRender(Component.java:991) 	at org.apache.wicket.Component.internalPrepareForRender(Component.java:2214) 	at org.apache.wicket.Component.render(Component.java:2303) 	at org.apache.wicket.MarkupContainer.renderNext(MarkupContainer.java:1390) 	at org.apache.wicket.MarkupContainer.renderAll(MarkupContainer.java:1554) 	at org.apache.wicket.MarkupContainer.renderComponentTagBody(MarkupContainer.java:1529) 	at org.apache.wicket.MarkupContainer.renderAssociatedMarkup(MarkupContainer.java:689) 	at org.apache.wicket.markup.html.panel.AssociatedMarkupSourcingStrategy.renderAssociatedMarkup(AssociatedMarkupSourcingStrategy.java:76) 	at org.apache.wicket.markup.html.panel.PanelMarkupSourcingStrategy.onComponentTagBody(PanelMarkupSourcingStrategy.java:112) 	at org.apache.wicket.Component.internalRenderComponent(Component.java:2549) 	... 29 more  See the attached quickstart.  I've looked a little into it, and it seems that RenderPage (used by ComponentRenderer to render components) is never initialized. Therefore the panel's children are never initialized too (see MarkupContainer l.930), and this causes Enclosure to have a null childComponent.  Thanks.$$patch1-wicket-55eb5212_Developer_PatchNaturalnessYe$$Added missing line. $$1
wicket-cd281092$$Localization messages stops working with validators since 1.4-rc2$$With the previous 1.3.6 and 1.4-rc1 releases I was capable to restrict a localization message for a validation to only one wicket id e.g. :  in foobar.java RequiredTextField nameTF = new RequiredTextField("name"); nameTF.add(StringValidator.lengthBetween(2, 255)); nameTF.add(new PatternValidator("[^|:]*"));  and in foobar.properties name.Required=some text name.StringValidator.range=some other text name.PatternValidator=some other text again  So, like this I could have to create an another RequiredTextField named "password", and attach to it a different localization message (for example "password.Required=blabla").  But somehow with the 1.4-rc2-5 it looks like that this function is broken, it only recognizes the localization text, when I remove the "name." prefix from my property.$$patch1-wicket-cd281092_Developer_PatchNaturalnessYe$$added localizer for form component key. $$1
wicket-6051019b$$A (stateless) page immediately disappears after the first render$$Using setResponsePage(new SomeStatelessNonBookmarkablePage(aParameter)) renders the page but trying to reload the page in the browser fails with PageExpiredException.  The reason is that the page is stateless and thus it is not saved in the page stores. Since it was scheduled for render with setResponsePage(Page) method its Url is created by PageInstanceMapper (i.e. something like: wicket/page?1). An attempt to refresh such page fails with "Page with id '1' is not found => PageExpiredException".  Igor suggested to call 'page.setStatelessHint(false)' for all pages passed to PageProvider(IRequestablePage) constructor, i.e. such pages must be stored. This solved the problem but exposed few more problems: - MockPageManager (used in WicketTester) until now always touched/stored pages, no matter their statelessness - org.apache.wicket.markup.html.internal.EnclosureTest.testRender10() was wrong for some unknown reason. All expectations against EnclosurePageExpectedResult_10-2.html should not have the enclosure rendered because "input" component is invisible$$patch1-wicket-6051019b_Developer_PatchNaturalnessYe$$add missing import. Set statelessHint to false for Page instances. Removed special treatment for page stateless endpoints .. Removed special treatment for page stateless endpoints .. Don ' t ignore page statesless tests. $$1
wicket-2624d2db$$SmartLinkLabel failing to process email with -$$In a similar vein to WICKET-3174 - using SmartLinkLabel with an email address that includes a "-" generates a link only on the right-most part of the address.   Example:  - my-test@example.com  Will generate a link like:  - my-<a href="mailto:test@example.com">test@example.com</a>   The addition of the "-" char is a valid email address format.$$patch1-wicket-2624d2db_Developer_PatchNaturalnessYe$$Adding quotes around email address in DefaultLinkParser. $$1
wicket-8e6a6ec5$$Fragment and Component with same id fail with misleading exception$$A page having a component from inherited markup *and* fragment with the *same* id fails with misleading exception message.  Exception message: The component(s) below failed to render. Possible reasons could be that: 1) you have added a component in code but forgot to reference it in the markup (thus the component will never be rendered), 2) if your components were added in a parent container then make sure the markup for the child container includes them in <wicket:extend> ... and list of component id's from fragment multiplied by amount of rows in DataTable  Cause: The markup of the component is used by the fragment.$$patch1-wicket-8e6a6ec5_Developer_PatchNaturalnessYe$$wicket - authors add < wicket : fragment > tag. $$1
wicket-64656c98$$Using ajax to update a component that has an AbstractTransformerBehavior attached throws a ClassCastException$$Using ajax to update a component that has an AbstractTransformerBehavior attached throws a ClassCastException:  java.lang.ClassCastException: org.apache.wicket.ajax.AjaxRequestTarget AjaxResponse cannot be cast to org.apache.wicket.request.http.WebResponse       at org.apache.wicket.markup.transformer.AbstractTransformerBehavior.beforeRender(AbstractTransformerBehavior.java:68)        at org.apache.wicket.Component.notifyBehaviorsComponentBeforeRender(Component.java:3421)       at org.apache.wicket.Component.internalRender(Component.java:2344)       at org.apache.wicket.Component.render(Component.java:2273)        at org.apache.wicket.MarkupContainer.renderNext(MarkupContainer.java:1474)       at org.apache.wicket.MarkupContainer.renderAll(MarkupContainer.java:1638)       at org.apache.wicket.MarkupContainer.renderComponentTagBody(MarkupContainer.java:1613)        at org.apache.wicket.MarkupContainer.onComponentTagBody(MarkupContainer.java:1567)       at org.apache.wicket.markup.html.form.Form.onComponentTagBody(Form.java:1570)       at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.onComponentTagBody(DefaultMarkupSourcingStrategy.java:72)        at org.apache.wicket.Component.internalRenderComponent(Component.java:2515)       at org.apache.wicket.MarkupContainer.onRender(MarkupContainer.java:1576)       at org.apache.wicket.Component.internalRender(Component.java:2345)        at org.apache.wicket.Component.render(Component.java:2273)       at org.apache.wicket.MarkupContainer.renderNext(MarkupContainer.java:1474)       at org.apache.wicket.MarkupContainer.renderAll(MarkupContainer.java:1638)        at org.apache.wicket.MarkupContainer.renderComponentTagBody(MarkupContainer.java:1613)       at org.apache.wicket.MarkupContainer.renderAssociatedMarkup(MarkupContainer.java:735)       at org.apache.wicket.markup.html.panel.AssociatedMarkupSourcingStrategy.renderAssociatedMarkup(AssociatedMarkupSourcingStrategy.java:76)        at org.apache.wicket.markup.html.panel.PanelMarkupSourcingStrategy.onComponentTagBody(PanelMarkupSourcingStrategy.java:112)       at org.apache.wicket.Component.internalRenderComponent(Component.java:2515)       at org.apache.wicket.MarkupContainer.onRender(MarkupContainer.java:1576)        at org.apache.wicket.Component.internalRender(Component.java:2345)       at org.apache.wicket.Component.render(Component.java:2273)       at org.apache.wicket.ajax.AjaxRequestTarget.respondComponent(AjaxRequestTarget.java:982)        at org.apache.wicket.ajax.AjaxRequestTarget.respondComponents(AjaxRequestTarget.java:796)       at org.apache.wicket.ajax.AjaxRequestTarget.constructResponseBody(AjaxRequestTarget.java:676)       at org.apache.wicket.ajax.AjaxRequestTarget.respond(AjaxRequestTarget.java:637)        at org.apache.wicket.request.cycle.RequestCycle HandlerExecutor.respond(RequestCycle.java:712)       at org.apache.wicket.request.RequestHandlerStack.execute(RequestHandlerStack.java:63)       at org.apache.wicket.request.RequestHandlerStack.execute(RequestHandlerStack.java:96)        at org.apache.wicket.request.cycle.RequestCycle.processRequest(RequestCycle.java:208)       at org.apache.wicket.request.cycle.RequestCycle.processRequestAndDetach(RequestCycle.java:251)       at org.apache.wicket.protocol.http.WicketFilter.processRequest(WicketFilter.java:162)        at org.apache.wicket.protocol.http.WicketFilter.doFilter(WicketFilter.java:218)       at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)       at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:233)       at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:175)       at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:128)        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:102)       at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109)       at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:286)        at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:844)       at org.apache.coyote.http11.Http11Protocol Http11ConnectionHandler.process(Http11Protocol.java:583)       at org.apache.tomcat.util.net.JIoEndpoint Worker.run(JIoEndpoint.java:447)        at java.lang.Thread.run(Thread.java:619)$$patch1-wicket-64656c98_Developer_PatchNaturalnessYe$$Fix warning. Remove redundant constructor .. temporarily set response to the same value as previous. Fixed an error in AbstractTransformerBehavior .. $$1
wicket-556a2236$$Do not use the parsed PageParameters when re-creating an expired page$$WICKET-4014 and WICKET-4290 provided functionality to re-create an expired page if there is a mount path in the current request's url. There is a minor problem with that because the page parameters are passed to the freshly created page. I.e. parameters for a callback behavior are now set as page construction parameters. Since the execution of the behavior is ignored for the recreated page these parameters should be ignored too.$$patch1-wicket-556a2236_Developer_PatchNaturalnessYe$$add WICKET - 4594 - re - creating an expired page instance if pageId is. $$1
wicket-f1e854b3$$Value exchange in a wicket:message throws an exception$$i tried to exchange values in a <wicket:message> like described in wiki <https://cwiki.apache.org/WICKET/wickets-xhtml-tags.html#Wicket%27sXHTMLtags-Elementwicket:message>. But i get an exception: ERROR - RequestCycle               - No get method defined for class: class org.apache.wicket.markup.resolver.MarkupInheritanceResolver TransparentWebMarkupContainer expression: vat1value org.apache.wicket.WicketRuntimeException: No get method defined for class: class org.apache.wicket.markup.resolver.MarkupInheritanceResolver TransparentWebMarkupContainer expression: vat1value at org.apache.wicket.util.lang.PropertyResolver.getGetAndSetter(PropertyResolver.java:488) at org.apache.wicket.util.lang.PropertyResolver.getObjectAndGetSetter(PropertyResolver.java:330) at org.apache.wicket.util.lang.PropertyResolver.getObjectAndGetSetter(PropertyResolver.java:237) ...  Maybe it's caused by usage of border. I've debugged a bit, but could get a real glue.  I added a quick start with test case.$$patch1-wicket-f1e854b3_Developer_PatchNaturalnessYe$$Fixed error where id was not being retrieved. Fixed error where a component could not be resolved. $$1
wicket-5f69685d$$HeaderResponse.renderCSSReference does not render context path relative url, but wicket filter url-pattern relative url$$In an application with a wicket filter url-pattern different than /*, if you use HeaderResponse.renderCSSReference(String url), where url is a context-path-relative url (css/main.css, for example), the generated css link is not context relative, but wicket url-pattern relative.$$patch1-wicket-5f69685d_Developer_PatchNaturalnessYe$$Remove unused imports. Allow relative url to be passed as arguments. add missing default variable. $$1
wicket-4d7f7359$$isVisibleInHierarchy() possibly unnecessarily checks children whose parents are invisible?$$Hi!  See attached quickstart with junit test reproducing the bug. See also patch proposal.  I have a page with two panels:  page.form.add(panel1); page.form.add(panel2);  in some situations panel1 is not visible.  However, a form submit event will visit all formcomponents of panel1 via:         at org.apache.wicket.markup.html.form.FormComponent.visitFormComponentsPostOrder(FormComponent.java:400)        at org.apache.wicket.markup.html.form.Form.visitFormComponentsPostOrder(Form.java:1209)        at org.apache.wicket.markup.html.form.Form.inputChanged(Form.java:1403)        at org.apache.wicket.markup.html.form.Form.onFormSubmitted(Form.java:865)  This results in a crash because panel1 components are not prepared to be invoked via isvisible when the panel itself is not visible.  I wonder if the component.isVisibleInHierarchy could be changed as follows, to first check parent visibility:   public final boolean isVisibleInHierarchy()  {    Component component = this;    while (component != null)    {      Component componentParent = component.getParent();       if (((componentParent == null) || componentParent.isVisibleInHierarchy()) && component.determineVisibility())      {        component = componentParent;      }      else      {        return false;      }    }    return true;  }  Similar change could/should maybe be possible also for isEnabledInHierarchy ?$$patch1-wicket-4d7f7359_Developer_PatchNaturalnessYe$$Removed unused static final metadata key. update metadata. $$1
wicket-84c3baac$$INullAcceptingValidator behavior seems broken in 1.5-RC4.2$$As discussed in this forum thread: http://apache-wicket.1842946.n4.nabble.com/INullAcceptingValidator-behavior-tp3570352p3570352.html  It appears that Wicket no longer calls INullAcceptingValidator intances when the validatable value is null.  Wicket wraps validators  as behaviors, using the adapter pattern. The adapter class (org.apache.wicket.validation.ValidatorAdapter) implements  the interface IValidator<T>. This "hides" the case where the actual validator is an INullAcceptingValidator. Therefore, when going through a component's attached validators, the code of org.apache.wicket.markup.html.form.FormComponent will never call INullAcceptingValidators when the value is null.$$patch1-wicket-84c3baac_Developer_PatchNaturalnessYe$$fixed NPE in form component. $$1
wicket-2abc18f1$$TableTree's NodeBorder does not properly close divs$$NodeBorder fails to properly close generated <div>s.$$patch1-wicket-2abc18f1_Developer_PatchNaturalnessYe$$added missing closing < div > tag. $$1
wicket-f3ec1503$$XmlPullParser doesn't parse correctly attributes with complex namespace$$Having a markup like: <a class="addthis_button_google_plusone_badge" g:plusone:size="smallbadge"  g:plusone:href="https://plus.google.com/25252/"></a> causes XmlPullParser to throw the following exception:  java.text.ParseException: Same attribute found twice: g:plusone (line 19, column 100)      at org.apache.wicket.markup.parser.XmlPullParser.parseTagText(XmlPullParser.java:673)      at org.apache.wicket.markup.parser.XmlPullParser.next(XmlPullParser.java:294)      at org.apache.wicket.markup.parser.filter.RootMarkupFilter.nextElement(RootMarkupFilter.java:58) .....$$patch1-wicket-f3ec1503_Developer_PatchNaturalnessYe$$wicket - util parse bug. $$1
wicket-b1f4e6a3$$URL IPv6 parsing$$There is an issue with native IPv6 address parsing. "https://[::1]/myapp", URL parsing fails:  org.apache.wicket.request.Url.parse("https://[::1]/myapp") generates an exception: java.lang.NumberFormatException: For input string: "1]" at java.lang.NumberFormatException.forInputString( NumberFormatException.java:65) at java.lang.Integer.parseInt(Integer.java:492)  However, "https://[::1]:80/myapp" works as expected.$$patch1-wicket-b1f4e6a3_Developer_PatchNaturalnessYe$$workflow improvements - added workflow improvements for IPv6 reverse links. add missing closing qr bracket. $$1
wicket-0f8a2990$$Regression: "Could not find child with id: <ID> in the wicket:enclosure" for non-component tag$$Attached testcase passes with wicket-1.4.1 but fails with 1.4.2 saying:  org.apache.wicket.WicketRuntimeException: Could not find child with id: radio in the wicket:enclosure 	at org.apache.wicket.markup.html.internal.Enclosure.checkChildComponent(Enclosure.java:210) 	at org.apache.wicket.markup.html.internal.Enclosure.ensureAllChildrenPresent(Enclosure.java:249) 	at org.apache.wicket.markup.html.internal.Enclosure.onComponentTagBody(Enclosure.java:169) 	at org.apache.wicket.Component.renderComponent(Component.java:2626) 	at org.apache.wicket.MarkupContainer.onRender(MarkupContainer.java:1512) 	at org.apache.wicket.Component.render(Component.java:2457) 	at org.apache.wicket.MarkupContainer.autoAdd(MarkupContainer.java:229) 	at org.apache.wicket.markup.resolver.EnclosureResolver.resolve(EnclosureResolver.java:61) 	at org.apache.wicket.markup.resolver.ComponentResolvers.resolve(ComponentResolvers.java:81) 	at org.apache.wicket.MarkupContainer.renderNext(MarkupContainer.java:1418) 	at org.apache.wicket.MarkupContainer.renderComponentTagBody(MarkupContainer.java:1577) 	at org.apache.wicket.MarkupContainer.onComponentTagBody(MarkupContainer.java:1501) 	at org.apache.wicket.Component.renderComponent(Component.java:2626) 	at org.apache.wicket.MarkupContainer.onRender(MarkupContainer.java:1512) 	at org.apache.wicket.Component.render(Component.java:2457) 	at org.apache.wicket.MarkupContainer.renderNext(MarkupContainer.java:1414) 	at org.apache.wicket.MarkupContainer.renderAll(MarkupContainer.java:1528) 	at org.apache.wicket.Page.onRender(Page.java:1545) 	at org.apache.wicket.Component.render(Component.java:2457) 	at org.apache.wicket.Page.renderPage(Page.java:914) 	at org.apache.wicket.request.target.component.BookmarkablePageRequestTarget.respond(BookmarkablePageRequestTarget.java:262) 	at org.apache.wicket.request.AbstractRequestCycleProcessor.respond(AbstractRequestCycleProcessor.java:105) 	at org.apache.wicket.RequestCycle.processEventsAndRespond(RequestCycle.java:1258) 	at org.apache.wicket.RequestCycle.step(RequestCycle.java:1329) 	at org.apache.wicket.RequestCycle.steps(RequestCycle.java:1428) 	at org.apache.wicket.RequestCycle.request(RequestCycle.java:594) 	at org.apache.wicket.protocol.http.MockWebApplication.processRequestCycle(MockWebApplication.java:478) 	at org.apache.wicket.protocol.http.MockWebApplication.processRequestCycle(MockWebApplication.java:390) 	at org.apache.wicket.util.tester.BaseWicketTester.startPage(BaseWicketTester.java:300) 	at org.apache.wicket.EnclosurePageTest.testRender(EnclosurePageTest.java:23) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) 	at java.lang.reflect.Method.invoke(Method.java:597) 	at junit.framework.TestCase.runTest(TestCase.java:154) 	at junit.framework.TestCase.runBare(TestCase.java:127) 	at junit.framework.TestResult 1.protect(TestResult.java:106) 	at junit.framework.TestResult.runProtected(TestResult.java:124) 	at junit.framework.TestResult.run(TestResult.java:109) 	at junit.framework.TestCase.run(TestCase.java:118) 	at junit.framework.TestSuite.runTest(TestSuite.java:208) 	at junit.framework.TestSuite.run(TestSuite.java:203) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) 	at java.lang.reflect.Method.invoke(Method.java:597) 	at org.apache.maven.surefire.junit.JUnitTestSet.execute(JUnitTestSet.java:213) 	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.executeTestSet(AbstractDirectoryTestSuite.java:140) 	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.execute(AbstractDirectoryTestSuite.java:127) 	at org.apache.maven.surefire.Surefire.run(Surefire.java:177) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) 	at java.lang.reflect.Method.invoke(Method.java:597) 	at org.apache.maven.surefire.booter.SurefireBooter.runSuitesInProcess(SurefireBooter.java:345) 	at org.apache.maven.surefire.booter.SurefireBooter.main(SurefireBooter.java:1009)$$patch1-wicket-0f8a2990_Developer_PatchNaturalnessYe$$Do not apply visibility on components in HTML view , because we do not yet support it .. Added missing params. Oops , forgot a piece. Restore original child visibility after detaching Enclosure .. $$1
wicket-499a9c6b$$FLAG_INHERITABLE_MODEL and default model change$$The issue is about correctness of Component#setDefaultModel (Component#setModelImpl) method behavior. I expect that the flag FLAG_INHERITABLE_MODEL should be checked there and turned off in case if new model is not a IComponentInheritedModel.   Let check the next code: public MyPanel(String id) {  super(id);   ...   form.setModel(new CompoundPropertyModel(this));   DropDownChoice ddc = new DropDownChoice("variant", Arrays.ofList(...)) {    // p1     @Override     protected void onInitialize() {        super.onInitialize();        setModel(new DefaultingWrapModel(getModel(), Model.of("default value"));            // p2     }   };   ddc.setNullValid(false);   ddc.setRequired(true);   form.add(ddc);   ... }  In the (p1) the DDC will initialize with CompoundPropertyModel and the FLAG_INHERITABLE_MODEL will be turned on soon by the first invocation of FormComponent#getModel().   In the (p2) we wrap the DDC model with the model which provide the default value (DefaultingWrapModel implements IWrapModel). So we change the model, but the FLAG_INHERITABLE_MODEL is still turned on. On the Component#detach() event, the method Component#setModelImpl(null) will be invoked for the ddc and the DefaultingWrapModel instance will be lost:  		// reset the model to null when the current model is a IWrapModel and 		// the model that created it/wrapped in it is a IComponentInheritedModel 		// The model will be created next time. 		if (getFlag(FLAG_INHERITABLE_MODEL)) 		{ 			setModelImpl(null); 			setFlag(FLAG_INHERITABLE_MODEL, false); 		}  I think that such behavior is unexpected.  http://apache-wicket.1842946.n4.nabble.com/1-4-15-FLAG-INHERITABLE-MODEL-and-default-model-change-td3252093.html$$patch1-wicket-499a9c6b_Developer_PatchNaturalnessYe$$reset ' inherited model ' flag if model is not IComponentInheritedModel. $$1
wicket-c1c1f794$$ListenerInterfaceRequestHandler#respond throws ComponentNotFoundException as a side-effect$$The following exception occurs instead of a generic WicketRuntimeException:  16:27:56.181 WARN  (RequestCycle.java:343) Handling the following exception [qtp9826071-207] org.apache.wicket.core.request.handler.ComponentNotFoundException: Could not find component 'xyz' on page 'class MyPage        at org.apache.wicket.core.request.handler.PageAndComponentProvider.getComponent(PageAndComponentProvider.java:182) ~[org.apache.wicket.core_6.12.0.jar:6.12.0]        at org.apache.wicket.core.request.handler.ListenerInterfaceRequestHandler.getComponent(ListenerInterfaceRequestHandler.java:90) ~[org.apache.wicket.core_6.12.0.jar:6.12.0]        at org.apache.wicket.core.request.handler.ListenerInterfaceRequestHandler.respond(ListenerInterfaceRequestHandler.java:231) ~[org.apache.wicket.core_6.12.0.jar:6.12.0]        at org.apache.wicket.request.cycle.RequestCycle HandlerExecutor.respond(RequestCycle.java:861) ~[org.apache.wicket.core_6.12.0.jar:6.12.0]        at org.apache.wicket.request.RequestHandlerStack.execute(RequestHandlerStack.java:64) ~[org.apache.wicket.request_6.12.0.jar:6.12.0]        at org.apache.wicket.request.cycle.RequestCycle.execute(RequestCycle.java:261) [org.apache.wicket.core_6.12.0.jar:6.12.0]        at org.apache.wicket.request.cycle.RequestCycle.processRequest(RequestCycle.java:218) [org.apache.wicket.core_6.12.0.jar:6.12.0]        at org.apache.wicket.request.cycle.RequestCycle.processRequestAndDetach(RequestCycle.java:289) [org.apache.wicket.core_6.12.0.jar:6.12.0]        at org.apache.wicket.protocol.http.WicketFilter.processRequestCycle(WicketFilter.java:259) [org.apache.wicket.core_6.12.0.jar:6.12.0]  in fact, this is a side effect, if you look at the code:         @Override        public void respond(final IRequestCycle requestCycle)        {              final IRequestablePage page = getPage();              final boolean freshPage = pageComponentProvider.isPageInstanceFresh();              final boolean isAjax = ((WebRequest)requestCycle.getRequest()).isAjax();              IRequestableComponent component = null;              try              {                     component = getComponent();              }              catch (ComponentNotFoundException e)              {                     // either the page is stateless and the component we are looking for is not added in the                     // constructor                     // or the page is stateful+stale and a new instances was created by pageprovider                     // we denote this by setting component to null                     component = null;              }              if ((component == null && freshPage) ||                      (component != null && getComponent().getPage() == page))               {              [....]              }               else               {                      throw new WicketRuntimeException("Component " + getComponent() +                            " has been removed from page.");               }         }  You see that getComponent() is called twice.  1) Once guarded by a catch   - and - 2) once unguarded  So if the component can't be found AND freshPage is false, as a sideeffect instead of the WicketRuntimeException with the removed message a componentnotfound exception is raised as a side effect.  I see two possible solutions for this  a) either it is intentional that a ComponentNotFoundException is thrown, then it should be thrown from the catch block like               catch (ComponentNotFoundException e)              {                     if (!freshPage) {                        throw e;                     }              }  b) if it is unintentionall in the else case ther should be a simple check like this   if (component == null) {                         throw new WicketRuntimeException("Component for path " + getPath() +                           " and page "+page.getClass().getName()+" has been removed from page.");                     } else {                        throw new WicketRuntimeException("Component " + component +                           " has been removed from page.");                     }   Beside this: it would be a good idea to mention at least the page class in either case.$$patch1-wicket-c1c1f794_Developer_PatchNaturalnessYe$$Add logging for listener interface. $$1
wicket-f1af9e03$$Adding behavior in component instantiation listener causes Page.onInitialize() being called even if constructor throws an exception$$Page.onInitialize() will be called even if constructor throws an exception in case below code is added in wicket WebApplication.init(): getComponentInstantiationListeners().add(new IComponentInstantiationListener() {               @Override               public void onInstantiation(Component component) {                   component.add(new Behavior() {                    });               }                         });  It seems that the instantiation listener adds the behavior to the page at very start of the page constructor, and then the page is marked as dirty to cause onInitialize() being called afterwards.$$patch1-wicket-f1af9e03_Developer_PatchNaturalnessYe$$Removed unneeded init .. Add missing Override annotation. Added constructor in Application should set the init method to be invoked in onNewComponent. add init method. $$1
wicket-8ccb1f6d$$IResourceCachingStrategy implementations should only set caching if version matches$$Implementations of IResourceCachingStrategy (FilenameWithVersionResourceCachingStrategy and QueryStringWithVersionResourceCachingStrategy) should only set cache duration to maximum if the version matches. Currently, if a user requests a resource with an arbitrary version, the version will be cached for one year (WebResponse.MAX_CACHE_DURATION). So people could polute proxy caches with potentially upcoming version.$$patch1-wicket-8ccb1f6d_Developer_PatchNaturalnessYe$$Use calculated version if available. Use the calculated version if it is the same as the request .. $$1
wicket-4f08e6f2$$CryptoMapper does not work for applications having a home page that needs query parameters$$CryptoMapper.decryptUrl() should not return null for requests like http://myhost/MyApplication/app/?param=xx   As a possible fix one can replace  if (encryptedUrl.getSegments().isEmpty() && encryptedUrl.getQueryParameters().isEmpty()) {            return encryptedUrl; }  with   if (encryptedUrl.getSegments().isEmpty()) {            return encryptedUrl; }  but I suspect that the original test is intended to answer to another use case...$$patch1-wicket-4f08e6f2_Developer_PatchNaturalnessYe$$Improved detection of url - params in AuthProvider .. $$1
wicket-12e1f39b$$CreditCardValidator accepts invalid inputs$$(1) The onValidate() method of the CreditCardValidator class returns true for invalid inputs with null or unicode character such as 4\0\0\0\0\0\0\0\0\0\0\0\0\0\0.  (2) Also there is no length check on the input, therefore even invalid length inputs such as 9845 are accepted.  (3) There is no check for invalid issuer identifier, i.e.,  840898920205250 is accepted, where 84XXXX is not a valid issuer identifier$$patch1-wicket-12e1f39b_Developer_PatchNaturalnessYe$$Fixed a bug in Controller & Accounts .. Allow credit card validation to proceed without error .. $$1
wicket-44a4132f$$Broken Link in Tomcat because of Page Mount$$I post this message on the user mailing List (http://apache-wicket.1842946.n4.nabble.com/Broken-Link-in-Tomcat-because-of-Page-Mount-tt4659663.html) and Martin Grigorov asked me, to create a ticket on Jira.  Broken Link in Tomcat because of Page Mount  Following situation: -I have a Wicket Application(6.8.0) which runs under the context "webapp" on a Tomcat 7.0.41 -I mount a Page with two parameters (this is important) in the WicketApplication. 	mountPage("/mount/ {parameter1}/ {parameter2}", MountedPage.class); -The mounted Page(MountedPage.class) has only a simple Link -There are two links on the HomePage to the mounted Page.  They are declared as follows:   	add(new Link<Void>("link") { 			@Override 			public void onClick() { 				setResponsePage(MountedPage.class, linkParameters); 			} 	});  	add(new Link<Void>("brokenLink") { 			@Override 			public void onClick() { 				setResponsePage(new MountedPage(linkParameters)); 			} 	}); 	 I deploy this Application as a war file on a Tomcat under the context "webapp". When I call the first Link on the HomePage and then the Link on the mounted Page, everything works fine.  But if I call the second Link and then the Link on the mounted Page, the link is broken. The context is missing in the generated link 	http://localhost:8080/wicket/bookmarkable/com.mycompany.LinkedPage  Does anyone have an idea, why the second link does not work on Tomcat?  I add a Quickstart and the war file as attachment.  Ps: Both links works fine in Jetty.  Pss:If I remove the mount command, both links will work in  Tomcat too.$$patch1-wicket-44a4132f_Developer_PatchNaturalnessYe$$Allow placeholder to be mounted on page view. $$1
wicket-c863b032$$CryptoMapper encrypts external URLs in ResourceReferences making the resources inaccessible$$Short Description:   CryptoMapper encrypts links to resources with URLs of the form:  - http://domain/path/script.js  - /local/absolute/path/script.js  Additionally there might be some inconsistencies in handling URLs in instances of ResourceReference.  The problem occurs when JavaScript resources are included in the following way:  @Override public void renderHead(IHeaderResponse response) { 	super.renderHead(response); 	 	UrlResourceReference reference = new UrlResourceReference(Url.parse("http://domain/path/script.js")); 	response.render(reference); }  The resulting JavaScript links can't be loaded (404 is returned) when CryptoMapper is used.  This is a minor problem, because the following always works for JavaScript files not served by Wicket ("external JavaScript files"):  response.render(new StringHeaderItem("<script type=\"text/javascript\" src=\"//domain/myPath/manual.js\"></script>");   Ways to reproduce:     A code example for wicket-examples is attached (example.zip)   Local URLs:      http://localhost:8080/enc/index      http://localhost:8080/unenc/index   Possible fix:    - disable encryption for URLs beginning with '/', '<schema>://' and '//' and not served/filtered by Wicket   (  - define different reference classes for external files and files served/filtered by Wicket, issue warnings when a wrong URL type is supplied by the user or treat URLs beginning with '/', '<schema>://' and '//' differently  )  Thank you$$patch1-wicket-c863b032_Developer_PatchNaturalnessYe$$update Authors. update javadoc. Fix checkstyle. do not encrypt full urls. remove whitespace from handler variable. Fixed typo in Controller docs. Fixed typo in cryptomapper. Fix typo in HTML page encrypted url due to incorrect checksum .. Added missing javadoc. $$1
wicket-ef3adb12$$TabbedPanel CSS "last" is wrong if last step is not visible$$TabbedPanel renders a "last" CSS class for the last tab, this fails however if the last tab is not visible.$$patch1-wicket-ef3adb12_Developer_PatchNaturalnessYe$$@@ - 188 - TabbedPanel should set visibilityCache to a proper variable. wicket extensions doesn ' t have access to VisiblityCache. Use the cache for TabbedPanel when the tab is selected .. fixed a small bug. Fixed a typo in TabbedPanel javadocs ( # 971 ). fixed a typo in TabbedPanel. Add one method. $$1
wicket-d558004b$$Problem with setting of IComponentInheritedModel and FLAG_INHERITABLE_MODEL$$Described in the mailing list: [http://mail-archives.apache.org/mod_mbox/wicket-users/201407.mbox/%3CCAF2_608c8TOZjprV8Md15KJpRET6YQdXHe%3DwRzF-y5G_zAXcDg%40mail.gmail.com%3E]  I'm aware of the another issue ([https://issues.apache.org/jira/browse/WICKET-3413]) which dealt with the exact same code - and I believe there was a mistake in the solution that leads to this issue.  Please see the attached quickstart (including a JUnit test) to reproduce the error.$$patch1-wicket-d558004b_Developer_PatchNaturalnessYe$$reset ' inherited model ' when model is explicitely set. remove component inherited model flag. $$1
wicket-7c364566$$DatePicker issues with locale medium date format$$DateTextField as follows: DateTextField d = new DateTextField(id, model, new StyleDateConverter("M-",false));  Case 1: - en-US locale - for example DatePicker insert 04 rather than Apr in DateTextField, even though pattern clearly says MMM  Case 2: - pl-PL locale - 2010-10-25 is in the DateTextField - DatePicker opens on April 2031 rather than October 2010  I believe the problem lies in wicket-date.js, in functions substituteDate and parseDate.  I know this might be duplicate of WICKET-2427 and WICKET-2375, but apparently this hasn't been properly fixed yet.$$patch1-wicket-7c364566_Developer_PatchNaturalnessYe$$Updating imports .. Fix copy / paste error .. Fix copy / paste error. Add withLocale to DateTimeConverter. $$1
wicket-b4274415$$StringValueConversionException for correct situation$$StringValue.toOptionalLong() produces org.apache.wicket.util.string.StringValueConversionException if empty string was passed. Let me suggest, that this behavior should be changes for all toOptionalXXX methods except getOptionalString method.  The problem in inner code:  The problem in following code:  public final Long toOptionalLong() throws StringValueConversionException     {         return (text == null) ? null : toLongObject();     }  Should be something like this:  The problem in following code:  public final Long toOptionalLong() throws StringValueConversionException     {         return Strings.isEmpty() ? null : toLongObject();     }  But there is another problem: what to do if incorrect param was passed - for example "abc" for parameter of Long type?$$patch1-wicket-b4274415_Developer_PatchNaturalnessYe$$Handle empty string texts as null / empty .. Fix documentation. $$1
wicket-a4459ef4$$Event broadcast type "Depth" does not work when the sink is a Component but not a MarkupContainer$$Event broadcast type "Depth" does not work when the sink is a Component but not a MarkupContainer. In this case, no sinks receive the event.$$patch1-wicket-a4459ef4_Developer_PatchNaturalnessYe$$Added breadth - first traversal of Broadcast. Added details on how Broadcast is applied. Forgot to call dispatchEvent ( ) in Controller. $$1
wicket-3431e60d$$missing base64/ URL encoding$$yesterday i showed the concept of omponents to a friend and stumled into something i dont understand and think it might be a bug.    I have a small panelcompoment that holds a searchform (textfield + submit) nothing special here, the code behind looks like:     @Override         public void onSubmit()          {             String suchFeld = getSuchfeld();             if(suchFeld.length()>0)             {                 PageParameters params = new PageParameters();                 params.add("finde",suchFeld);                 setResponsePage(Suche.class,params);             }             else             {                 setResponsePage(getPage().getClass());             }         }   the component is put into a "BasePage":    public BasePage() {         ....             add(bar);         add(new SuchPanel("SuchPanel"));         ..... }   wich is then extended by the real page:   public class Foo extends BasePage{          /** Creates a new instance of Zigarren */     public Foo() {         }   wich works all fine, however if the class name contains non ascii letters (e.g:    etc.) it gives me a bug if nothing is entered into the search and the part   public class Zubehr extends BasePage{          /** Creates a new instance of Zubehr */     public Zubehr() {     }   "setResponsePage(getPage().getClass());" comes to action, the trouble is that the page might have the URL: ?wicket:bookmarkablePage=:de.pages.Zubeh%C3%B6r but the form tries to go to :  wicket:bookmarkablePage=:de.pages.Zubeh%F6r   wich results in a CODE 404 in the App Server$$patch1-wicket-3431e60d_Developer_PatchNaturalnessYe$$encodeURIComponent ( String ) and remove backslash. $$1
wicket-6a8fc1cc$$MarkupNotFoundException when refreshing a component with AJAX inside a TransparentWebMarkupContainer$$A component placed inside a TransparentWebMarkupContainer, added to its parent cannot be refreshed with AJAX. See quickstart.$$patch1-wicket-6a8fc1cc_Developer_PatchNaturalnessYe$$added missing import. Added missing TODO .. removed unused import. Adding missing import. Adding missing @ Override annotation. Added missing @ Override annotation. Add missing @ Override annotation. Reduced the number of times that a component resolver is found. Added empty line to the top of the HTML header so that we can override existing code. Added IComponentResolver to imports. Added a protected method to searchMarkupInTransparentResolvers ( ). "avoid "" multiple renders "" error ( IDEADEV - 21852 )". $$1
wicket-cda34428$$multiple <style> tags in header are rendered incorrectly$$I created a small quickstart.  The BasePage has some multiple <style> tags. Only he first one is rendered correctly, all following render the tag body only, the surrounding <style></style> is missing.$$patch1-wicket-cda34428_Developer_PatchNaturalnessYe$$Removed unused imports. use the new StringHeaderItem constructor here too. Remove debug code. $$1
wicket-5efb8091$$Unable to find markup for children of deeply nested IComponentResolvers during Ajax response$$Component hierarchy: Page -> WebMarkupContainer -> IComponentResolver (that uses Page to resolve) and Page -> Panel.  Markup hierarchy: Page -> WebMarkupContainer -> IComponentResolver -> Panel.  When rendering whole page, it works, because it is markup driven. Wicket encounters ComponentTag for Panel and resolves the Panel using IComponentResolver, which retrieves the Panel from the Page.  When you add the Panel to an AjaxRequestTarget, the render is component driven. In order to render the Panel, we must retrieve the markup for the Panel from its parent MarkupContainer, which happens to be the Page.  Markup.java around line 230 skips to closing tags of ComponentTag, so when Page gets to the opening tag of the WebMarkupContainer, it skips to the closing tag of the WebMarkupContainer, and so passes over the ComponentTag for Panel without noticing it. There is actually another check, in DefaultMarkupSourcingStrategy, that tries to fetch from all the "transparent" components in the markup container, but this is not good enough, because in our example, the IComponentResolver is not actually a direct child of the Panel's parent, to it is never used to try find the markup.  One solution might be to traverse the tree, and attempt to find the markup from all IComponentResolving MarkupContainers, but we should be careful. I'm a bit concerned at how various parts of Wicket just assume that an IComponentResolver is transparent and resolves from its direct parent only.  If we do go down the route of traversing the tree to find IComponentResolvers, then try find the markup from each of them, we really should add a check in AbstractMarkupSourcingStrategy#searchMarkupInTransparentResolvers() to check that the Component that the IComponentResolver resolves for the markup id is the same component for which we are looking for markup.  This is a difficult one. I am working around it for the mean time, just recording the difficulty here. Will try make a patch when I can.$$patch1-wicket-5efb8091_Developer_PatchNaturalnessYe$$Add missing import. Stops unnecessary component resolvers on component .. $$1
wicket-ba516f02$$FormTester throws an exception when a Palette component is added to a Form associated with a compound property model$$FormTester throws an exception when a Palette component is added to a Form associated with a compound property model: org.apache.wicket.WicketRuntimeException: No get method defined for class ... expression: choices  It worked fine in Wicket 6.5.0, and works fine if the form is not associated with a compound property model.$$patch1-wicket-ba516f02_Developer_PatchNaturalnessYe$$Added missing javadoc. $$1
wicket-9aec4f33$$@SpringBean(name="something", required=false) still throws org.springframework.beans.factory.NoSuchBeanDefinitionException: No bean named 'something' is defined$$Example:  {code} public class TwitterLoginLink extends StatelessLink<Void> { 			 	@SpringBean(name="twitterMgr", required=false) 	private TwitterManager twitterMgr; {code}  still throws:  {code} org.springframework.beans.factory.NoSuchBeanDefinitionException: No bean named 'twitterMgr' is defined at org.springframework.beans.factory.support.DefaultListableBeanFactory.getBeanDefinition(DefaultListableBeanFactory.java:641) at org.springframework.beans.factory.support.AbstractBeanFactory.getMergedLocalBeanDefinition(AbstractBeanFactory.java:1159) at org.springframework.beans.factory.support.AbstractBeanFactory.isSingleton(AbstractBeanFactory.java:418) at org.springframework.context.support.AbstractApplicationContext.isSingleton(AbstractApplicationContext.java:1002) at org.apache.wicket.spring.SpringBeanLocator.isSingletonBean(SpringBeanLocator.java) at org.apache.wicket.spring.injection.annot.AnnotProxyFieldValueFactory.getFieldValue(AnnotProxyFieldValueFactory.java:141) at org.apache.wicket.injection.Injector.inject(Injector.java:111) at org.apache.wicket.spring.injection.annot.SpringComponentInjector.inject(SpringComponentInjector.java:124) at org.apache.wicket.spring.injection.annot.SpringComponentInjector.onInstantiation(SpringComponentInjector.java:130) at org.apache.wicket.application.ComponentInstantiationListenerCollection 1.notify(ComponentInstantiationListenerCollection.java:38) at org.apache.wicket.application.ComponentInstantiationListenerCollection 1.notify(ComponentInstantiationListenerCollection.java:34) at org.apache.wicket.util.listener.ListenerCollection.notify(ListenerCollection.java:80) at org.apache.wicket.application.ComponentInstantiationListenerCollection.onInstantiation(ComponentInstantiationListenerCollection.java:33) at org.apache.wicket.Component.<init>(Component.java:686) at org.apache.wicket.MarkupContainer.<init>(MarkupContainer.java:121) at org.apache.wicket.markup.html.WebMarkupContainer.<init>(WebMarkupContainer.java:52) at org.apache.wicket.markup.html.link.AbstractLink.<init>(AbstractLink.java:57) at org.apache.wicket.markup.html.link.AbstractLink.<init>(AbstractLink.java:44) at org.apache.wicket.markup.html.link.Link.<init>(Link.java:105) at org.apache.wicket.markup.html.link.StatelessLink.<init>(StatelessLink.java:42) at org.soluvas.web.login.twitter.TwitterLoginLink.<init>(TwitterLoginLink.java:40) at org.soluvas.web.login.DedicatedLoginPanel FormSignIn.<init>(DedicatedLoginPanel.java:90) at org.soluvas.web.login.DedicatedLoginPanel.onInitialize(DedicatedLoginPanel.java:58) at org.apache.wicket.Component.fireInitialize(Component.java:876) at org.apache.wicket.MarkupContainer 3.component(MarkupContainer.java:967) at org.apache.wicket.MarkupContainer 3.component(MarkupContainer.java:963) at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:144) at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:123) at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:192) at org.apache.wicket.MarkupContainer.visitChildren(MarkupContainer.java:875) at org.apache.wicket.MarkupContainer.internalInitialize(MarkupContainer.java:962) at org.apache.wicket.Page.isPageStateless(Page.java:463) at org.apache.wicket.core.request.mapper.AbstractBookmarkableMapper.getPageInfo(AbstractBookmarkableMapper.java:447) at org.apache.wicket.core.request.mapper.AbstractBookmarkableMapper.mapHandler(AbstractBookmarkableMapper.java:391) at org.apache.wicket.core.request.mapper.MountedMapper.mapHandler(MountedMapper.java:395) at org.apache.wicket.request.mapper.CompoundRequestMapper.mapHandler(CompoundRequestMapper.java:215) at org.apache.wicket.request.cycle.RequestCycle.mapUrlFor(RequestCycle.java:429) at org.apache.wicket.request.handler.render.WebPageRenderer.respond(WebPageRenderer.java:273) at org.apache.wicket.core.request.handler.RenderPageRequestHandler.respond(RenderPageRequestHandler.java:175) at org.apache.wicket.request.cycle.RequestCycle HandlerExecutor.respond(RequestCycle.java:862) at org.apache.wicket.request.RequestHandlerStack.execute(RequestHandlerStack.java:64) at org.apache.wicket.request.cycle.RequestCycle.execute(RequestCycle.java:261) at org.apache.wicket.request.cycle.RequestCycle.processRequest(RequestCycle.java:218) at org.apache.wicket.request.cycle.RequestCycle.processRequestAndDetach(RequestCycle.java:289) at org.apache.wicket.protocol.http.WicketFilter.processRequestCycle(WicketFilter.java:259) at org.apache.wicket.protocol.http.WicketFilter.processRequest(WicketFilter.java:201) at org.apache.wicket.protocol.http.WicketFilter.doFilter(WicketFilter.java:282) at org.soluvas.web.site.SecuredWicketAtmosphereHandler CustomFilterChain.doFilter(SecuredWicketAtmosphereHandler.java:199) at org.apache.shiro.web.servlet.AbstractShiroFilter.executeChain(AbstractShiroFilter.java:449) at org.apache.shiro.web.servlet.AbstractShiroFilter 1.call(AbstractShiroFilter.java:365) at org.apache.shiro.subject.support.SubjectCallable.doCall(SubjectCallable.java:90) at org.apache.shiro.subject.support.SubjectCallable.call(SubjectCallable.java:83) at org.apache.shiro.subject.support.DelegatingSubject.execute(DelegatingSubject.java:383) at org.apache.shiro.web.servlet.AbstractShiroFilter.doFilterInternal(AbstractShiroFilter.java:362) at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125) at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:344) at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:261) at org.soluvas.web.site.SecuredWicketAtmosphereHandler CustomFilterChain.doFilter(SecuredWicketAtmosphereHandler.java:199) at org.soluvas.web.site.SecuredWicketAtmosphereHandler CustomFilterChain.invokeFilterChain(SecuredWicketAtmosphereHandler.java:185) at org.soluvas.web.site.SecuredWicketAtmosphereHandler.onRequest(SecuredWicketAtmosphereHandler.java:91) at org.atmosphere.cpr.AsynchronousProcessor.action(AsynchronousProcessor.java:187) at org.atmosphere.cpr.AsynchronousProcessor.suspended(AsynchronousProcessor.java:98) at org.atmosphere.container.Tomcat7CometSupport.service(Tomcat7CometSupport.java:96) at org.atmosphere.cpr.AtmosphereFramework.doCometSupport(AtmosphereFramework.java:1809) at org.atmosphere.cpr.AtmosphereServlet.event(AtmosphereServlet.java:255) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilterEvent(ApplicationFilterChain.java:484) at org.apache.catalina.core.ApplicationFilterChain.doFilterEvent(ApplicationFilterChain.java:377) at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:220) at org.apache.catalina.core.StandardContextValve.__invoke(StandardContextValve.java:123) at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java) at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:502) at org.apache.catalina.core.StandardHostValve.__invoke(StandardHostValve.java:171) at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java) at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:99) at org.apache.catalina.valves.AccessLogValve.invoke(AccessLogValve.java:953) at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:118) at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:408) at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1023) at org.apache.coyote.AbstractProtocol AbstractConnectionHandler.process(AbstractProtocol.java:589) at org.apache.tomcat.util.net.AprEndpoint SocketWithOptionsProcessor.run(AprEndpoint.java:1810) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) {code}  Workaround:  {code}     @Autowire(required=false)     private TwitterManager twitterMgr; {code}$$patch1-wicket-9aec4f33_Developer_PatchNaturalnessYe$$fixed annot support ; ). Allow the field to be cached in the same way as if it was wrapped in the last commit. Use the correct bean name for annotations. fix missing return statement. add missing import. add notNull arguments. Remove deprecated javadoc. remove final modifier. Fix broken build. $$1
wicket-d79d0192$$Bounds error in PageableListView#getCurrentPage()$$In the getCurrentPage() method of class PageableListView, the following code:  while ((currentPage * rowsPerPage) > getList().size()) {            currentPage--; }  checks if "first cell if out of range". However, the index of that first cell is (currentPage * rowsPerPage), and then the comparison with getList().size() should use a ">=" instead a ">".$$patch1-wicket-d79d0192_Developer_PatchNaturalnessYe$$improve readme. $$1
wicket-84bbbf68$$Incorrect URL for setResponsePage() within a Form#onSubmit( )$$If the WebApplication uses IRequestCycleSettings.RenderStrategy.ONE_PASS_RENDER, the issue described and exemplified in the attached quickstart at  https://issues.apache.org/jira/browse/WICKET-3442  prevails.   Clicking the link on /pageone results in this URL: /pageone?0-1.IFormSubmitListener-form$$patch1-wicket-84bbbf68_Developer_PatchNaturalnessYe$$Change default request redirect policy to ALWAYS_REDIRECT. Fix wicket request redirect policy. wicket 1 . 4 . 3 - old test. $$1
wicket-c62b66c1$$Interaction betwen IAjaxRegionMarkupIdProvider, renderPlaceholderTag and visibility$$I've just discovered what I think is a bug with IAjaxRegionMarkupIdProvider. We are using it on a Behavior that provides a border to form components (label, mandatory marker, etc), which for the most part works great.  We have encountered a problem when toggling the visibility of a form component with this behavior via ajax.   The component is first sent out visible and the markup is all correct.  A change elsewhere on the page causes the component to be set to not visible and redrawn via ajax. The ajax response contains a tag with a markupid generated via renderPlaceholderTag. This does not take into account the  IAjaxRegionMarkupIdProvider behaviour.  Another change happens on the page causing the component to become visible, and the ajax replace can't happen because the component with the correct markupId is not present.$$patch1-wicket-c62b66c1_Developer_PatchNaturalnessYe$$Remove unused imports. respond missing getAjaxRegionMarkupId. Add ability to override getAjaxRegionMarkupId ( Component ). add missing import. added missing AJAX reference to avoid JS exceptions on WC. Add ability to override getAjaxRegionMarkupId ( ). $$1
wicket-8f7805f8$$AutocompleteTextField after Submit does not work$$I use an AutocompleteTextfield together with a submit-Button. After once submitting the content oft the AutocompleteTextField the parameter q is added to the URL. After that the autocompletion will only complete the parameter q in the url and not the parameter given by ajax.  I tracked the problem down to the callbackURL.  It contains a pattern looking as follows: ....&q=<paramproducedbysubmit>&q=<paramproducedbyajaxautocomplete>  The callbackurl is build of the parameter q and the extraction of parameters only accepts the first parameter$$patch1-wicket-8f7805f8_Developer_PatchNaturalnessYe$$Removed redundant code. $$1
wicket-6ce34ccf$$Minified name resolves incorrectly if default resource reference is used$$In PackageResourceReference.  When a default reference to a minified resource is used (i.e. the resource wasn't mounted) the resource reference name includes '.min'.   When trying to resolve the minified name, another '.min' is appended, resulting in the minified name resolving to 'html5.min.min.js'.   As a result, the PackageResourceReference concludes that the resource was not minified, and adds compression.$$patch1-wicket-6ce34ccf_Developer_PatchNaturalnessYe$$Extend PackageResourceReference to include min . suffix. $$1
wicket-0eb596df$$FencedFeedbackPanel is broken with RefreshingView(and it's implementations)$$FencedFeedbackPanel doesn't work correctly if inner form(s) are in RefreshingView(or it's implementations).. in this case outerform feedbackpanel just starts including messages meant for inner feedbackpanel. with ListView FencedFeedbackPanel works correctly.. actually one user(Mike Dundee) created this issue in quickview https://github.com/vineetsemwal/quickview/issues/19  so in that link he has described his problem and pasted the code you can use to reproduce ... there i have also explained why it's broken with RefreshingView and it's implementations currently(it's a little complex so i am trying to avoid explaining all again ,also english is not my first language :-) )   thank you !$$patch1-wicket-0eb596df_Developer_PatchNaturalnessYe$$Adding in depth expanded feedback panel. Added author tag. Added missing star. Added missing Javadoc. Added missing javadoc. Added missing javadoc. Added code to incrementFenceCount ( ). remove unused variable. removed circular reference in IFeedbackMessageFilter. "Revert "" remove componentIsMarkedAsFence ( Component ) "" from recursion". count FencedFeedbackPanel fences per added event. $$1
wicket-5e2c6702$$Wicket 1.5 RC-3 Bug with conditional comments$$IE Conditional Comments with script block causes malformed HTML on Chrome and Firefox.$$patch1-wicket-5e2c6702_Developer_PatchNaturalnessYe$$Fix IE7 bug in JavaScriptUtils. $$1
wicket-b293b75c$$HomePageMapper ignores request to '/' with query string parameters$$Issue a request to http://host:port/contextpath/?something Wicket will log an error message like: ERROR - RequestCycle               - Unable to execute request. No suitable RequestHandler found. URL=?something  I think the reason is in HomePageMapper which maps to the configured home page only if there are no query parameters.  HomePageMapper.java: {code} public IRequestHandler mapRequest(Request request) 	{ 		if (request.getUrl().getSegments().size() == 0 && 			request.getUrl().getQueryParameters().size() == 0) 		{ 			return new RenderPageRequestHandler(new PageProvider(getContext().getHomePageClass())); 		} 		else 		{ 			return null; 		} 	} {code}$$patch1-wicket-b293b75c_Developer_PatchNaturalnessYe$$Set the page source for the home page .. $$1
wicket-557de7bc$$FileUpload writeToTempFile() method throws NPE for sessionless requests$$I have created stateless page with stateless form containing FileUploadField, however when I tried to post file to it, NPE was thrown.  The issue is caused by method FileUpload#writeToTempFile() method trying to use session id as temp file prefix.   Workaround: create temp file manually and use method FileUpload#writeToFile( myTempFile)$$patch1-wicket-557de7bc_Developer_PatchNaturalnessYe$$add missing import. add tempFileName used for form uploads. $$1
wicket-bb7f9cf5$$WebPage#onAfterRender erroneously reports missing header$$In WebPage#onAfterRender() there's a check wether a header was missing on a page and header contributions would be lost.  In the following case this check erroneously barks: - page A was requested - in A's onBeforeRender() a RestartResponseAtInterceptPageException to page B is thrown - page A's onAfterRender() is invoked in a finally block - processing continues with page B  Page A's onAfterRender() complains about the missing header, althought his page was never completely rendered.  IMHO there's a check missing in WebPage#onAfterRender():      	if (getRequestCycle().getResponsePage() == this) { 		..... 	}  Or is Page A not allowed to throw RestartResponseAtInterceptPageException in onBeforeRender() at all?$$patch1-wicket-bb7f9cf5_Developer_PatchNaturalnessYe$$Remove unused import. Allow plugins to validate headers when page was rendered. $$1
wicket-7b8b6767$$Multiple events in AjaxEventBehavior with prefix 'on'$$if multiple events are used and one starts with "on", it only works if it is the first one, because of:  {code} 		if (event.startsWith("on")) 		{ 			event = event.substring(2); 		} {code}  Why are events possible to start with "on" ?   Is this legacy? Perhaps should be removed for Wicket 7 ?$$patch1-wicket-7b8b6767_Developer_PatchNaturalnessYe$$Add missing imports. Add note about inline event names. Warn about wrong event name. Accept empty event name as long as it is not empty .. Cleaned up empty event names. $$1
wicket-02ebc8ae$$BufferedWebResponse fails to add/clear cookie in redirect$$bufferedWebResponse.addCookie( cookie );  That fails under certain conditions: (1) when called on the last of three 302 redirects during OpenID login; and (2) on single redirect immediately after container startup, though it later recovers.  Failure confirmed in Firebug; no cookies sent in any of the response headers.  My workaround is to bypass the buffered response.  This works:  ((HttpServletResponse)bufferedWebResponse.getContainerResponse()).addCookie( cookie );$$patch1-wicket-02ebc8ae_Developer_PatchNaturalnessYe$$Remove unused imports. keep the original response object. Fixed a bug where workflow from AJAX - 500 - Remove special handling of metadata headers in. $$1
wicket-ffc0cae9$$IRequestCycleListener: RequestCycle.get() is null inside onBeginRequest$$I expect the request cycle that is supplied as an argument to onBeginRequest to be the same as RequestCycle.get.  == == == CODE == == ==      @Override     public void onBeginRequest(RequestCycle cycle) {         Session session = Session.get(); // throws IllegalArgumentException         if (session.getMetaData(REDIRECTED_JSESSIONID) == null) {             logger.debug("first application request - redirecting to loading page");             session.setMetaData(REDIRECTED_JSESSIONID, Boolean.TRUE);             String url = getServletRequestContextPath() + "/" + cycle.getRequest().getUrl();             throw new RestartResponseException(newLoadingPage(url));         }     } == == == == == == == ==   == == == STACK TRACE == == ==  java.lang.IllegalArgumentException: Argument 'requestCycle' may not be null.     at org.apache.wicket.util.lang.Args.notNull(Args.java:37)     at org.apache.wicket.Application.fetchCreateAndSetSession(Application.java:1436)     at org.apache.wicket.Session.get(Session.java:154)     at com.joynit.tuv.common.view.request.SessionIdRemoveListener.onBeginRequest(SessionIdRemoveListener.java:30)  ... [snipped -other part is not relevant]  == == == == == == == == == == ==$$patch1-wicket-ffc0cae9_Developer_PatchNaturalnessYe$$Added onBeginRequest ( ) callback. Remove debug code. $$1
wicket-2ac29d30$$Nested Redirects and REDIRECT_TO_BUFFER$$When the render strategy is REDIRECT_TO_BUFFER, redirects cannot be nested. After the second redirect, Wicket renders the buffered first page in preference to the second page. The relevant code is in WebPageRenderer.respond:  {noformat} 		if (bufferedResponse != null) 		{ 			logger.warn("The Buffered response should be handled by BufferedResponseRequestHandler"); 			// if there is saved response for this URL render it 			bufferedResponse.writeTo((WebResponse)requestCycle.getResponse()); 		} {noformat}  The attached quickstart demonstrates the issue. Simply navigate to the home page. The observed behavior is that Page1 is displayed, but I expect Page2 to be displayed.  I can work around the issue by calling WebApplication.getAndRemoveBufferedResponse() to clear the render buffer, but I am uneasy with this solution since it seems like I am playing with Wicket internals; albeit the function is public.$$patch1-wicket-2ac29d30_Developer_PatchNaturalnessYe$$Remove unused method .. Removed debug code. Add missing documentation. $$1
wicket-d1b62639$$SHOW_NO_EXCEPTION_PAGE responding with HTTP status 500 is overwritten by redirect$$If the application is configured with SHOW_NO_EXCEPTION_PAGE as unexpectedExceptionDisplay, an exception thrown while submitting a form should result in an HTTP 500 status. Since the request is already marked as a redirect in AbstractListenerInterfaceRequestTarget#onProcessEvents(), the 500 status is overwritten with status 200 when the redirect is handled afterwards.$$patch1-wicket-d1b62639_Developer_PatchNaturalnessYe$$revert 121. $$1
wicket-843b76b1$$regression on strategy to integrate cas authentication$$yes, It happens in org.apache.wicket.request.handler.PageProvider.getPageInstance() , but not for the WelcomePage, but the redirection page (RedirectPage). the CASPageAuthorizationStrategy as we are not authentified a org.apache.wicket.RestartResponseAtInterceptPageException with in parameter an instance of RedirectPage. On the second call of PageProvider.getPageInstance, the pageId is of 0, an all other parameters are nulls.  The run seems quite different on 1.5-RC4.2 version - There is only one call of the method PageProvider.getPageInstance() and it come after the CASPageAuthorizationStrategy.isPageAuthorized  Le 27/06/2011 11:24, Martin Grigorov a crit : > Put a breakpoint in > org.apache.wicket.request.handler.PageProvider.getPageInstance() and > see what happens. > It seems the test tries to retrieve a page from the page store by id > but there is no such. > > On Mon, Jun 27, 2011 at 12:20 PM, Thomas Franconville > <tfranconville@tetraedge.com>  wrote: >> Hi, >> >> Upgrading wicket from 1.5-RC4.2 to 1.5-RC5.1  make my Junit Test down with >> the error 'Page expired' >> >> /** >>   * Simple test using the WicketTester >>   */ >> public class TestHomePage >> { >>     private WicketTester tester; >> >>     @Before >>     public void setUp() >>     { >>         tester = new WicketTester(new MyApplication()); >>     } >> >>     @Test >>     public void homepageRendersSuccessfully() >>     { >>         //start and render the test page >>         tester.startPage(WelcomePage.class); >> >>         //assert rendered page class >>         tester.assertRenderedPage(RedirectPage.class); >>     } >> } >> >> My application use a CASPageAuthorizationStrategy inspired of >> http://www.lunikon.net/2009/11/24/integrating-cas-and-wicket/ >> >> >> Kind Regards >> >> Thomas$$patch1-wicket-843b76b1_Developer_PatchNaturalnessYe$$changed default redirect policy to auto redirect. changed default redirect policy to auto redirect. $$1
wicket-99e22ce4$$Component reAttach and versioning$$I'm reAttaching a component doing something like:   MyFooPanel p1 = new MyFooPanel(this, "panel";);  MyBarPanel p2 = new MyBarPanel(this, "panel");  p1.reAttach();   When I try to restore to the initial page version I found that the component with id "panel" is not a children component of the page.   I have investigated it and I think it is because when the component is reAttached the order in which the changes are added to the ChangesList is:  - Add p2.  - Remove p1.   When the initial version is restored the undo functionality is done in reverse mode like,  - Add p1.  - Remove p2.   The problem is p1 and p2 have the same id, so when p2 is removed what is removing is p1 that has just added.   Oscar.$$patch1-wicket-99e22ce4_Developer_PatchNaturalnessYe$$added missing import. added 124. added missing cast in MarkupContainer. added missing closing tag. added missing < ? >. addedMarkupContainer ( child ). Fix MarkupContainer . contains ( ). Added missing closing < ? > forMarkupContainer. added missing closing < ? >. added missing < ? >. Adding missing type parameter. added missing cast. FixMarkupContainer detaching. added iterator on MarkupContainer. added missing < ? >. added iterator on component type. Fix MarkupContainer # sort. added missing parameter. added missing cast in MarkupContainer. added missing cast to prevent warning on elements. Fix visitChildren to return a non - null value. added missing < ? >. Adding missing < ? >. added children_add ( ) to avoid warning. added missing < ? > element. added missing cast in MarkupContainer. added static modifier to children_get ( ). added id to components in case of found component .. Added children_indexOf ( ) which will return - 1 if the id is not found. fixed inline with < ? >. added missing closing tag. Fix MarkupContainer ' s children_remove ( ) method. added missing closing < ? > tag. added missing closing < ? > tag. Fix MarkupContainer . children_set ( ) to return a component set form a template. added children_indexOf ( ) to the map. added missing cast in Controller. < ? >. Make ItemsIterator ( ) constructor return type components instead of component instance ; WI - 2929. workflow changes. $$1
wicket-825da305$$Ignore the path parameters when reading the page class$$http://localhost:8080/linkomatic/wicket/bookmarkable/org.apache.wicket.examples.linkomatic.Page3;myjsessionid=123456 leads to :  WARN  - AbstractRepeater           - Child component of repeater org.apache.wicket.markup.repeater.RepeatingView:area has a non-safe child id of page1. Safe child ids must be composed of digits only. WARN  - WicketObjects              - Could not resolve class [org.apache.wicket.examples.linkomatic.Page3;blass=koko] java.lang.ClassNotFoundException: org/apache/wicket/examples/linkomatic/Page3;blass=koko 	at java.lang.Class.forName0(Native Method) 	at java.lang.Class.forName(Class.java:264) 	at org.apache.wicket.application.AbstractClassResolver.resolveClass(AbstractClassResolver.java:108) 	at org.apache.wicket.core.util.lang.WicketObjects.resolveClass(WicketObjects.java:72) 	at org.apache.wicket.core.request.mapper.AbstractComponentMapper.getPageClass(AbstractComponentMapper.java:139) 	at org.apache.wicket.core.request.mapper.BookmarkableMapper.parseRequest(BookmarkableMapper.java:118) 	at org.apache.wicket.core.request.mapper.AbstractBookmarkableMapper.mapRequest(AbstractBookmarkableMapper.java:292) 	at org.apache.wicket.request.mapper.CompoundRequestMapper.mapRequest(CompoundRequestMapper.java:152) 	at org.apache.wicket.request.cycle.RequestCycle.resolveRequestHandler(RequestCycle.java:190) ...  Such request at the moment works only if the the path parameter name is 'jsessionid'$$patch1-wicket-825da305_Developer_PatchNaturalnessYe$$cleanClassName ( name ). Allow the user to specify a class name without the ; sign .. $$1
wicket-dfc56674$$DiskDataStore returns the wrong page when the page disk space is full$$If the configured file size for the session data is overflowed (see org.apache.wicket.settings.IStoreSettings#setMaxSizePerSession(Bytes)) then Wicket may return wrong page data (bytes) for a expired page.  The problem is in org.apache.wicket.pageStore.PageWindowManager#idToWindowIndex which may have several page ids (the keys) pointing to the same window index (values).$$patch1-wicket-dfc56674_Developer_PatchNaturalnessYe$$Add inversed index of PageWindowManager . idToWindowIndex .. wicket - util - wicket - pagestore should remove old map value. remove duplicated code. added missing initialization .. wicket - util / Collections / Maps / PageWindowManager .. reset windowIndexToPageId after removing window from list. add missing index pointer in PageWindowManager. $$1
wicket-724066f4$$NPE in JsonUtils when the value is null$$Most part of org.apache.wicket.ajax.json.JsonUtils.asArray(Map<String, Object> map) is trying carefully avoid null value. But there is following line  else if (value.getClass().isArray())  which cause NPE in case of empty value for some key.   P.S. Will provide patch.$$patch1-wicket-724066f4_Developer_PatchNaturalnessYe$$Allow null values for JSON arrays. $$1
wicket-ded3c583$$Wicket doesn't encrypt links and Ajax URLs for mounted pages when CryptoMapper is used$$URL encryption does not work in Wicket links and Ajax URLs.  For links the URL appears unencrypted in the href attribute value and is only later forwarded to the encrypted URL using a 302 response.  I am uploading a quickstart.$$patch1-wicket-ded3c583_Developer_PatchNaturalnessYe$$add missing import. Added note about encrypted URLs. Added hidden variable .. Fix error in Controller and IDE web interface. Added missing inheritDoc. wicket - crypto mapper returns null request object. Encrypts a URL , without query parameters it will throw an error if the URL is invalid. Improve decryptRequestListenerParameter method. $$1
wicket-f88721fd$$Any empty url-parameter will make wicket 1.5 crash$$Adding an empty parameter to the query string will make wicket crash.  http://www.example.com/?oneParam&   How to reproduce in test:  PageParameters params = new PageParameters(); params.set("",""); params.getAllNamed();   Cause: Wicket accepts empty parameters, but when encoding the url for a rendered page it will call params.getAllNamed().  params.getAllNamed() instantiates new NamedPairs, which calls Args.notEmpty() on the key during instantiation, causing the application to crash.   The NamedPair constructor should probably allow empty string as a key, and call Args.notNull() on the key in stead.$$patch1-wicket-f88721fd_Developer_PatchNaturalnessYe$$Don ' t strip empty query string as it will prevent url to be parsed by default. Removed erroneous legacy check .. allow null keys in named parameters. $$1
wicket-32c76c4a$$Select component loses it's value$$Select component loses selected option and shows the first option in some situations (one example is when you try to submit a form, but there are validation errors).  It was working fine in 1.4.18, but it's broken in 1.4.19.This must be caused by the solution from this issue https://issues.apache.org/jira/browse/WICKET-3962 I think the problem is likely in Select.isSelected method, where String[] paths = getInputAsArray() is actually an array of uuid-s, so uuid-s are compared to paths.  I haven't tested wicket 1.5, but this problem may also affect 1.5 versions.$$patch1-wicket-32c76c4a_Developer_PatchNaturalnessYe$$Updating to avoid warning in Select ( because of @ SuppressWarnings ). cleaned up code. Fixed case for select ( ). $$1
wicket-34634266$$StringResourceModel doesn't detach model in some cases$$We have come across an issue with StringResourceModel not detaching the model it holds under a certain condition.  The problem is the case where the StringResourceModel is created but it is not used - for example when it is on a tab that is not displayed.  StringResourceModel is a subclass of LoadableDetachableModel and it simply implements onDetach(), letting the superclass decide whether it is attached or not. The problem is that when StringResourceModel is created, LoadableDetachableModel.attached will be false.  If the StringResourceModel is never read (i.e. getObject() is not called) the LoadableDetachableModel will not be marked as attached and when detach() is called, onDetach() will not be called.  Therefore StringResourceModel will not call detach() on the model that it holds.$$patch1-wicket-34634266_Developer_PatchNaturalnessYe$$Fixed onDetach ( ) method. $$1
wicket-e743fd7e$$AutoLabelTextResolver fails to pick up locale changes in the session$$When using <wicket:label key="..."> AutoLabelTextResolver correctly picks up the localized message identified by the key. However, if the Session locale is changed, neither the printed label nor the FormComponent's label model get updated - both will still contain the initial message. This is inconsistent with the behavior of <wicket:message> and StringResourceModel. The principle of least surprise (and in my opinion, also that of highest usefulness ;-) ) suggests that AutoLabelTextResolver should dynamically get the localized string whenever it deals with something that can be localized. That includes the <wicket:label key="..."> case mentioned above, as well as when using FormComponent#getDefaultLabel.  I have only tested this in trunk 1.5 (since it recently came up during a training I gave on Wicket 1.5). I suspect it also affects 1.4.x.$$patch1-wicket-e743fd7e_Developer_PatchNaturalnessYe$$add missing import. - - better to check if the label model is a form component. $$1
wicket-53442bb4$$Component#setDefaultModel() should call #modelChanging()$$Component#setDefaultModel() should call #modelChanging() as #setDefaultModelObject() does. It worked by chance so far because addStateChange() is called.  http://markmail.org/thread/uxl6uufusggqbb6s$$patch1-wicket-53442bb4_Developer_PatchNaturalnessYe$$- modelChanged ( ) now calls modelChanging ( ) and modelChanging ( ). $$1
wicket-d3d42d42$$404 Error on Nested ModalWindows in IE7 and IE8$$When opening a ModalWindow inside a ModalWindow, the inner ModalWindow generates a 404 error.  Both windows use a PageCreator for content.  To replicate, you must use an actual IE 7 or IE 8 browser, as this does not replicate using developer tools and setting the document and brower to IE 7.  The problem can be seen at http://www.wicket-library.com/wicket-examples/ajax/modal-window.  I will attach a Quickstart as well.$$patch1-wicket-d3d42d42_Developer_PatchNaturalnessYe$$wicket - core - request . getClientUrl ( ) + urlStartsWith ( baseUrl , namespace. add marker for url in bookmarkable form. $$1
wicket-e24874da$$StringResourceModels doesn't seem to detach properly$$If a StringResourceModel contains a model for property substitutions, and there has not been assigned a component it is relative to on construction time, it will not detach the property substitution model.  See this thread for a full explanation http://apache-wicket.1842946.n4.nabble.com/StringResourceModels-doesn-t-seem-to-detach-properly-td4257267.html$$patch1-wicket-e24874da_Developer_PatchNaturalnessYe$$Added missing onDetach ( ) method. Added missing semicolon .. $$1
wicket-b00920f3$$StackOverflowError after form submit with a validation error$$I was not able to find a cause or make a small quickstart, but it has something to do with a form validation, my workaround was to setDefaultFormProcessing(false) or not use required TextFields.  It can be reproduced on https://github.com/krasa/DevSupportApp/tree/stackOverflow  1) run StartVojtitkoDummy 2) go to http://localhost:8765/wicket/bookmarkable/krasa.release.frontend.TokenizationPage 3) click on "Generate Release json" button  - instead of SOE, it should give a validation error, probably even on fields which I would not want to validate, but that's just because I've made the page badly...     {code} java.lang.StackOverflowError: null ... 	at org.apache.wicket.Component.getMarkup(Component.java:755) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:81) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:74) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:66) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:144) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:123) 	at org.apache.wicket.MarkupContainer.visitChildren(MarkupContainer.java:862) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy.searchMarkupInTransparentResolvers(AbstractMarkupSourcingStrategy.java:65) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:99) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.Component.getMarkup(Component.java:755) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:81) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:74) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:66) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:144) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:123) 	at org.apache.wicket.MarkupContainer.visitChildren(MarkupContainer.java:862) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy.searchMarkupInTransparentResolvers(AbstractMarkupSourcingStrategy.java:65) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:99) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.Component.getMarkup(Component.java:755) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:81) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:74) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:66) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:144) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:123) 	at org.apache.wicket.MarkupContainer.visitChildren(MarkupContainer.java:862) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy.searchMarkupInTransparentResolvers(AbstractMarkupSourcingStrategy.java:65) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:99) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.Component.getMarkup(Component.java:755) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:81) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:74) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:66) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:144) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:123) 	at org.apache.wicket.MarkupContainer.visitChildren(MarkupContainer.java:862) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy.searchMarkupInTransparentResolvers(AbstractMarkupSourcingStrategy.java:65) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:99) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.Component.getMarkup(Component.java:755) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:81) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:74) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:66) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:144) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:123) 	at org.apache.wicket.MarkupContainer.visitChildren(MarkupContainer.java:862) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy.searchMarkupInTransparentResolvers(AbstractMarkupSourcingStrategy.java:65) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:99) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.Component.getMarkup(Component.java:755) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:81) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:74) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:66) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:144) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:123) 	at org.apache.wicket.MarkupContainer.visitChildren(MarkupContainer.java:862) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy.searchMarkupInTransparentResolvers(AbstractMarkupSourcingStrategy.java:65) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:99) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.Component.getMarkup(Component.java:755) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:81) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:74) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:66) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:144) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:123) 	at org.apache.wicket.MarkupContainer.visitChildren(MarkupContainer.java:862) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy.searchMarkupInTransparentResolvers(AbstractMarkupSourcingStrategy.java:65) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:99) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.Component.getMarkup(Component.java:755) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:81) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:74) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:66) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:144) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:123) 	at org.apache.wicket.MarkupContainer.visitChildren(MarkupContainer.java:862) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy.searchMarkupInTransparentResolvers(AbstractMarkupSourcingStrategy.java:65) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:99) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.Component.getMarkup(Component.java:755) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:81) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:74) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:66) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:144) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:162) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:162) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:162) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:123) 	at org.apache.wicket.MarkupContainer.visitChildren(MarkupContainer.java:862) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy.searchMarkupInTransparentResolvers(AbstractMarkupSourcingStrategy.java:65) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:99) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.Component.getMarkup(Component.java:755) 	at org.apache.wicket.Component.internalRender(Component.java:2344) 	at org.apache.wicket.Component.render(Component.java:2307) 	at org.apache.wicket.ajax.XmlAjaxResponse.writeComponent(XmlAjaxResponse.java:128) 	at org.apache.wicket.ajax.AbstractAjaxResponse.writeComponents(AbstractAjaxResponse.java:218) 	at org.apache.wicket.ajax.AbstractAjaxResponse.writeTo(AbstractAjaxResponse.java:150) 	at org.apache.wicket.ajax.AjaxRequestHandler.respond(AjaxRequestHandler.java:359) 	at org.apache.wicket.request.cycle.RequestCycle HandlerExecutor.respond(RequestCycle.java:865) 	at org.apache.wicket.request.RequestHandlerStack.execute(RequestHandlerStack.java:64) 	at org.apache.wicket.request.RequestHandlerStack.execute(RequestHandlerStack.java:97) 	at org.apache.wicket.request.cycle.RequestCycle.execute(RequestCycle.java:265) 	at org.apache.wicket.request.cycle.RequestCycle.processRequest(RequestCycle.java:222) 	at org.apache.wicket.request.cycle.RequestCycle.processRequestAndDetach(RequestCycle.java:293) 	at org.apache.wicket.protocol.ws.AbstractUpgradeFilter.processRequestCycle(AbstractUpgradeFilter.java:59) 	at org.apache.wicket.protocol.http.WicketFilter.processRequest(WicketFilter.java:203) 	at org.apache.wicket.protocol.http.WicketFilter.doFilter(WicketFilter.java:284) 	at org.eclipse.jetty.servlet.ServletHandler CachedChain.doFilter(ServletHandler.java:1652) 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:585) 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143) 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:577) 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:223) 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1125) 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:515) 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185) 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1059) 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141) 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:97) 	at org.eclipse.jetty.server.Server.handle(Server.java:497) 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:310) 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:248) 	at org.eclipse.jetty.io.AbstractConnection 2.run(AbstractConnection.java:540) 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:620) 	at org.eclipse.jetty.util.thread.QueuedThreadPool 3.ru {code}$$patch1-wicket-b00920f3_Developer_PatchNaturalnessYe$$added missing import. Don ' t resolve component resolvers by default. $$1
wicket-89184b79$$MountMapper does not support correctly parameter placeholders$$Package mounting doesn't support parameter placeholders. The problem seems to be inside MountMapper which should wrap PackageMapper and take care of substituting placeholders with their actual value.  More precisely this class doesn't read parameter values from PageParameters and it's not very clear to me how it tries to read these values. Does anybody have some hints about this class?$$patch1-wicket-89184b79_Developer_PatchNaturalnessYe$$Allow null page parameters for stateful pages. null check for stateful pages. $$1
wicket-e8dab4a0$$Wicket does not correctly handle http OPTIONS requests$$currently these requests cause regular processing (page rendering), when in fact they should have a special response.  rendering the page in OPTIONS causes renderCount to be incremented and this messes with the subsequent request to the same url via a GET or POST$$patch1-wicket-e8dab4a0_Developer_PatchNaturalnessYe$$Fixing whitespace in WicketFilter .. Fix typo. Fixed a bug where "" OPTIONS "" request is processed with the new request . """. added missing closing parenthesis. Add missing Javadoc. Reduced too broad catch block. Added filterPath getter for protocol / http .. Missing closing < filter > tag. Fixed invalid parameter name in URL path. $$1
wicket-1485a856$$Form components' name/value are encoded in stateless form's action url$$Stateless forms aren't working well as you can see on wicket examples: http://www.wicket-library.com/wicket-examples/stateless/foo  The first time you submit, (for example, the value 10), everything works as supposed to. If you now change the value (to 11 for example) and submit the form, the value wicket shows is 10.  I think the problem is stateless forms are generating an action URL with submitted values on query string, and when you resubmit the form, this values on query string replace the POST(or GET) values.$$patch1-wicket-1485a856_Developer_PatchNaturalnessYe$$Removed unused private field. Remove unused imports. Remove unused page parameters for all form components. $$1
wicket-5fd03973$$ListenerInterfaceRequestHandler should not assume existence of a page$$ListenerInterfaceRequestHandler should not assume a page instance is always available in isPageInstanceCreated. This handler can also be used for links on bookmarkable pages. The attached patch fixes this.$$patch1-wicket-5fd03973_Developer_PatchNaturalnessYe$$add warning. $$1
wicket-4a875f46$$Mapping ResourceReferences to Urls is slow$$PackageResourceReference is often used for stylesheets and JavaScript resources, many of which can appear on a typical page (WicketAjaxReference is one common example). Every time the page is rendered, these resources are mapped to urls in order to build the appropriate <link href="..."> or <script src="..."> tags.  The trouble is that this mapping process is extremely inefficient. To map a ResourceReference to a url, ResourceReference#getLastModified() must be consulted for FilenameWithTimestampResourceCachingStrategy, and ResourceReference#getUrlAttributes() is called to append appropriate query parameters.  In PackageResourceReference, both of these methods delegate to the very expensive PackageResourceReference#lookupStream(), which makes several attempts to locate the underlying file or classpath item using various permutations of locale, style, and variation. Each of these attempts involves I/O. The default ResourceStreamLocator, which does the actual file and classpath queries, does no caching whatsoever.  On a trivial Wicket page containing 7 total PackageResourceReferences for images, stylesheets and JavaScript files, the average response time in my tests was 211 ms. The vast majority of that time was spent in ResourceStreamLocator, due to the expensive steps described above.  It seems that putting caching at the ResourceStreamLocator would be extremely beneficial. I am attaching a simple implementation. With caching enabled in ResourceStreamLocator, the response time of my test page dropped from 211 ms to 49 ms.$$patch1-wicket-4a875f46_Developer_PatchNaturalnessYe$$add missing import. Updating docs - -. wicket - util / resource / locator / CachingResourceStreamLocator. fixed accidental shadowing of FileResourceStream. Allow reference to be constructed by URL + constructor. ResourceStreamLocator should update the cache by default. Ease in closures ( IResourceStreamLocator . locate ). $$1
wicket-44f4782a$$PackageMapper - Could not resolve class$$It seems that the PackageMapper try to resolve much more than it is supposed to do, for instance if I've 2 pages test1/TestPage1 and test2/TestPage2 then it tries to resolve test2/TestPage1 when I reach the page1...   WARN  - WicketObjects              - Could not resolve class [com.mycompany.test2.TestPage1] java.lang.ClassNotFoundException: com.mycompany.test2.TestPage1     at org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.java:1714)     at org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.java:1559)     at java.lang.Class.forName0(Native Method)     at java.lang.Class.forName(Class.java:270)     at org.apache.wicket.application.AbstractClassResolver.resolveClass(AbstractClassResolver.java:108)     at org.apache.wicket.core.util.lang.WicketObjects.resolveClass(WicketObjects.java:71)     at org.apache.wicket.core.request.mapper.AbstractComponentMapper.getPageClass(AbstractComponentMapper.java:134)     at org.apache.wicket.core.request.mapper.PackageMapper.parseRequest(PackageMapper.java:152)     at org.apache.wicket.core.request.mapper.AbstractBookmarkableMapper.mapRequest(AbstractBookmarkableMapper.java:322)     at org.apache.wicket.request.mapper.CompoundRequestMapper.mapRequest(CompoundRequestMapper.java:152)     at org.apache.wicket.request.cycle.RequestCycle.resolveRequestHandler(RequestCycle.java:189)     at org.apache.wicket.request.cycle.RequestCycle.processRequest(RequestCycle.java:219)     at org.apache.wicket.request.cycle.RequestCycle.processRequestAndDetach(RequestCycle.java:293)     at org.apache.wicket.protocol.http.WicketFilter.processRequestCycle(WicketFilter.java:261)     at org.apache.wicket.protocol.http.WicketFilter.processRequest(WicketFilter.java:203)     at org.apache.wicket.protocol.http.WicketFilter.doFilter(WicketFilter.java:284)$$patch1-wicket-44f4782a_Developer_PatchNaturalnessYe$$Fix url matcher for PackageMapper. $$1
wicket-5b730c0b$$Failing HTTPS redirect to RequireHttps annotated pages with ONE_PASS_RENDER strategy$$Activated JS: Start the quickstart -> Press the submit buttons -> See the secured page with https!  Deactivates JS: (NoScript Firefox Plugin): Start the quickstart -> Press the submit buttons -> See the secured page BUT with HTTP!  There was no proper https redirect.  If I change the rendering strategy to REDIRECT_TO_BUFFER everything works fine, but if I change the strategy to ONE_PASS_RENDER the https forwarding does't work anymore. But only if I deactivate all scripts...  Regards, Dmitriy$$patch1-wicket-5b730c0b_Developer_PatchNaturalnessYe$$add missing import. Fix possible client URL conflict in reverseEach. Fix a warning. $$1
wicket-fbfd17e6$$IllegalStateException: Header was already written to response!$$Getting this error for no apparent reason, the code works fine on wicket 1.4.17. Code example with error is attached.  Click on the 'click here' link to see the error occur in the console, below is part of the stack trace.   ERROR - DefaultExceptionMapper     - Unexpected error occurred java.lang.IllegalStateException: Header was already written to response!         at org.apache.wicket.protocol.http.HeaderBufferingWebResponse.checkHeader(HeaderBufferingWebResponse.java:62)         at org.apache.wicket.protocol.http.HeaderBufferingWebResponse.setDateHeader(HeaderBufferingWebResponse.java:131)          at org.apache.wicket.protocol.http.BufferedWebResponse SetDateHeaderAction.invoke(BufferedWebResponse.java:241)         at org.apache.wicket.protocol.http.BufferedWebResponse.writeTo(BufferedWebResponse.java:487)         at org.apache.wicket.request.handler.render.WebPageRenderer.respond(WebPageRenderer.java:225)         at org.apache.wicket.request.handler.RenderPageRequestHandler.respond(RenderPageRequestHandler.java:139)         at org.apache.wicket.request.cycle.RequestCycle HandlerExecutor.respond(RequestCycle.java:715)         at org.apache.wicket.request.RequestHandlerStack.execute(RequestHandlerStack.java:63)         at org.apache.wicket.request.cycle.RequestCycle.executeExceptionRequestHandler(RequestCycle.java:274)         at org.apache.wicket.request.cycle.RequestCycle.executeExceptionRequestHandler(RequestCycle.java:283)         at org.apache.wicket.request.cycle.RequestCycle.executeExceptionRequestHandler(RequestCycle.java:283)         at org.apache.wicket.request.cycle.RequestCycle.executeExceptionRequestHandler(RequestCycle.java:283)         at org.apache.wicket.request.cycle.RequestCycle.executeExceptionRequestHandler(RequestCycle.java:283)         at org.apache.wicket.request.cycle.RequestCycle.executeExceptionRequestHandler(RequestCycle.java:283)         at org.apache.wicket.request.cycle.RequestCycle.processRequest(RequestCycle.java:227)         at org.apache.wicket.request.cycle.RequestCycle.processRequestAndDetach(RequestCycle.java:253)         at org.apache.wicket.protocol.http.WicketFilter.processRequest(WicketFilter.java:138)         at org.apache.wicket.protocol.http.WicketFilter.doFilter(WicketFilter.java:194)         at org.eclipse.jetty.servlet.ServletHandler CachedChain.doFilter(ServletHandler.java:1323)         at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:474)         at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:119)         at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:517)         at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)         at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:935)         at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:404)         at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:184)         at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:870)         at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:117)         at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:247)         at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:151)         at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)         at org.eclipse.jetty.server.Server.handle(Server.java:346)         at org.eclipse.jetty.server.HttpConnection.handleRequest(HttpConnection.java:596)         at org.eclipse.jetty.server.HttpConnection RequestHandler.headerComplete(HttpConnection.java:1051)         at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:592)         at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:214)         at org.eclipse.jetty.server.HttpConnection.handle(HttpConnection.java:426)         at org.eclipse.jetty.server.bio.SocketConnector ConnectorEndPoint.run(SocketConnector.java:241)         at org.eclipse.jetty.server.ssl.SslSocketConnector SslConnectorEndPoint.run(SslSocketConnector.java:646)         at org.eclipse.jetty.util.thread.QueuedThreadPool 3.run(QueuedThreadPool.java:528)         at java.lang.Thread.run(Thread.java:619)$$patch1-wicket-fbfd17e6_Developer_PatchNaturalnessYe$$add missing import. Fix BufferedWebResponse . compareTo ( ). Fix BufferedWebResponse . compareTo ( ). Fix BufferedWebResponse . compareTo ( ). Fix BufferedWebResponse . compareTo ( ). add metaDataAction. add metaDataAction. added sort on actions to BufferedWebResponse. $$1
wicket-f20b2d70$$Mounted page is not throwing ExpireException with setting setRecreateMountedPagesAfterExpiry(false)$$We have a page that is both bookmarkable (and accessible with certain page parameters) and has a second constructor taking an object.  When ever the session time-out we want to show a session expired page. But we get a exception because Wicket is trying to rebuild the page with no page parameters.  We have set the setting getPageSettings().setRecreateMountedPagesAfterExpiry(false); This works when clicking on (ajax)links, but it's not working when using the back/forward button in the browser (or javascript:history.go(-1)).  I'll attache a quickstart.$$patch1-wicket-f20b2d70_Developer_PatchNaturalnessYe$$add missing import. add @ author tag. Adding author tag. update marker. Allow * _after_type_check. Updating old comment. Add missing javadoc. add @ return. update javadoc. Added missing * javadoc *. prevent pages from being rendered in live mode. $$1
wicket-ccb8fc9e$$Inline enclosure doesn't work if wicket:message attribute is used on the same tag$$Markup like:          <div wicket:enclosure="child" wicket:message="title:something"> 	        <div>Inner div 		        <span wicket:id="child">Blah</span> 	        </div>         </div>  doesn't work (Inner div is visible, no matter whether 'child' is visible or not) because the auto component created for wicket:message breaks somehow wicket:enclosure.$$patch1-wicket-ccb8fc9e_Developer_PatchNaturalnessYe$$wicket - driver - 172 fixed issue. $$1
wicket-b91154ea$$Inline enclosure doesn't work if wicket:message attribute is used on the same tag$$Markup like:          <div wicket:enclosure="child" wicket:message="title:something"> 	        <div>Inner div 		        <span wicket:id="child">Blah</span> 	        </div>         </div>  doesn't work (Inner div is visible, no matter whether 'child' is visible or not) because the auto component created for wicket:message breaks somehow wicket:enclosure.$$patch1-wicket-b91154ea_Developer_PatchNaturalnessYe$$add message tag handler to filters. Removed unused tag handler. Removed redundant component resolvers. wicket - core doesn ' t have inlineEnclosureFilter on IE8. wicket - id = > htmlId. Fixed unreachable code. wicket - message - attribute checks are still empty ( wicket - message - attribute is considered empty ). "remove "" wc """. $$1
wicket-b80f6640$$LongConverter converts some values greater than Long.MAX_VALUE$$Currently it's possible to submit some values via Long Textfield<Long> that are greater than Long.MAX_VALUE. This will produce converted input and model update with value of Long.MAX_VALUE  I'm not sure what the behavior should be - imho throwing ConversionException seems fair as the input isn't a valid Long.  The reason seems to be precision loss during Double.valueOf(input) execution while converting, and then comparing to Long.MAX_VALUE  using Long.doubleValue() in *AbstractNumberConverter*, which by casting leads to to the same precision loss and the numbers are seemingly equal during comparison of ranges.  Maybe using BigDecimals for parsing could help here.  The quickstart is available at [https://github.com/zeratul021/wicket-number-conversion].  For the fastest demonstration I extended Wicket's _longConversion()_ test-case in *ConvertersTest*: [https://github.com/zeratul021/wicket-number-conversion/blob/master/src/test/java/com/github/zeratul021/wicketnumberconversion/ConvertersTest.java#L300]$$patch1-wicket-b80f6640_Developer_PatchNaturalnessYe$$remove unused import. Added private constructor for IntegerConverter .. workflow improvements - added parse to IntegerConverter. Remove unused imports .. Fix codenarc. Remove unused code. ShortConverter should import java . math . BigDecimal. Added ByteConverter with min / max value. workflow improvements - - 1d8b0e01095c1 updated to 1d. ShortConverter should have removed unused imports. Added private constructor for ShortConverter .. ShortConverter should parse BigDecimal too .. Added unused imports. Fix # 1862. parse number with locale. Fix errors with BigDecimal class .. Handle BigDecimalConverter from String to BigDecimal object in case of any other number type .. remove unused imports. fixed rare npe after parse ( String , Long , Integer , Locale . US ). Fix whitespace in AbstractIntegerConverter .. Fix # 1796. add warning. Added missing constructor .. workflow improvements - - 1ls6dc9 Nullcheck the long precision thing .. Added missing import. Added private constructor for DoubleConverter .. remove special treatment for parse ( String , Number , BigDecimal ). Removed unused imports. Added private constructor for FloatConverter .. Fixed erroneous conversion .. Remove deprecated annotation. Fixed erroneous conversion - remove parse ( ) method from zeroPaddingIntegerConverter. $$1
wicket-bb7a6995$$FileResourceStream returns unknown content type$$See http://apache-wicket.1842946.n4.nabble.com/PackageResourceReference-and-Doctype-in-Markup-file-tp3889467p3889587.html  The response for FileResourceStreams returns an unknown content type for css- and image-files. Correct content types should be "text/css" and "image/png" (see also attached quickstart).$$patch1-wicket-bb7a6995_Developer_PatchNaturalnessYe$$Remove unused import. fixed NPE for ByteArrayResource ( this is an empty class ). add text / javascript mime mapping. wicket - request . resource . ResourceStreamResource should use its own mimeType ( or null if no. fixed NPE for PackageResource. $$1
wicket-c250db9c$$bug in org.apache.wicket.validation.validator.UrlValidator$$Looks like there is a bug in UrlValidator. It validates URLs like "http://testhost.local/pages/index.php" as invalid. But URL is valid! Try to execute "new java.net.URL("http://testhost.local/pages/index.php");" for example. It does not throws "MalformedURLException" because URL is valid.  In method: UrlValidator.isValidAuthority() there is code: "if (topLevel.length() < 2 || topLevel.length() > 4){return false;}" Looks like this "> 4" is a wrong constraint.$$patch1-wicket-c250db9c_Developer_PatchNaturalnessYe$$Fix 427. $$1
wicket-be70e608$$getMarkupId() can be used only if the component's markup is attached$$With change r1037139 Component#getMarkupImpl() first tries to get the markup id from the component's markup. If the markup is not available/attached yet for this component the call ends with : org.apache.wicket.markup.MarkupException: Can not determine Markup. Component is not yet connected to a parent. [Component id = label]$$patch1-wicket-be70e608_Developer_PatchNaturalnessYe$$added missing closing < > tag in MarkupContainer. add missing import. Removed unused class. add missing return statement , because it is not allowed to be set on a component. added missing closing MarkupFragment. $$1
wicket-d547fcd4$$ResourceUtils.getLocaleFromFilename can't handle minimized resources well$$I think the ResourceUtils.getLocaleFromFilename(String path) has the order of locale and minimization wrong: It currently parses: File.min_Lang_Coun_Var.ext while the typical convention is File_Lang_Coun_Var.min.ext Surely considering the ResourceUtils.getMinifiedName() method which does work according to convention.$$patch1-wicket-d547fcd4_Developer_PatchNaturalnessYe$$add locale min regex. Added helpful note about resource locale from minified filename .. Put inline inline with ResourceUtils. Missing locale matcher pattern , this should fix the build .. wicket - util - resource . ResourceUtils now uses language / path as it is done in the. $$1
wicket-5cdc1c8d$$Stateless/Statefull pages - incorrect behaviour$$Please advise how to do in following situation or confirm that's a bug and should be fixed.  There is a page (login page) with stateless form. That page has lots of common components (menu and etc.). There are some stateful components in the components tree that are visible only for signed in users: but once user isn't signed in - that components are hidden. That's why page is becoming "stateless" (no visible components) and form prepared correspondingly. But when form data is submitted: during obtaining of form component to process request - wicket thinks that page actually is stateful. As a result - the page is recreated and fully rendered - instead of processing of the form.  There is a workaround: setStatelessHint(false). But imho reason is a little bit another: 1) After construction of page: page is stateful - because of some stateful components are in the tree. 2) After initialization of page: page is still stateful - because there are that stateful components 3) After configuration of page (method onConfigure) - page is becoming stateless - because all stateful components marked as invisible. 4) Form has been rendered as stateless - with no version number is in the URL. 5) Page can'be reconstructed correctly because of p.1 and p.2  I think that stateless flag should be precalculated right after initialization step and should be changed due to some stuff in "configuration" methods.  What do you think?  Will provide "quick start" in near future!$$patch1-wicket-5cdc1c8d_Developer_PatchNaturalnessYe$$Fix canCallListenerInterfaceAfterExpiry .. $$1
wicket-38e928c1$$Header contributions order is not stable$$In the last RCs, I started to experience problems with the contributions order. For example, I add jQuery, and until 1.5RC5, it worked well, but now the call to the jQuery script has been moved to the bottom of the page head, and this disables all my other scripts that are expecting jQuery's   to be defined.  I attach a quickstart to demonstrate the problem. Maybe the order in the quickstart is not the expected one, but what it shows is that the order does not make real sense (at least to me) : In the quickstart, the wicket:head tag contributions are in the order 3 - 8 - 9 - 5, and the renderHead methods contributions are in the order 4 - 1 - 2 - 6 - 7.$$patch1-wicket-38e928c1_Developer_PatchNaturalnessYe$$Don ' t render the root component if it is not the root component .. $$1
wicket-c2e12216$$FormComponent.updateCollectionModel  does not handle unmodifiableList$$FormComponent.updateCollectionModel should handle situation, when getter returns unmodifiable list.  Proposed solution:  			formComponent.modelChanging(); 			booelan isChanged; 			try { 				collection.clear(); 				if (convertedInput != null) 				{ 					collection.addAll(convertedInput); 				} 				isChanged = true; 			catch (Exception e) 			{ 				// ignore this exception as Unmodifiable list does not allow change 				 				logger.info("An error occurred while trying to modify list attached to " + formComponent, e); 			}  			try 			{ 				if(isChanged)				 					formComponent.getModel().setObject(collection); 				else  					// TODO: create here collection as non-abstract successor of setObject declared argument 					formComponent.getModel().setObject(new ArrayList(convertedInput)); 				isChanged = true; 			} 			catch (Exception e) 			{ 				// ignore this exception because it could be that there 				// is not setter for this collection. 				logger.info("An error occurred while trying to set the new value for the property attached to " + formComponent, e); 			} 			// at least one update method should pass successfully			 			if(isChanged) 				formComponent.modelChanged(); 			else 				throw new RuntimeException("An error occurred while trying to modify value for the property attached to " + formComponent);$$patch1-wicket-c2e12216_Developer_PatchNaturalnessYe$$Added Error. Log model changed errors when trying to set a collection .. $$1
wicket-9decad35$$POST params ignored by IPageParametersEncoder#decodePageParameters()$$As per this conversation: http://apache-wicket.1842946.n4.nabble.com/how-to-get-https-port-number-in-Wicket-1-5-td4295139.html  it seems that POST params are not properly processed and made available as PageParameters. Can anyone say whether this is intended behavior or not? I will attach a Quickstart to demonstrate.  Martin's proposed fix is straightforward, but I am not comfortable enough with Wicket internals to say whether or not this would break something.  Thanks$$patch1-wicket-9decad35_Developer_PatchNaturalnessYe$$Updated Imports. Removed note on parameter ordering. add missing import. Allow null values for CombinedRequestParametersAdapter constructor. wicket request adapter missed one. Removed unused imports .. remove debug code. Add note about UrlPathPageParametersEncoder. $$1
wicket-b61fe92c$$Wicket generates invalid HTML by expanding col tags$$hi,  I just noticed that wicket expands col tags, even though the (x)html specifications forbids it.  take this markup as an example:  <table>     <colgroup>         <col width="20%" />         <col width="80%" />     </colgroup>     <tbody>         <tr>             <td>I take a fifth of the available space</td>             <td>I take four fifth of the available space</td>         </tr>     </tbody> </table>  Instead of return this as-is, it get's converted to:  <table>     <colgroup>         <col width="20%"></col>         <col width="80%"></col>     </colgroup>     <tbody>         <tr>             <td>I take a fifth of the available space</td>             <td>I take four fifth of the available space</td>         </tr>     </tbody> </table>  But the specifications mention that col tags must not have end tags. This may be related to WICKET-2765, as this seems to be the point when col was added to the OpenCloseTagExpander class. Note that it is ok to have a non closing col tag in html (self-closing in xhtml). It's all about generating a separated end tag.  This happens in wicket 6.8, but I guess it's relevant to all versions down to wicket 1.4.  Specs for reference:  http://www.w3.org/TR/1999/REC-html401-19991224/struct/tables.html#edef-COL http://www.w3.org/TR/html-markup/col.html  Kind regards,  Konrad$$patch1-wicket-b61fe92c_Developer_PatchNaturalnessYe$$Added static final list of elements that should not be expanded from TagType . OPEN_CLOSE. Removed erroneous col "" s ( < col > ). $$1
wicket-eb125865$$Significant Performance Degradation From Wicket 6.20.0 to Wicket 7.0.0$$I am experiencing a significant performance degradation for component adds in Wicket 7.0.0, once the component tree for a page gets reasonably large.  The attached quick start can be used to reproduce the issue.  Please note that NUM_ROWS is set to 10,000 to exaggerate the performance degradation as the size of the component tree increases.  The same degradation (to a lesser extent) can be viewed with a smaller NUM_ROWS variable.  In Wicket 6.20.0, as the size of the component tree increases, the cost of add() remains relatively constant time-wise.  In Wicket 7.0.0, a component add () is much more expensive (and actually makes our internal web application unusable) with form submits taking more than two or three minutes to come back from the server.  Here's some timing examples.    =============================================================================================================  NUM_ROWS = 5000 Wicket 6.20.0 -> ~200 milliseconds of server side rendering (before browser paints HTML). Wicket 7.0.0 -> ~ 10 seconds of server side rendering  NUM_ROWS = 10000 Wicket 6.20.0 -> ~ 500 milliseconds of server side rendering Wicket 7.0.0 -> ~ 40 seconds of server side rendering  =============================================================================================================  The attached quickstart can be used to reproduce the issue on your side.  My guess is that this has to do with the new component queuing feature that was added as part of Wicket 7.0.0.$$patch1-wicket-eb125865_Developer_PatchNaturalnessYe$$Allow null values for index. $$1
wicket-61122bab$$org.apache.wicket.util.string.StringValue#equals broken$$The #equals implementation for org.apache.wicket.util.string.StringValue is broken. The following throws an exception instead of just printing 'false':  StringValue val = StringValue.valueOf("bla", Locale.FRANCE); StringValue val2 = StringValue.valueOf("bla", Locale.CANADA); System.out.println(val.equals(val2));   This part of #equals Objects.isEqual(locale, stringValue.locale)  should probably be replaced with something like (locale == stringValue.locale || (locale != null && locale.equals(stringValue.locale))  -> Objects.isEqual is not suitable to determine equality of Locale$$patch1-wicket-61122bab_Developer_PatchNaturalnessYe$$Fix can ' t convert string to enum value due to string class .. Added convertToString ( ) in StringValue. Added convertible property to StringValue. Log exception if it prevented to convert boolean value from string to boolean .. Added missing + * @ javadoc javadoc. Log exception if it is not a string. applied Stefan ' s patch. Log exception if it prevented to convert doubles to doubles .. Added missing javadoc. Add inverse notation for toDuration. Log the error message if it is not a string. Fix probably typo in exception message. wicket - util - stringConverter to an integer allows formatting error messages. Fix StringConverterException message. Fix error message. allow convertible text to long value. wicket - util - stringConverter to long works fine. Fix error message. Remove - > wicket exception. Log exception if it prevented to convert Time objects to ADT .. Added missing javadoc. Added missing javadoc. Added missing Javadoc. Added one more test. remove redundant isEqual ( ) check. $$1
wicket-7c89598a$$BookmarkablePageLinks not working on a forwarded page$$While migrating our app from 1.4 to 1.5 we have discovered a problem with how BookmarkablePageLinks are rendered.  The attached quickstart demonstrates the problem:  Two pages: HomePage and OtherPage mounted at: /content/home and /content/other respectively.  These are mounted using the encoder UrlPathPageParametersEncoder for backwards compatibility with existing 1.4 style URLs.  A filter has been established in web.xml to forward requests to root (eg., localhost) to localhost/content/home [Note: I have left out the port :8080 part from all URL references so please insert when testing]  Point browser to http://localhost and the page is forwarded to http://localhost/content/home and displays correctly (browser URL still shows http://localhost as desired) but the links do not work because they remove the 'content' segment of the URL:  eg., Home link -> http://localhost/home - fails - should have been http://localhost/content/home  If you type in the full URL: http://localhost/content/home  then the home page displays and the links work correctly.  A similar page set  up works fine in 1.4.$$patch1-wicket-7c89598a_Developer_PatchNaturalnessYe$$Add forwardAttributes to ServletWebRequest .. Added missing forward attributes in ServletWebRequest .. parse request uri as String. I should probably keep this in ServletWebRequest for now .. Add ability to access private fields on request object. $$1
wicket-d1e0e411$$AbstractTransformerBehavior sets wrong namespace$$AbstractTransformerBehaviour adds a wicket namespace (http://wicket.apache.org) to its tag which is different from that of the whole page (http://wicket.apache.org/dtds.data/wicket-xhtml1.4-strict.dtd).  This causes (at least) XPath queries for Wicket nodes to fail when matching the contents of components with an AbstractTransformerBehavior.$$patch1-wicket-d1e0e411_Developer_PatchNaturalnessYe$$Removed unused imports .. Removed tag namespace stuff .. Removed unused import. Fix XsltOutputTransformerContainer to respect XHTML 1 . 3 restrictions. added missing loop in MarkupContainer . onAfterRenderChildren ( ). Remove unused code. Removed unused import. Allow the XsltTransformerBehavior to handle the wicket tag .. Added WICKET_XHTML_DTD constant to MarkupResourceStream. Updating documentation for wicket namespace. $$1
wicket-5c592d85$$WebRequestCodingStrategy: path mounting and matching$$Assuming a mount path to "/p", it will match /pxyz  Assuming this is the desired behavior of matching (warning), then to avoid this match it should be declared "/p/" but it will create urls such as '/app/p//SomePage'. which is wrong.  In the servlet specs,  the mapping syntax '/p' is an exact match, this is not what you want in your case since you're doing path mapping, so the syntax if you want to stick close to the servlet specs should be '/p/*' or if you wan to get close to mod_proxy syntax it would be '/p/'  Note that the examples are also using this wrong mapping declaration. In the example below: both should throw a 404: http://www.wicket-library.com/wicket-examples/niceurl/my/mounted/packageXXX http://www.wicket-library.com/wicket-examples/niceurl/my/mounted/Xpackage$$patch1-wicket-5c592d85_Developer_PatchNaturalnessYe$$add missing import. add a log. log debug. add return. Allow / mount / point with / extra / path as well. $$1
wicket-1a2bc1bc$$IResponseFilter cannot change buffer contents$$Changes to the responseBuffer, passed to an IResponseFilter, are not picked up, nor are newly created AppendingStringBuffer (return value of the method). Both callers of the method invoke it with a copy of the buffer and ignore return values (BufferedWebResponse line 145 and AjaxRequestTarget line 687).$$patch1-wicket-1a2bc1bc_Developer_PatchNaturalnessYe$$invokeResponseFilters ( ) didn ' t invoke the response filters on the client. Added private method to avoid compiler warning. wicket ajax response target modified ( as per # 1262 ). responseFilters should use the result of the builder instead of the builder .. $$1
wicket-089303f4$$wicketTester.executeAjaxEvent(combo, "onchange"); works with 1.4-rc1 but not anymore with 1.4-rc2$$Try the attached Unit Test.$$patch1-wicket-089303f4_Developer_PatchNaturalnessYe$$add AJAX request redirect header. $$1
wicket-ee02c883$$Mounted bookmarkable Page not recreated on Session Expiry$$With the default true of org.apache.wicket.settings.IPageSettings#getRecreateMountedPagesAfterExpiry() PageExpiryException is thrown when a Link on a page is clicked.  I find it very useful and in fact indispensible to rely on RecreateMountedPagesAfterExpiry especially on logout links.  But it doesn't seem to work for me in 6.4.0. I think this is because Link#getUrl() calls Component#utlFor() which requires a stateless page for this to work:  		if (page.isPageStateless()) 		{ 			handler = new BookmarkableListenerInterfaceRequestHandler(provider, listener); 		} 		else 		{ 			handler = new ListenerInterfaceRequestHandler(provider, listener); 		}  With a stateless page a url is:  http://localhost:8080/wicket/HomePage?-1.ILinkListener-toolBar-signout  With a non stateless but bookmarkable page a url is:  http://localhost:8080/wicket/page?1-1.ILinkListener-toolBar-signout  So I guess that a stateful page cannot be recovered because after session expiry Wicket cannot find out what the page is. It is not coded in the URL.  Looking at the semantics of UrlFor(), I thought this might be a bug and I copied and changed the code in my Link subclass from  //		if (page.isPageStateless()) { to:         if (page.isBookmarkable()) { 		 It works as expected but I don't know whether it would break other things if implemented in Wicket.  I guess I am not the only one who needs recovery for bookmarkable pages in this way. Would it be possible to change Wicket to fix this so it becomes possible?$$patch1-wicket-ee02c883_Developer_PatchNaturalnessYe$$improve performance of components on expire. improve performance of components on expire. $$1
wicket-2737d7c7$$The tbody section of a DataTable is empty when no records are returned by the provider.$$When a DataTable is rendered without records, the tbody section is empty. This violates the html spec.  From the spec: "When present, each THEAD, TFOOT, and TBODY contains a row group. Each row group must contain at least one row, defined by the TR element." and "The THEAD, TFOOT, and TBODY sections must contain the same number of columns."$$patch1-wicket-2737d7c7_Developer_PatchNaturalnessYe$$added onConfigure method on data table. $$1
wicket-e93fdd5a$$Last-modified header of external markup is ignored$$When using external base markup(in my case a drupal page with a wicket:child element in it) this markup is supposed to be cached after first fetch. For subsequent requests the last-modified header is checked to see if the markup has changed and when it has the markup is fetched again.  This does not work, Connections.getLastModified(URL url) always returns 0 when the URL is a http url(in fact, when url.openConnection returns a sun.net.www.protocol.http.HttpURLConnection.  Solution could be to not setDoInput to false on this URLConnection(tested that)$$patch1-wicket-e93fdd5a_Developer_PatchNaturalnessYe$$Remove breaks wicket - util / io / Connections .. $$1
wicket-6122df49$$Minified css/js gets compressed$$Given an application with a resource reference to a minified script, i.e. html5.js and html5.min.js.  When the ResourceRequestHandler responds  it will set compress to false, if the resource reference was PackageResourceReference but it will not change compression if the resource reference was JavaScriptResourceReference.   PackageResourceReference handles minified resources more or less correctly (if they are minified, they should not be further compressed), but this behavior is overwritten in its subclasses.$$patch1-wicket-6122df49_Developer_PatchNaturalnessYe$$remove compress flag if necessary. wicket - api does not compress css files by default. disable minified flag for PackageResourceReference. $$1
wicket-8e794fc4$$@Inject should require the bean dependency instead of setting null$$When using {{@SpringBean}}, if the bean cannot be injected then Wicket will throw {{Exception}}.  However current behavior if when using {{@Inject}} inside component, the field will be left as null. This is inconsistent behavior with what CDI spec and how the "real" Spring does it.  Wicket should change its behavior so that {{@Inject}} is always required. If the dependency is optional the user can use {{@SpringBean(required=false)}} as always.$$patch1-wicket-8e794fc4_Developer_PatchNaturalnessYe$$Add missing author tag. wicket - spring - injection - annot field is not required by default. $$1
wicket-4d3d1f85$$RestartResponseAtInterceptPageException.InterceptData is never cleared$$RestartResponseAtInterceptPageException.InterceptData is supposed to be cleared after continueToOriginalDestination() is called. This is accomplished via RestartResponseAtInterceptPageException.MAPPER, which is registered in the SystemMapper.  However there seems to be a serious bug here. The MAPPER always returns a compatibilityScore of 0, and thus is never actually invoked. The InterceptData is thus never cleared. Furthermore, even if the MAPPER did return a Integer.MAX_VALUE score, it would still not be invoked in many scenarios, since other mappers in the SystemMapper are registered later and therefore have higher priority.  In practice, this can lead to very odd login behavior in Wicket applications (which is where RestartResponseAtInterceptPageException is typically used). For example, if the user clicks a "login" link they may end up on a totally unexpected page, due to stale InterceptData that is hanging around in the session.  I am attaching a quick start that demonstrates the problem, as well as a patch the fixes the compatibilityScore and moves the MAPPER to a higher priority in the SystemMapper.$$patch1-wicket-4d3d1f85_Developer_PatchNaturalnessYe$$Fix requestCompatibilityScore ( ). wicket - core - 226 fixed case with IWritableRequestParameters. revert accidently change. $$1
wicket-e60bac5f$$Wicket 1.5 Form Post Action and Link Get discard Page Class Information$$Page expiry is a very annoying and perplexing event especially if users stay logged in via remember-me cookie.  It is therefore not a fancy enhancement but an essential business requirement to not drop the user out of context after session expiry. Only stateless pages can fully achieve this, but it is not always desirable to go fully stateless, especially while a recovery solution already exists. In 1.4, this appears to be automatic with BookmarkablePageRequestTargetUrlCodingStrategy - without any additional coding.  The solution is well known - keep as much state in the client as required to recover the page class, and possibly even its page parameters, and to not destroy this information.  The two attached testcases show two possible methods of page fallback recovery (one with AJAX, one without) that already work behind the scenes. Of course it is easy with AJAX, to just force a page reload, but this is not discussed here. AJAX just serves to demonstrate how easy the principle actually is. In most cases the user could successfully reload the page but Wicket 1.5 can't create a response because it has forgotten the class of the expired page.  In 1.4, it is possible to recover the class of an expired page via its mount path. This feature is lost in 1.5.  To get this functionality back in a more streamlined fashion, I am additionaly proposing in a separate jira issue 4013 to store page class and page parameters in PageExpiredException.  Meanwhile, the focus of this issue is to request whatever means to not overwrite the path of a page in a form post action or get request, and to get the page class back as in 1.4 by whatever means.  The two attached testcases may be helpful for expermintation. The 1.4 tescase demonstrates how the scheme works, unfortunately I could not fill the blanks in the 1.5 testcase.  In 1.4,  a form tag is rendered as: <form wicket:id="form" action="?wicket:interface=:0:form::IFormSubmitListener::" This is requested as: /testForm.0?wicket:interface=:0:form::IFormSubmitListener:: and the page class can be recovered from the mount path "testForm" as in     mount(new HybridUrlCodingStrategy("testForm", TestPageForm.class));  an anchor tag is rendered as: <a href="?wicket:interface=:0:linkSwitch::ILinkListener::" This is requested as: /testLink.0?wicket:interface=:0:linkSwitch::ILinkListener:: and the page class can be recovered from the mount path "test" as in     mount(new HybridUrlCodingStrategy("testLink", TestPageLink.class));    In 1.5,  a form tag is rendered as: <form wicket:id="form" action="wicket/page?0-2.IFormSubmitListener-form" This is requested requested as: /wicket/page?0-1.IFormSubmitListener-form  This overwrites the mount path "testForm" as in     mountPage("testForm", TestPageForm.class); Consequently the server cannot discover the page class   an anchor tag is rendered as: <a href="wicket/page?0-1.ILinkListener-linkSwitch" This is requested requested as: /wicket/page?0-1.ILinkListener-linkSwitch  This overwrites the mount path "testLink" as in     mountPage("testLink", TestPageLink.class); Consequently the server cannot discover the page class$$patch1-wicket-e60bac5f_Developer_PatchNaturalnessYe$$add missing imports. add missing method .. $$1
wicket-7ca927c1$$HttpSession getSession() in MockHttpServletRequest is not compliant with the j2ee servlet spec$$The implementation of httpRequest.getSession(); for MockHttpServletRequest seems not correct since it can return null when the servler api specs (http://docs.oracle.com/javaee/1.4/api/) says:  public HttpSession getSession() Returns the current session associated with this request, or if the request does not have a session, creates one.  So as far as I understand httpRequest.getSession(); and httpRequest.getSession(true); are equivalent  The MockHttpServletRequest implementation is     public HttpSession getSession()    {        if (session instanceof MockHttpSession && ((MockHttpSession)session).isTemporary())        {            return null;        }        return session;    }  I think it should be     public HttpSession getSession()    {        return getSession(true);    }$$patch1-wicket-7ca927c1_Developer_PatchNaturalnessYe$$add getSession ( ) to the mock request so that we can use getSession ( ). session . isTemporary ( ) == false. $$1
wicket-3d8c9d75$$Adding a component in Component#onInitialize() leads to StackOverflowError$$Adding a component in Page#onInitialize() leads to StackOverflowError:   at org.apache.wicket.MarkupContainer.addedComponent(MarkupContainer.java:978)      at org.apache.wicket.MarkupContainer.add(MarkupContainer.java:168)      at org.apache.wicket.examples.WicketExamplePage.onInitialize(WicketExamplePage.java:67)      at org.apache.wicket.Component.initialize(Component.java:970)      at org.apache.wicket.MarkupContainer.initialize(MarkupContainer.java:992)      at org.apache.wicket.Page.componentAdded(Page.java:1130)      at org.apache.wicket.MarkupContainer.addedComponent(MarkupContainer.java:978)      at org.apache.wicket.MarkupContainer.add(MarkupContainer.java:168)      at org.apache.wicket.examples.WicketExamplePage.onInitialize(WicketExamplePage.java:67)      at org.apache.wicket.Component.initialize(Component.java:970)      at org.apache.wicket.MarkupContainer.initialize(MarkupContainer.java:992)      at org.apache.wicket.Page.componentAdded(Page.java:1130)      at org.apache.wicket.MarkupContainer.addedComponent(MarkupContainer.java:978)      at org.apache.wicket.MarkupContainer.add(MarkupContainer.java:168)      at org.apache.wicket.examples.WicketExamplePage.onInitialize(WicketExamplePage.java:67)      at org.apache.wicket.Component.initialize(Component.java:970)      at org.apache.wicket.MarkupContainer.initialize(MarkupContainer.java:992)      at org.apache.wicket.Page.componentAdded(Page.java:1130)      at org.apache.wicket.MarkupContainer.addedComponent(MarkupContainer.java:978)      at org.apache.wicket.MarkupContainer.add(MarkupContainer.java:168)      at org.apache.wicket.examples.WicketExamplePage.onInitialize(WicketExamplePage.java:67)      at org.apache.wicket.Component.initialize(Component.java:970)      at org.apache.wicket.MarkupContainer.initialize(MarkupContainer.java:992)      at org.apache.wicket.Page.componentAdded(Page.java:1130)      at org.apache.wicket.MarkupContainer.addedComponent(MarkupContainer.java:978)      at org.apache.wicket.MarkupContainer.add(MarkupContainer.java:168)      at org.apache.wicket.examples.WicketExamplePage.onInitialize(WicketExamplePage.java:67)      at org.apache.wicket.Component.initialize(Component.java:970) ...$$patch1-wicket-3d8c9d75_Developer_PatchNaturalnessYe$$wicket - 967 added default constructor onInitialize. $$1
wicket-3fc7234e$$Url.canonical() breaks when there are two consecutive "parent" segments followed by a normal segment$$assertEquals("a/d", Url.parse("a/b/c/../../d").canonical().getPath());   breaks with : Expected :a/d Actual   :a/b/../d$$patch1-wicket-3fc7234e_Developer_PatchNaturalnessYe$$Allow Url . segments . add ( segment ) to be added to the Url object. $$1
wicket-0cf14725$$SmartLinkLabel failing to process email with +$$Using SmartLinkLabel with an email address that includes a "+" generates a link only on the right-most part of the address.  Example: - my+test@example.com Will generate a link like: - my+<a href="mailto:test@example.com">test@example.com@pappin.ca</a>  THe addition of the "+" char is a valid email address format.$$patch1-wicket-0cf14725_Developer_PatchNaturalnessYe$$Adding backslash to email address pattern. $$1
wicket-5d64196a$$XsltOutputTransformerContainer incorrectly claims markup type "xsl"$$XsltOutputTransformerContainer return "xsl" from getMarkupType(), forcing is on all contained components.  If the components in org.apache.wicket.markup.outputTransformer.Page_1 are reordered (XsltOutputTransformerContainer coming first) the test fails because no markup for SimpleBorder can be found.$$patch1-wicket-5d64196a_Developer_PatchNaturalnessYe$$Removed unused imports. Fix well formedness of xhtml1 . 3 - strict .. Removed unused getter forMarkupType. $$1
wicket-fae1601b$$Page.checkRendering fails after setting BorderBodyContainer visiblity to false$$After toggling visibility of the BorderBodyContainer to false the Page.checkRendering method fails in line 1157, claiming an iterator IllegalStateException. This happens because iterator.remove() is called twice for a child component in the border component, if the body is not visible.  My Code:  public class TogglePanel extends Border { 	private boolean expanded = true;  	public TogglePanel(String id, IModel<String> titleModel) { 		super(id, titleModel);  		Link link = new Link("title") {  			@Override 			public void onClick() { 				expanded = !expanded; 				getBodyContainer().setVisible(expanded); 			} 		}; 		link.add(new Label("titleLabel", titleModel));  		add(link); 	}  }  Markup:  <wicket:border> 	<h3 class="collapse" wicket:id="title"> 		<span class="label" wicket:id="titleLabel">Panel Title</span> 		<a class="foldicon">&nbsp;</a> 	</h3> 	<wicket:body /> </wicket:border>$$patch1-wicket-fae1601b_Developer_PatchNaturalnessYe$$removed try block for URL to work when form is loaded .. improve comment. wicket - 91717 Fix unrenderedComponents . iterator ( ). avoid infinite loop. Remove erroneous comment. $$1
wicket-36a41358$$IndexOutOfBoundsException when PropertyResolver is using an invalid list index$$When using  PropertyResolver.getValue("myList[1]", myBean),  the PropertyResolver ListGetSet.getValue() (line 762) unconditionally does: return ((List)object).get(index); which throws an   java.lang.IndexOutOfBoundsException: Index: 1, Size: 1  if the backing list contains only one element (at index 0). Shouldn't the implementation rather return null like with every other property not found? Like when using "bla.bli.blo" as a lookup string and there is no bla field and no getBla() method?  So this method should rather be:  org.apache.wicket.util.lang.PropertyResolver ListGetSet.getValue():  		/** 		 * @see org.apache.wicket.util.lang.PropertyResolver.IGetAndSet#getValue(java.lang.Object) 		 */ 		public Object getValue(Object object) 		{ 			List list = (List) object; 			if (index >= list.size()) { 				return null; 			} 			return list.get(index); 		}$$patch1-wicket-36a41358_Developer_PatchNaturalnessYe$$Allow the expression to be resolved if it is empty .. enables null values for property resolvers ( wicket / util / PropertyResolver . getValue ( object ,. Fixed error in PropertyResolver # getValue. $$1
wicket-74e77676$$ISecuritySettings#getEnforceMounts(true) prevents access to *all* non-mounted bookmarkable pages$$ISecuritySettings#setEnforceMounts(true) is meant to be used to prevent access to mounted-pages via BookmarkableMapper, e.g. when Page1.class is mounted:     http://localhost:8080/niceurl/a/nice/path/to/the/first/page  ... then the following url will not be accepted:     http://localhost:8080/niceurl/wicket/bookmarkable/org.apache.wicket.examples.niceurl.Page1  But starting with Wicket 1.5.x access to *all* non-mounted pages via BookmarkableMapper is prevented, i.e. no url "http://localhost:8080/niceurl/wicket/bookmarkable/*" is matched.$$patch1-wicket-74e77676_Developer_PatchNaturalnessYe$$add missing import. add back missing author tag. Added missing javadoc. Allow security settings to be mounted on mobile devices. do not force mount of old homepage. $$1
wicket-50b52742$$ByteArrayResource throws error if data is null$$When ByteArrayResource#getData(org.apache.wicket.request.resource.IResource.Attributes) returns null, the class throws a WicketRuntimeException.  This behavior differs from DynamicImageResource and ResourceStreamResource which instead issue the following call: response.setError(HttpServletResponse.SC_NOT_FOUND);  ByteArrayResource should follow the same behavior. This would allow for instance to use it for resources which depend on the contents of attributes.getParameters(). When the parameters are invalid, a 404 should be issued instead of an exception.$$patch1-wicket-50b52742_Developer_PatchNaturalnessYe$$add missing semicolon. Added missing import. fixed error code. $$1
wicket-b6259e5f$$arraycopy with bad length in AbstractRequestLogger:172$$When clicking on DebugBar org.apache.wicket.devutils.inspector.LiveSessionsPage NullPointerException is thrown. After investigating the reason I think AbstractRequestLogger:172 arraycopy params cause it.  {{arraycopy(requestWindow, 0, copy, requestWindow.length - oldestPos, indexInWindow);}} Should be changed to: {{arraycopy(requestWindow, 0, copy, requestWindow.length - oldestPos, oldestPos);}}$$patch1-wicket-b6259e5f_Developer_PatchNaturalnessYe$$remove copyRequestsInOrder redundant code. $$1
wicket-2fcb3417$$Url#getQueryString(charset) method returns quesrystring with "?" prefixed to it$$i have just pointed out 6.0.0-beta3/6.x but it must be same in 1.5.x too ,afaik "?" is not considered part of querystring ,"?" is considered separator see http://tools.ietf.org/html/rfc3986#section-3 this method is used in Url#toString() too which can be easily fixed but it may be used at other places too so i don't know if removing "?" will break things now.  so how things break currently RequestUtils.decodeParameters(url.getQueryString(),parameters); decodeparameters will considered first key to be "?key"  so may be requestutils#decodeparameters method should strip away "?" if it's present in the query string before populating pageparameters  thanks!$$patch1-wicket-2fcb3417_Developer_PatchNaturalnessYe$$wicket - request doesn ' t include query string in Url class instance in case of error. $$1
wicket-1fb66533$$ServletWebResponse#encodeUrl() makes absolute Urls relative$$When an absolute (full) URL is passed to ServletWebResponse#encodeUrl(), it will be returned relative if the container encodes a session ID.$$patch1-wicket-1fb66533_Developer_PatchNaturalnessYe$$Fix bug in ServletWebResponse. Fix bug in ServletWebResponse. $$1
wicket-b76f9c44$$CreditCardValidator returns incorrect cardId for VISA$$When the validation for a VISA is correct, it returns a SWITCH cardId instead of a VISA. This error occurs in both 1.4.x and 1.5.X$$patch1-wicket-b76f9c44_Developer_PatchNaturalnessYe$$Allow credit card number to be used as a valid card number. $$1
wicket-aadaa4e9$$PageParameters#set not follow INamedParameters#set behavior$$Couple of problems to work with page parameters: Major - The PageParameters#set(final String name, final Object value, final int index) used remove/add pattern instead of set parameter value by specified index. Minor - Inposible to get the index of key in elegant way to use obtained index in #set operation$$patch1-wicket-aadaa4e9_Developer_PatchNaturalnessYe$$@@ set ( String , Object ). $$1
wicket-6e0b40bc$$MockHttpServletRequest is broken when used with CryptedUrlWebRequestCodingStrategy$$Upgraded to 1.3.6. One of my test cases started to fail with  org.apache.wicket.WicketRuntimeException: Internal error parsing wicket:interface = ?x=GR7uTj8e-D8FE0tmM9vvYcwdiASd9OJ5GgveAhSNaig       I tracked down the issue to MockHttpServletRequest .setRequestToComponent() In line 1253 it check for url starting with 6*. However, in CryptedUrlWebRequestCodingStrategy following encryption is employed:  198:					queryString = shortenUrl(queryString).toString(); 199: 200:					// encrypt the query string 201:					String encryptedQueryString = urlCrypt.encryptUrlSafe(queryString);   shortenUrl will replace 'wicket:interface=' with '6*' but then it gets immediately encrypted, consequently MockHttpServletRequest  will never recognize it correctly.$$patch1-wicket-6e0b40bc_Developer_PatchNaturalnessYe$$remove useless check for url encodable parameters. $$1
wicket-6470c3f7$$encodeUrl fails parsing jsessionid when using root context$$We are using Selenium 2.26.0 to test our Wicket application, using Jetty 6.1.25 (also tried 7.0.0.pre5) and Firefox 12 as client browser.  With Wicket 1.5.8 everything worked fine but updating to 1.5.9 the following error occurs on first request:  java.lang.NumberFormatException: For input string: "56704;jsessionid=t3j8z4tsuazh1jfbcnjr8ryg" 	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:48) 	at java.lang.Integer.parseInt(Integer.java:458) 	at java.lang.Integer.parseInt(Integer.java:499) 	at org.apache.wicket.request.Url.parse(Url.java:195) 	at org.apache.wicket.request.Url.parse(Url.java:121) 	at org.apache.wicket.protocol.http.servlet.ServletWebResponse.encodeURL(ServletWebResponse.java:194) 	at org.apache.wicket.protocol.http.HeaderBufferingWebResponse.encodeURL(HeaderBufferingWebResponse.java:161) 	at org.apache.wicket.request.cycle.RequestCycle.renderUrl(RequestCycle.java:524) 	at org.apache.wicket.request.cycle.RequestCycle.urlFor(RequestCycle.java:492) 	at org.apache.wicket.request.cycle.RequestCycle.urlFor(RequestCycle.java:477) 	at org.apache.wicket.Component.urlFor(Component.java:3319) 	at org.apache.wicket.markup.html.link.BookmarkablePageLink.getURL(BookmarkablePageLink.java:209) 	at org.apache.wicket.markup.html.link.Link.onComponentTag(Link.java:361) 	at org.apache.wicket.Component.internalRenderComponent(Component.java:2530) 	at org.apache.wicket.MarkupContainer.onRender(MarkupContainer.java:1530) 	at org.apache.wicket.Component.internalRender(Component.java:2389) 	at org.apache.wicket.Component.render(Component.java:2317) 	at org.apache.wicket.MarkupContainer.renderNext(MarkupContainer.java:1428) 	at org.apache.wicket.MarkupContainer.renderAll(MarkupContainer.java:1592) 	at org.apache.wicket.Page.onRender(Page.java:907) 	at org.apache.wicket.markup.html.WebPage.onRender(WebPage.java:140) 	at org.apache.wicket.Component.internalRender(Component.java:2389) 	at org.apache.wicket.Component.render(Component.java:2317) 	at org.apache.wicket.Page.renderPage(Page.java:1035) 	at org.apache.wicket.request.handler.render.WebPageRenderer.renderPage(WebPageRenderer.java:118) 	at org.apache.wicket.request.handler.render.WebPageRenderer.respond(WebPageRenderer.java:246) 	at org.apache.wicket.request.handler.RenderPageRequestHandler.respond(RenderPageRequestHandler.java:167) 	at org.apache.wicket.request.cycle.RequestCycle HandlerExecutor.respond(RequestCycle.java:784) 	at org.apache.wicket.request.RequestHandlerStack.execute(RequestHandlerStack.java:64) 	at org.apache.wicket.request.cycle.RequestCycle.executeExceptionRequestHandler(RequestCycle.java:304) 	at org.apache.wicket.request.cycle.RequestCycle.executeExceptionRequestHandler(RequestCycle.java:313) 	at org.apache.wicket.request.cycle.RequestCycle.executeExceptionRequestHandler(RequestCycle.java:313) 	at org.apache.wicket.request.cycle.RequestCycle.executeExceptionRequestHandler(RequestCycle.java:313) 	at org.apache.wicket.request.cycle.RequestCycle.executeExceptionRequestHandler(RequestCycle.java:313) 	at org.apache.wicket.request.cycle.RequestCycle.executeExceptionRequestHandler(RequestCycle.java:313) 	at org.apache.wicket.request.cycle.RequestCycle.executeExceptionRequestHandler(RequestCycle.java:313) 	at org.apache.wicket.request.cycle.RequestCycle.executeExceptionRequestHandler(RequestCycle.java:313) 	at org.apache.wicket.request.cycle.RequestCycle.executeExceptionRequestHandler(RequestCycle.java:313) 	at org.apache.wicket.request.cycle.RequestCycle.executeExceptionRequestHandler(RequestCycle.java:313) 	at org.apache.wicket.request.cycle.RequestCycle.executeExceptionRequestHandler(RequestCycle.java:313) 	at org.apache.wicket.request.cycle.RequestCycle.processRequest(RequestCycle.java:227) 	at org.apache.wicket.request.cycle.RequestCycle.processRequestAndDetach(RequestCycle.java:283) 	at org.apache.wicket.protocol.http.WicketFilter.processRequest(WicketFilter.java:188) 	at org.apache.wicket.protocol.http.WicketFilter.doFilter(WicketFilter.java:244)  Using debugger, the encodeUrl method has variables   fullUrl = http://localhost:56704 encodedFullUrl = http://localhost:56704;jsessionid=8kxeo3reannw1qjtxgkju8yiu  before the exception occurs. I believe this is related to https://issues.apache.org/jira/browse/WICKET-4645.$$patch1-wicket-6470c3f7_Developer_PatchNaturalnessYe$$Add missing ; in Url. $$1
wicket-fb45a781$$Page not recognized as stateless although stateful component is hidden in #onConfigure()$$Page#stateless gets cached. If Page#isStateless() is called before rendering, a page might not be considered stateless although in #onConfigure() all stateful components are hidden.$$patch1-wicket-fb45a781_Developer_PatchNaturalnessYe$$remove duplicated code. nullify stateless in renderCount. $$1
wicket-917dd2b5$$Handling of NO_MINIFIED_NAME in PackageResourceReference#internalGetMinifiedName()$$The Value NO_MINIFIED_NAME is not handled correctly as entry in the MINIFIED_NAMES_CACHE in PackageResourceReference#internalGetMinifiedName()     	private String internalGetMinifiedName() 	{ 		String minifiedName = MINIFIED_NAMES_CACHE.get(this); 		if (minifiedName != null && minifiedName != NO_MINIFIED_NAME) 		{                                                        ^^^^^^^ 			return minifiedName;                 }                 ...  You should remove the condition "minifiedName != NO_MINIFIED_NAME" here to leverage the  MINIFIED_NAMES_CACHE for NO_MINIFIED_NAME cache entries. Otherwise you always run into the resource resolving code if there is no minified resource.$$patch1-wicket-917dd2b5_Developer_PatchNaturalnessYe$$Fixed test in PackageResourceReference. $$1
wicket-295e73bd$$IResponseFilter doesn't work in 1.5$$In current 1.5-SNAPSHOT there are no callers of org.apache.wicket.settings.IRequestCycleSettings.getResponseFilters() and thus filters are never executed.$$patch1-wicket-295e73bd_Developer_PatchNaturalnessYe$$add missing import. Add missing import. restore the original response object on after response writer. Added after - respond listeners. Remove use of deprecated WebResponse API. Remove unused WebResponse . respondComponents ( ) modifier from above. add missing import. responseFilters support. add missing @@. $$1
wicket-1dcaec98$$SmartLinkLabel doesn't recognize already tagged links$$The SmartLinkLabel works as expected for the texts without <a>..</a> tag.  for text like extensions @ http://www.wicketframework.org/wicket-extensions/index.html are cool!! SmartLinkLabel generates the html -  extensions @ <a href="http://www.wicketframework.org/wicket-extensions/index.html">http://www.wicketframework.org/wicket-extensions/index.html</a> are cool!!  but for the text like extensions @ <a href='http://www.wicketframework.org/wicket-extensions/index.html'>http://www.wicketframework.org/wicket-extensions/index.html</a> are cool!! SmartLinkLabel generates the html -  extensions @ <a href='<a href="http://www.wicketframework.org/wicket-extensions/index.html">http://www.wicketframework.org/wicket-extensions/index.html</a>'><a href="http://www.wicketframework.org/wicket-extensions/index.html">http://www.wicketframework.org/wicket-extensions/index.html</a></a> are cool!!  I think this is a bug & needs a fix.$$patch1-wicket-1dcaec98_Developer_PatchNaturalnessYe$$Don ' t try and parse plain text with < > , as we don ' t do anything. $$1
wicket-9ced53a5$$Inheritance layout excludes XML header from output$$When using inheritance layout, if the superclass (Layout class) has an ?xml header at the top, it's excluded from the rendering of subclasses, if they have an associated html file. If the subclass has no .html file associated with it, the ?xml header is preserved in the rendering output.  To reproduce: Create a SuperPage class extending WebPage. At the top of SuperPage.html, put "<?xml version="1.0" encoding="utf-8"?>" . Create two subclasses of SuperPage, one with an HTML file and one without. View the sub pages. Notice when the one with an HTML file is rendered, the xml header is excluded.  Expected: The ?xml header should always be preserved in the rendered output as it's vital to the layout.$$patch1-wicket-9ced53a5_Developer_PatchNaturalnessYe$$Copy settings from baseMarkup to mergedMarkup. $$1
wicket-217fbb3b$$Ajax update renders parent/child JS in different order than initial Page render$$See attached quickstart.  On initial page load, the child Javascripts are rendered and executed first, followed by the parent's JS - in this case a Datatables.net JS. Everything works fine.  However, if you click on a link in the DefaultDataTable, we trigger a DDT refresh via Ajax, and then you can see that the parent's JS is executed first, before the child JS - this causes a problem since the parent JS modifies the visible rows in the table and Wicket can no longer find some of the child rows.  I expected the order of JS contributions to be the same for initial page render and any Ajax updates.$$patch1-wicket-217fbb3b_Developer_PatchNaturalnessYe$$wicket - ajax - event - callback script appends to the target ( which will never be null. "Revert "" Remove unused import """. Add missing import. Remove unused imports. I should not have pushed this in. $$1
wicket-35843c19$$HtmlHandler wrongly handles tags not requiring closed tags if the markup does not have "top" level tag$$Hi,   I have custom component (extends MarkupContainer implements IMarkupCacheKeyProvider, IMarkupResourceStreamProvider) which fetches its HTML markup from database.  Following HTML markup:   <img alt="" src="logo.png">  <br>Some text  <br>Some more text   causes following error:   2012-04-12 10:52:53,012 [http-8080-6] ERROR: Unexpected error occurred  Unable to find close tag for: '<img alt="logo" src="logo.png">' in org.apache.wicket.util.resource.StringResourceStream@3d7e16fc   MarkupStream: [unknown]          at org.apache.wicket.markup.MarkupFragment.<init>(MarkupFragment.java:127)          at org.apache.wicket.markup.MarkupStream.getMarkupFragment(MarkupStream.java:485)          at org.apache.wicket.MarkupContainer.autoAdd(MarkupContainer.java:244)          at org.apache.wicket.MarkupContainer.renderNext(MarkupContainer.java:1421)          at org.apache.wicket.MarkupContainer.renderAll(MarkupContainer.java:1596)          at org.apache.wicket.MarkupContainer.renderComponentTagBody(MarkupContainer.java:1571)          at org.apache.wicket.MarkupContainer.onComponentTagBody(MarkupContainer.java:1525)   I think the problem is that org.apache.wicket.markup.parser.filter.HtmlHandler does not handle such markup correctly. It does not call ComponentTag.setHasNoCloseTag(true) for the img tag. Such call is missing in postProcess() method. I think that this problem can be fixed by inserting:   top.setHasNoCloseTag(true);   after line 80 in HtmlHandler.java file.    Michal$$patch1-wicket-35843c19_Developer_PatchNaturalnessYe$$forgot a close tag in HtmlHandler .. $$1
wicket-a7ce7f91$$DownloadLink doesn't wrap the String model used for file name nor does it detach$$Component DownloadLink doesn't call method wrap of class Component on parameter fileNameModel. This causes models like StringResourceModel to not resolve resource bundles correctly. See the discussion here: http://stackoverflow.com/questions/12196533/how-to-use-wicket-stringresourcemodel-in-downloadlink  The patch seems quite trivial.   Detachment is also missing.$$patch1-wicket-a7ce7f91_Developer_PatchNaturalnessYe$$add detachModels ( ). $$1
wicket-03663750$$Page header isn't rendered for pages where URL has changed during render$$Due to the changes in WICKET-5309, a page is re-rendered when any of the URL segments is modified during the request:  From WebPageRenderer.java:  {code} 	// the url might have changed after page has been rendered (e.g. the 	// stateless flag might have changed because stateful components 	// were added) 	final Url afterRenderUrl = requestCycle 		.mapUrlFor(getRenderPageRequestHandler());  	if (beforeRenderUrl.getSegments().equals(afterRenderUrl.getSegments()) == false) 	{ 		// the amount of segments is different - generated relative URLs 		// will not work, we need to rerender the page. This can happen 		// with IRequestHandlers that produce different URLs with 		// different amount of segments for stateless and stateful pages 		response = renderPage(afterRenderUrl, requestCycle); 	}  	if (currentUrl.equals(afterRenderUrl)) {code}  The re-render causes the <head> section to be empty because it was already rendered in the first try.$$patch1-wicket-03663750_Developer_PatchNaturalnessYe$$added onAfterRender method. $$1
wicket-2fc6a395$$Method Strings.join doesn't work correctly if separator is empty.$$If we use an empty separator ("") to join strings, the first character of any fragment is truncated. Es "foo", "bar", "baz" became "ooaraz".$$patch1-wicket-2fc6a395_Developer_PatchNaturalnessYe$$remove empty separator string in last piece. $$1
wicket-71b6e905$$NPE with nested property models$$After updated from 1.4.8 to 1.4.14 I got this bug.  The problem is with nested property models where the "top" model has a null model object that is bound to a TextField. You get a NPE when the page is rendered. There is a quick workaround by overriding getOjbectClass() on the property model.  I provide a running example of the problem.$$patch1-wicket-71b6e905_Developer_PatchNaturalnessYe$$corrected a null targetClass value .. $$1
wicket-71674df5$$Problem with WICKET-4441 and RestartResponseAtInterceptPageException$$WICKET-4441 introduced an issue when our app has an authorization strategy and user is logged out. If user tries to access a protected url/page, RestartResponseAtInterceptPageException is handled by DefaultExceptionMapper and leads to exception page instead of redirecting user.$$patch1-wicket-71674df5_Developer_PatchNaturalnessYe$$Correctly handle pagination in WICKET - 5734. $$1
wicket-7e1000dd$$Debug settings / serialize session attributes option not working$$Session attributes are serialized even if this debug setting is turned off. I've noticed that the code that serializes attributes and logs their serialized size in HttpSessionStore#setAttribute is duplicated in Session#setAttribute - but without the debug settings condition. This code was added by the recent patch resolving WICKET-100 and only in the trunk, not in the wicket-1.x branch... why???  Regards, Bendis$$patch1-wicket-7e1000dd_Developer_PatchNaturalnessYe$$Removed unused imports .. remove unused imports. Remove debug code. $$1
wicket-381b90fd$$Cookies#isEqual(Cookie, Cookie) may fail with NullPointerException$$If c1.getPath == null but c2.getPath != null then a NPE will occur. Same is valid for the 'domain' property.$$patch1-wicket-381b90fd_Developer_PatchNaturalnessYe$$add missing import. allow same cookie names. $$1
wicket-0b4f78cc$$ClassCastException when requesting for non-page class$$org.apache.wicket.request.mapper.BookmarkableMapper tries to instantiate Page even for classes which are not Page. Requesting http://localhost:8080/wicket/bookmarkable/com.mycompany.Pojo fails with:  ERROR - DefaultExceptionMapper     - Unexpected error occurred java.lang.ClassCastException: com.mycompany.Pojo 	at org.apache.wicket.session.DefaultPageFactory.newPage(DefaultPageFactory.java:155) 	at org.apache.wicket.session.DefaultPageFactory.newPage(DefaultPageFactory.java:59) 	at org.apache.wicket.session.DefaultPageFactory.newPage(DefaultPageFactory.java:43) 	at org.apache.wicket.Application 2.newPageInstance(Application.java:1425) 	at org.apache.wicket.request.handler.PageProvider.getPageInstance(PageProvider.java:259) 	at org.apache.wicket.request.handler.PageProvider.getPageInstance(PageProvider.java:160) 	at org.apache.wicket.request.handler.render.WebPageRenderer.getPage(WebPageRenderer.java:59) 	at org.apache.wicket.request.handler.render.WebPageRenderer.renderPage(WebPageRenderer.java:131) 	at org.apache.wicket.request.handler.render.WebPageRenderer.respond(WebPageRenderer.java:232) 	at org.apache.wicket.request.handler.RenderPageRequestHandler.respond(RenderPageRequestHandler.java:147) 	at org.apache.wicket.request.RequestHandlerStack.executeRequestHandler(RequestHandlerStack.java:84) 	at org.apache.wicket.request.cycle.RequestCycle.processRequest(RequestCycle.java:217) 	at org.apache.wicket.request.cycle.RequestCycle.processRequestAndDetach(RequestCycle.java:253) 	at org.apache.wicket.protocol.http.WicketFilter.processRequest(WicketFilter.java:135) 	at org.apache.wicket.protocol.http.WicketFilter.doFilter(WicketFilter.java:188) 	at org.mortbay.jetty.servlet.ServletHandler CachedChain.doFilter(ServletHandler.java:1157)          .....$$patch1-wicket-0b4f78cc_Developer_PatchNaturalnessYe$$Updating old import. fixed NPE in Controller. $$1
wicket-240ab3c3$$OnChangeAjaxBehavior attached to DropDownChoice produces two Ajax requests in Chrome v35$$I have a DropDownChoice with attached OnChangeAjaxBehavior, like this: {code:borderStyle=solid} new DropDownChoice<>("dd", new Model<>(), Arrays.asList( "First", "Second"))     .add( new OnChangeAjaxBehavior() {         @Override         protected void onUpdate(AjaxRequestTarget target) {             System.out.println( "update" );         } }); {code}  When selecting any of drop down options, two Ajax requests being generated. It behaves OK in IE, FF and Chrome v34, only Chrome v35 is affected$$patch1-wicket-240ab3c3_Developer_PatchNaturalnessYe$$add missing imports. Added missing implementation of updateAjaxAttributes for text input and textarea components .. $$1
wicket-420ac965$$&amp; instead of & in javascript$$the non httpsessionstore part of: https://issues.apache.org/jira/browse/WICKET-1971  is that   in the  wicket:ignoreIfNotActive actually becomes  amp;wicket:ignoreIfNotActive=true  in:  	protected CharSequence encode(RequestCycle requestCycle, 			IListenerInterfaceRequestTarget requestTarget)  of WebRequestCodingStrategy on the line:  			url.append(url.indexOf("?") > -1 ? "&amp;" : "?").append( 					IGNORE_IF_NOT_ACTIVE_PARAMETER_NAME).append("=true");   so when this happens in  	public final RequestParameters decode(final Request request) {  --- 		if (request.getParameter(IGNORE_IF_NOT_ACTIVE_PARAMETER_NAME) != null) 		{ 			parameters.setOnlyProcessIfPathActive(true); 		} ---  this never actually happens.   then if you have a throttle, ajaxlazyloadpanel etc with onlyprocessifpathactive set to true, and you logout, but go to another wicket page, then the original session is destroyed and a new one is created  if this is worked around in the way the  guys on WICKET-1971 suggest, WebRequestCycleProcessor  method  	public IRequestTarget resolve(final RequestCycle requestCycle, 			final RequestParameters requestParameters)   				if (requestParameters.isOnlyProcessIfPathActive()) last branch falls through: 					else 					{ 						// TODO also this should work.. 					}   and it throws PageExpiredException because the request component/page/behavior does not exist in this new session.   even though onlyprocessifpathactive was set to true, and it's purpose is precisely to avoid pageexpiredexception.$$patch1-wicket-420ac965_Developer_PatchNaturalnessYe$$Removed $ from < pre > statement in AJAX - form submit behavior. Added escapeAttribute method. $$1
wicket-a2f848f2$$404 Error on Nested ModalWindows in IE7 and IE8$$When opening a ModalWindow inside a ModalWindow, the inner ModalWindow generates a 404 error.  Both windows use a PageCreator for content.  To replicate, you must use an actual IE 7 or IE 8 browser, as this does not replicate using developer tools and setting the document and brower to IE 7.  The problem can be seen at http://www.wicket-library.com/wicket-examples/ajax/modal-window.  I will attach a Quickstart as well.$$patch1-wicket-a2f848f2_Developer_PatchNaturalnessYe$$Fixed url instance mapper. context url matcher method for PageInstanceMapper. $$1
wicket-b154d12f$$PagingNavigator.setEnabled(false) doesn't work$$1. Create paging navigator PagingNavigator  2. call PagingNavigator.setEnabled(false) 3. navigator will be rendered as enabled, if click on any link ("1", "2" etc) - content of the data view will be changed.  In many cases it's necessary disable navigator, for example, when user need to edit only single line of DataView other controls need to be disabled.$$patch1-wicket-b154d12f_Developer_PatchNaturalnessYe$$enable PagingNavigator on instanceof PagingLink. Added override to parent class to prevent override by children. disable PagingNavigator on form validation. Added missing super in PagingNavigation. $$1
wicket-57d8f051$$Rescheduling the same ajax timer behavior causes memory leak in the browser$$AbstractAjaxTimerBehavior uses JavaScript setTimeout() function to do its job. It has a #stop() method that clears the timeout but if the timeout is re-scheduled without being cleared a memory leak is observed in the browser.$$patch1-wicket-57d8f051_Developer_PatchNaturalnessYe$$Remove unused imports. Added timers id. Remove blank line. Remove broken setTimeout. Remove debug code. $$1
wicket-54c86ebb$$PageProvider should create a new Page instance if PageParameters are changed, even if a stored page exists.$$The 'getStoredPage(int)' method returns a stored page instance even if user changes parameter values encoded into URL, and the PageParameters object of the stored page instance is never changed. So same page is displayed always though user changes url on browser manually.  ** HOW TO REPRODUCT **  1. unpack the attached sample project 'pagebug.tar.gz'. 2. mvn jetty:run 3. access to http://localhost:8080/user/user1  You will see a form filled with information about user 1. The user's name is 'user 1', age is 30 and country is 'Japan'. The mount path of this page is '/user/ {userId}'. so 'user1' in the accessed url is a parameter value.  after accessing to the url, the url will be changed to http://localhost:8080/user/user1?0 .  it contains the page id of the currently displayed page.  4. change some values and submit the form. page id will be changed on every submit.  5. change only parameter value in url to 'user2'. Never change page-id.  for example, if you now access to http://localhost:8080/user/user1?5, change the url to http://localhost:8080/user/user2?5 .  6. This program must display information about user2, because the parameter value of url is changed. But you will see the information of user 1. Wicket always display the page of page-id = 5 (even though user changed url manually).  In this sample program, I use LoadableDetachableModel for retrieving current parameter-value. But I don't get the new parameter-value because pageParameters object in a page instance is never changed after the construction. pageParameters is fixed in the constructor of Page class.  I think that there are no easy way to retrieve parameter-values encoded into mount-path. Request.getRequestParameters() does not contain parameters encoded into mount-path. So there are no work-around for this issue.   ** HOW TO FIX THIS ISSUE **  We must return null from getStoredPage(int) method of PageProvider class, if current PageParameters is not same with the PageParameters of a stored page. In current code, getStoredPage(int) checks only if the class of both pages are same. We must check the PageParameters of both pages.   ** PATCH **  I attached a pache for PageProvider class. try it.$$patch1-wicket-54c86ebb_Developer_PatchNaturalnessYe$$Added check for same page class in PageProvider. add a helper method for checking page parameters. $$1
wicket-4a6a573b$$MiniMap.iterator().next() should throw NoSuchElementException$$The wicket.util.collections.MiniMap.iterator().next() should throw NoSuchElementException when there are no more elements to return (line 235), please add: if(i >= size)     throw new NoSuchElementException();$$patch1-wicket-4a6a573b_Developer_PatchNaturalnessYe$$Fixed NPE in MiniMap , WICKET - 428 .. throw exception if index > size - 1. $$1
wicket-bcea89fc$$NullPointerException in IntHashMap$$I was looking through a tester's log file to track down a separate issue. I came across a {{NullPointerException}} with {{IntHashMap}}, apparently when the server was shutting down.  See also WICKET-5584, which also deals with a {{NullPointerException}} with {{IntHashMap}}, and also seems to relate to a {{null}} {{modCount}} (judging by the line number).  {noformat} INFO  (ExampleServer) [2014-11-06 00:49:24,979] - com.example.server.ExampleServer.stopServer(ExampleServer.java:268): Stopping server. INFO  (ServerConnector) [2014-11-06 00:49:24,982] - org.eclipse.jetty.server.AbstractConnector.doStop(AbstractConnector.java:306): Stopped ServerConnector@3b7d3a38{HTTP/1.1}{0.0.0.0:8099} INFO  (Application) [2014-11-06 00:49:24,983] - org.apache.wicket.Application.destroyInitializers(Application.java:588): [org.apache.wicket.protocol.http.WicketFilter-55b0dcab] destroy: Wicket core library initializer INFO  (Application) [2014-11-06 00:49:24,983] - org.apache.wicket.Application.destroyInitializers(Application.java:588): [org.apache.wicket.protocol.http.WicketFilter-55b0dcab] destroy: Wicket extensions initializer ERROR (DiskDataStore) [2014-11-06 00:49:24,988] - org.apache.wicket.pageStore.DiskDataStore.saveIndex(DiskDataStore.java:282): Couldn't write DiskDataStore index to file C:\Windows\SERVIC~2\NETWOR~1\AppData\Local\Temp\org.apache.wicket.protocol.http.WicketFilter-55b0dcab-filestore\DiskDataStoreIndex. java.lang.NullPointerException 	at org.apache.wicket.util.collections.IntHashMap HashIterator.<init>(IntHashMap.java:777) 	at org.apache.wicket.util.collections.IntHashMap EntryIterator.<init>(IntHashMap.java:871) 	at org.apache.wicket.util.collections.IntHashMap EntryIterator.<init>(IntHashMap.java:871) 	at org.apache.wicket.util.collections.IntHashMap.newEntryIterator(IntHashMap.java:896) 	at org.apache.wicket.util.collections.IntHashMap EntrySet.iterator(IntHashMap.java:1055) 	at org.apache.wicket.util.collections.IntHashMap.writeObject(IntHashMap.java:1128) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.lang.reflect.Method.invoke(Method.java:483) 	at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988) 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496) 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432) 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178) 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548) 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509) 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432) 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178) 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548) 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509) 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432) 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178) 	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348) 	at java.util.HashMap.internalWriteEntries(HashMap.java:1777) 	at java.util.HashMap.writeObject(HashMap.java:1354) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.lang.reflect.Method.invoke(Method.java:483) 	at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988) 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496) 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432) 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178) 	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348) 	at org.apache.wicket.pageStore.DiskDataStore.saveIndex(DiskDataStore.java:274) 	at org.apache.wicket.pageStore.DiskDataStore.destroy(DiskDataStore.java:106) 	at org.apache.wicket.pageStore.AsynchronousDataStore.destroy(AsynchronousDataStore.java:118) 	at org.apache.wicket.pageStore.AbstractPageStore.destroy(AbstractPageStore.java:53) 	at org.apache.wicket.pageStore.AbstractCachingPageStore.destroy(AbstractCachingPageStore.java:102) 	at org.apache.wicket.page.PageStoreManager.destroy(PageStoreManager.java:437) 	at org.apache.wicket.Application.internalDestroy(Application.java:659) 	at org.apache.wicket.protocol.http.WebApplication.internalDestroy(WebApplication.java:607) 	at org.apache.wicket.protocol.http.WicketFilter.destroy(WicketFilter.java:605) 	at org.eclipse.jetty.servlet.FilterHolder.destroyInstance(FilterHolder.java:173) 	at org.eclipse.jetty.servlet.FilterHolder.doStop(FilterHolder.java:151) 	at org.eclipse.jetty.util.component.AbstractLifeCycle.stop(AbstractLifeCycle.java:89) 	at org.eclipse.jetty.util.component.ContainerLifeCycle.stop(ContainerLifeCycle.java:143) 	at org.eclipse.jetty.util.component.ContainerLifeCycle.doStop(ContainerLifeCycle.java:162) 	at org.eclipse.jetty.server.handler.AbstractHandler.doStop(AbstractHandler.java:73) 	at org.eclipse.jetty.servlet.ServletHandler.doStop(ServletHandler.java:230) 	at org.eclipse.jetty.util.component.AbstractLifeCycle.stop(AbstractLifeCycle.java:89) 	at org.eclipse.jetty.util.component.ContainerLifeCycle.stop(ContainerLifeCycle.java:143) 	at org.eclipse.jetty.util.component.ContainerLifeCycle.doStop(ContainerLifeCycle.java:162) 	at org.eclipse.jetty.server.handler.AbstractHandler.doStop(AbstractHandler.java:73) 	at org.eclipse.jetty.security.SecurityHandler.doStop(SecurityHandler.java:411) 	at org.eclipse.jetty.security.ConstraintSecurityHandler.doStop(ConstraintSecurityHandler.java:457) 	at org.eclipse.jetty.util.component.AbstractLifeCycle.stop(AbstractLifeCycle.java:89) 	at org.eclipse.jetty.util.component.ContainerLifeCycle.stop(ContainerLifeCycle.java:143) 	at org.eclipse.jetty.util.component.ContainerLifeCycle.doStop(ContainerLifeCycle.java:162) 	at org.eclipse.jetty.server.handler.AbstractHandler.doStop(AbstractHandler.java:73) 	at org.eclipse.jetty.server.session.SessionHandler.doStop(SessionHandler.java:127) 	at org.eclipse.jetty.util.component.AbstractLifeCycle.stop(AbstractLifeCycle.java:89) 	at org.eclipse.jetty.util.component.ContainerLifeCycle.stop(ContainerLifeCycle.java:143) 	at org.eclipse.jetty.util.component.ContainerLifeCycle.doStop(ContainerLifeCycle.java:162) 	at org.eclipse.jetty.server.handler.AbstractHandler.doStop(AbstractHandler.java:73) 	at org.eclipse.jetty.server.handler.ContextHandler.doStop(ContextHandler.java:833) 	at org.eclipse.jetty.servlet.ServletContextHandler.doStop(ServletContextHandler.java:215) 	at org.eclipse.jetty.util.component.AbstractLifeCycle.stop(AbstractLifeCycle.java:89) 	at org.eclipse.jetty.util.component.ContainerLifeCycle.stop(ContainerLifeCycle.java:143) 	at org.eclipse.jetty.util.component.ContainerLifeCycle.doStop(ContainerLifeCycle.java:162) 	at org.eclipse.jetty.server.handler.AbstractHandler.doStop(AbstractHandler.java:73) 	at org.eclipse.jetty.server.Server.doStop(Server.java:456) 	at org.eclipse.jetty.util.component.AbstractLifeCycle.stop(AbstractLifeCycle.java:89) 	at com.example.server.ExampleServer.stopServer(ExampleServer.java:269) 	at com.example.server.ExampleServer.stop(ExampleServer.java:279) INFO  (ContextHandler) [2014-11-06 00:49:24,990] - org.eclipse.jetty.server.handler.ContextHandler.doStop(ContextHandler.java:863): Stopped o.e.j.s.ServletContextHandler@63f259c3{/,null,UNAVAILABLE} {noformat}$$patch1-wicket-bcea89fc_Developer_PatchNaturalnessYe$$workflow improvements - added missing read in. $$1
wicket-debca73b$$unable to add nodes to an empty rootless Tree (e.g. LinkTree)$$2 scenarios which adding new nodes (via ajax) to a rootless Tree is not working as expected. the node is getting added to the treemodel but non is displayed.  1) adding a node to the rootnode. the newly added node is not displayed. 2) the rootless tree already has a node. if you add additional nodes to the root node, they will be displayed (compare to 1), if you add an additional node to one of the added nodes, the complete tree will disappear.  see attached quickstart$$patch1-wicket-debca73b_Developer_PatchNaturalnessYe$$Improved JSP compile error. $$1
wicket-f1c0f263$$UrlValidator failes to validate urls that containt multiple dots in path$$refer to UrlValidator.java:466 (isValidPath). if we have an url, that contains more than two consequent dots, for example "http://www.somedomain.com/this_one_is_tricky...but...still.....valid", validator will fail. btw, the other side effect is that countTokens actually counts '...' a two 2dots. One possible workaround is not just count '..' tokens, but count them along with slash, like '../'.$$patch1-wicket-f1c0f263_Developer_PatchNaturalnessYe$$Fix url for example. $$1
wicket-4fc82e35$$WebApplication doesn't recognize if an incoming request is multipart.$$Thanks to the mail at http://apache-wicket.1842946.n4.nabble.com/Read-POST-based-request-from-external-site-td4651269.html we have spotted a problem with method  newWebRequest of class WebApplication.  It seems that this method doesn't test if the original request is multipart and doing so post parameters go lost.  We should create a  MultipartServletWebRequestImpl when such a type of request is being served. I attach a possible patch but I'm not 100% about two things: - which is the best way to determinate if a HttpServletRequest is multipart? - in order to build a MultipartServletWebRequestImpl we need to provide a string identifier for the upload.   How can we generate it (in my patch it's a constant value)?$$patch1-wicket-4fc82e35_Developer_PatchNaturalnessYe$$Add enctype attribute. added enctype to tag. Fixed HTML5 form enctype check. Add missing import. Remove unused import. Adding missing catch block for upload files from form data. $$1
wicket-5226978a$$WicketTester Cookie handling$$While trying to test my SecureForm implementation (https://issues.apache.org/jira/browse/WICKET-1885) with WicketTester I ran into this issue: A cookie set in the response never shows up in the "next" request, because both have their own lists of cookies that aren't shared.  Afaik both should share the same List instance to handle cookies. That way its possible to set a cookie in the response and read it from the request.  A simple testcase is attached.$$patch1-wicket-5226978a_Developer_PatchNaturalnessYe$$Removed unused imports .. add hidden var. added add cookie method to the mock web application. add cookies to the servlet request in case we had source knee - abs case. $$1
wicket-d3dc9a50$$UrlUtils.isRelative returns false if URL parameter contains an absolute URL$$I have a page that gets a return path for a back link as a parameter. A link to this page looks like this:  ./mypage?return=http://example.com  In WebRequestCodingStrategy.encode, this URL is returned by pathForTarget. Then it is checked whether this URL is relative using UrlUtils.isRelative. The URL is apparently relative, but UrlUtils.isRelative returns false, since the check contains:  (url.indexOf("://") < 0  this is false for the above example. Thus, an incorrect path is returned by WebRequestCodingStrategy.encode (relative path resolution does not take place).  A fix for the problem would be to check for   !(url.startsWith("http://") || url.startsWith("https://"))  Or, if other protocols should also be supported, a regular expression like "^[^/?]*://" should work.$$patch1-wicket-d3dc9a50_Developer_PatchNaturalnessYe$$add url matcher. $$1
wicket-7d5b8645$$Form Input example fails when changing the language$$Trying to change the language of http://localhost:8080/forminput example fails with:   Caused by: java.lang.ArrayIndexOutOfBoundsException: -1 	at java.util.ArrayList.remove(ArrayList.java:390) 	at org.apache.wicket.request.Url.resolveRelative(Url.java:884) 	at org.apache.wicket.markup.html.form.Form.dispatchEvent(Form.java:1028) 	at org.apache.wicket.markup.html.form.Form.onFormSubmitted(Form.java:699) 	at org.apache.wicket.markup.html.form.Form.onFormSubmitted(Form.java:670) 	... 37 more$$patch1-wicket-7d5b8645_Developer_PatchNaturalnessYe$$parse url. cleaned up error. Remove queryAt + 147. Remove extraneous whitespace. Fixing attribution for default port for protocol. remove old comment. Updating old comment. update Url class to reflect changes to long form. Fix an issue with Url . isEmpty ( ) being able to represent default port for protocols. Removed old comment. do not strip the first non - folder segment if it is not empty.  wicket - request / Url / Path method with given encoding.  wicket - request does not have iso_ * date in it. cleaned up error. $$1
wicket-8b294488$$Date converters should use a new instance of DateFormat to be thread safe$$Please consider the linked issue WICKET-4833.  I had to open a new issue because I cannot attach the quickstart project I've prepared to a closed issue.$$patch1-wicket-8b294488_Developer_PatchNaturalnessYe$$Fixed thread unsafe converters. $$1
wicket-60d07288$$DropDownChoice no selection value$$This problem came from this topic: http://apache-wicket.1842946.n4.nabble.com/DropDownChoice-no-selection-value-td3160661.html  I've noticed that the method AbstractSingleSelectChoice.getNoSelectionValue() returns the value for no selection. In AbstractSingleSelectChoice.getDefaultChoice(final Object selected) on line 314:        return "\n<option selected=\"selected\" value=\"\">" + option + "</option>";   and on line 296:        buffer.append(" value=\"\">").append(option).append("</option>");   In those cases the null value option has empty value attribute. Wouldn't it be more consistent for this option to have the "value" attribute with the result provided from getNoSelectionValue()?$$patch1-wicket-60d07288_Developer_PatchNaturalnessYe$$avoid NPE in JSF. add code to check null selection value .. avoid NPE. "remove the "" return is not selected "" shortcut in AbstractSingleSelectChoice". $$1
wicket-813d8bee$$Ajax behavior on component with setRenderBodyOnly(true) don't get called - improve warning$$When you put AJAX behavior on component with setRenderBodyOnly(true) and try to call it with callback script it won't get called and no error / warning is displayed. See attached quickstart sample. Just unzipp and run with: mvn jetty:run  Navigate browser to http://localhost:8080/ When you try to click on labels AJAX behavior should get called. But it won't. This kind of behavior is correct (i assume). But i think user should be warned that behavior can't be called. I think proper place is somewhere on server side? But I don't know where exactly put the warning.  Now only message is in Wicket Ajax Debug window - "Ajax request stopped because of precondition check". I had to debug wicket javascript to find what precondition check failed. Maybe more detailed message in default precondition check would be useful too?$$patch1-wicket-813d8bee_Developer_PatchNaturalnessYe$$added group id into the class name. Updating radio imports. added group id into radio tag. $$1
wicket-1b57b51c$$Component#getMarkupId() throws exceptions when not added to page yet$$When retrieving the markup ID for a component that has not yet been added to a page, Wicket currently throws an exception telling that the markup could not be found, or that the markup type (in case the component was added to a Panel) could not be determined. In 1.4, Wicket would generate a markup ID in these cases.  Proposed solution: to first see if a markup ID has been either generated or set (using setOutputMarkupId), and then returning that, or if no ID was yet available *and* the component has been added to a Page: use the ID from the markup, or if the component has not been added to a Page nor a markup ID: generate the ID.$$patch1-wicket-1b57b51c_Developer_PatchNaturalnessYe$$using the inline editor for components on a page. $$1
wicket-86066852$$Queuing a component within an enclosure$$Queueing doesn't work if component is in a enclosure tag.   <wicket:enclosure child="inlink"> <a href="panier.html"> 	<span wicket:id="inlink"></span> </a> </wicket:enclosure>$$patch1-wicket-86066852_Developer_PatchNaturalnessYe$$redundant import. resolve child by ID. added resolveChild ( ). added new dequeue context to Enclosure , possibly hiding opengles , for instance. $$1
wicket-30255f11$$WicketTester does not follow absolute redirects$$Wicket tester does not follow absolute redirects:  This is a problem when using HttpsMapper. For example when requesting a page over http:// with an forced redirect to https:// for secure access will make wicket tester return 'null' for the last renderer page instead of the rendered page instance. In general all kinds of absolute redirects to another page will not be tracked by wicket tester. So this potentially a problem for all kinds of tests that rely on absolute redirects.$$patch1-wicket-30255f11_Developer_PatchNaturalnessYe$$Removed final modifier from Url . parse ( ). Fix Url instance correctly set host and port. Removed unnecessary semicolon .. $$1
wicket-ef880545$$MethodGetAndSet.setValue uses wrong source to determine which type to convert to when there's no setter$$MethodGetAndSet.setValue uses wrong source to determine which type to convert to when there's no setter, resulting in exceptions like this: org.apache.wicket.WicketRuntimeException: Error setting field: private int PropertyResolverTest DirectFieldSetWithDifferentTypeThanGetter.value on object: PropertyResolverTest DirectFieldSetWithDifferentTypeThanGetter@396477d9 	at org.apache.wicket.util.lang.PropertyResolver MethodGetAndSet.setValue(PropertyResolver.java:1150) 	at org.apache.wicket.util.lang.PropertyResolver ObjectAndGetSetter.setValue(PropertyResolver.java:588) 	at org.apache.wicket.util.lang.PropertyResolver.setValue(PropertyResolver.java:136) 	at PropertyResolverTest.testDirectFieldSetWithDifferentTypeThanGetter(PropertyResolverTest.java:12)  Bug is located in: converted = converter.convert(value, getMethod.getReturnType());  Instead, it should read: converted = converter.convert(value, type);  Testcase attached.  Additional thoughts: if (setMethod != null) {   type = getMethod.getReturnType(); } This is really confusing (we check setMethod presence but get type from getMethod). Luckily, this works as expected because in MethodGetAndSet.findSetter only methods with same (or superclass) type as getter are returned.$$patch1-wicket-ef880545_Developer_PatchNaturalnessYe$$Added missing closing parenthesis in ConverterLocator. Fixed NPE in PropertyResolver. PropertyResolver should use the type parameter instead of the method parameter. $$1
wicket-24ac1a35$$Form gets submitted using AjaxSubmitBehavior when sub-form has error's$$from http://www.nabble.com/Should-a-form-submit-when-sub-form-has-error%27s--tt22803314.html  I have a main-form where I add a panel that contains another form. This sub-form contains a formvalidator that gives the error. However the main-form is submitted, but the feedbackpanel does show the error message set in the sub-form's validator.  I'll attach 2 patches with testcases displaying the behavior in wicket 1.3 vs 1.4  (As a side note, I had to rename the org.apache.wicket.markup.html.form.validation.TestHomePage to org.apache.wicket.markup.html.form.validation.HomePageTest to get the test to run when building wicket)$$patch1-wicket-24ac1a35_Developer_PatchNaturalnessYe$$component method should always return true for form or component instance. $$1
wicket-292a2582$$DateTimeField improperly converts time causing wrong dates when the server's current date is different from the client's date.$$The bug is in DateTimeField#convertInput(). <code> // Get year, month and day ignoring any timezone of the Date object Calendar cal = Calendar.getInstance(); cal.setTime(dateFieldInput); int year = cal.get(Calendar.YEAR); int month = cal.get(Calendar.MONTH) + 1; int day = cal.get(Calendar.DAY_OF_MONTH); int hours = (hoursInput == null ? 0 : hoursInput % 24); int minutes = (minutesInput == null ? 0 : minutesInput);  // Use the input to create a date object with proper timezone MutableDateTime date = new MutableDateTime(year, month, day, hours, minutes, 0, 0, 	DateTimeZone.forTimeZone(getClientTimeZone())); </code> If the server's current date is different from the client's, this produces wrong output. I attached a patch with a test case that simulates this condition.  I don't know why this "casting" of day, month, year is done.$$patch1-wicket-292a2582_Developer_PatchNaturalnessYe$$enable short style selection. enable timeZoneDifference for DateTextField. Fix for Persian and Urdu language maps # 2019. $$1
wicket-9da430fb$$Generated urls for mounted pages contain redundant trailing "/"$$Is it OK (i.e. "by design" as opposed to "by mistake") that the urls generated for the mounted pages end up with the "/"?  Provided that there's a page that expects single parameter (here: "content")... public class HelpPage extends WebPage { public HelpPage(PageParameters p) { super(p); add(new DynamicContentPanel("contentPanel", new Model<String>(p.getString("content")))); } }  ...and it is mounted in the Application#init() mount(new BookmarkablePageRequestTargetUrlCodingStrategy("help", HelpPage.class, null));  ...and further referred to somewhere else as: add(new BookmarkablePageLink("helpPage", HelpPage.class, new PageParameters("content=a")));  the url in the generated markup is in the following form: http://localhost:8080/dummy-web/help/content/a/;jsessionid=11624C6125F8DF4867E3218676D79A29  While IMHO it should read: http://localhost:8080/dummy-web/help/content/a;jsessionid=11624C6125F8DF4867E3218676D79A29  It looks even more awkward when there are more parameters and part of them is encoded as a query string: http://localhost:8080/dummy-web/help/content/a/?param2=value2/;jsessionid=11624C6125F8DF4867E3218676D79A29  The page parameter for both cases is resolved correctly by the HelpPage's constructor, so it seems that even though there's an extra "/" at the end of the url it gets omitted. Then why bother generating it?  I stumbled upon an issue https://issues.apache.org/jira/browse/WICKET-765. Apart from the compatibility with wicket 1.2 I see no rationale for trailing "/". Looking at implementations of IRequestTargetUrlCodingStrategy I come to the conclusion the the "append("/")" is being overused and redundant especially when it is preceded by the following code which makes sure that the "/" is in place before adding another parameter.$$patch1-wicket-9da430fb_Developer_PatchNaturalnessYe$$Added missing Override annotation. add iso3939. add incorrect error message. wicket - 917 - update the IndexedHybridUrlCodingStrategy. Removed unneeded logger. Remove hardcoded class name in PackageRequestTargetUrlCodingStrategy. Removing unused ( empty ) parameter from request URL. Remove unnecessary remove. "fix for Eclipse "" encodePageMapName "" method in PackageRequestTargetUrlCodingStrategy. Adding toString ( ) method in PackageRequestTargetUrlCodingStrategy. Don ' t append the last url component if it already has a closing url. Remove trailling comma. add urlEncodePathComponent ( pageMap ). Remove double forward slash in IndexedParamUrlCodingStrategy. Allow null values for URL parameters. Remove url fragment for request target coding strategy. remove old method. $$1
wicket-b4e9d426$$WicketSessionFilter and HttpSessionStore use different attribute name for Wicket Session$$from this topic  http://apache-wicket.1842946.n4.nabble.com/WicketSessionFilter-and-ignorePaths-in-WicketFilter-td3570291.html Please, look at the second post (the ignorePaths param is not linked with this issue as the title suggests).  How to reproduce with the quickstart: 1. open localhost:8080 - a wicket test page is displayed. 2. open localhost:8080/external - this is the external servlet that tries to access the wicket session. An exception is thrown.$$patch1-wicket-b4e9d426_Developer_PatchNaturalnessYe$$fixed test data. $$1
wicket-afc7034d$$support custom response headers in AbstractResource.ResourceResponse$$I'm converting an application to Wicket 1.5 and I see some problems with resources.  There is a case I need to add headers (not present in ResourceResponse properties) and it looks ugly.  This is what I need to do:      @Override     protected void configureCache(ResourceResponse data, Attributes attributes)     {         super.configureCache(data, attributes);         ((WebResponse) attributes.getResponse()).setHeader("Accept-Ranges", "bytes");     }  It's a hack to use configureCache here, but this can't be added to setResponseHeaders, which seams a better apparent method name for it.$$patch1-wicket-afc7034d_Developer_PatchNaturalnessYe$$Allow null values for http headers. $$1
wicket-dd1df04b$$onError call order doesn't match onSubmit$$onError in Forms and Buttons should be called in the same order as onSubmit (i.e. post-order).$$patch1-wicket-dd1df04b_Developer_PatchNaturalnessYe$$cleaned up error reporting. added error filter. added onSubmitBefore / AfterForm methods. Removed 1226 comment. $$1
wicket-d35d2d85$$Component's onAfterRender() is called so many times as it is depth in the component tree + 1$$org.apache.wicket.Component.afterRender() calls org.apache.wicket.Component.onAfterRenderChildren() which for MarkupContainers calls afterRender() for its children.  So for code like:   WebMarkupContainer comp1 = new WebMarkupContainer("c1");         add(comp1);                  WebMarkupContainer comp2 = new WebMarkupContainer("c2");         comp1.add(comp2);                  WebMarkupContainer comp3 = new WebMarkupContainer("c3") {              @Override             protected void onAfterRender() {                 super.onAfterRender();                 System.err.println("called");             }                      };         comp2.add(comp3);  you'll see "called" printed 4 times in a single request.  Additionally I think onAfterRenderChildren() should be called before onAfterRender() in Component.afterRender(). The flow should be first-in last-out: onBeforeRender > onBeforeRenderChildren > onAfterRenderChildren > onAfterRender,$$patch1-wicket-d35d2d85_Developer_PatchNaturalnessYe$$add onAfterRenderChildren ( ) and onAfterRender ( ). remove duplicated line. Allow null to be set to markup id .. disable rendering on parse. set RENDERING_FLAG to false for components that are not rendered. $$1
wicket-8ee095bf$$StatelessForm submitted to the wrong page$$I made a small application to reproduce the problem. You can download it from http://aditsu.net/wickettest.zip , I'll try to attach it too. Dependencies: jetty 6, wicket 1.4-m3, slf4j, log4j Steps to reproduce: 1. Run the test.Start class 2. Open http://localhost:8080 in a browser 3. Open http://localhost:8080/page2 in a new tab 4. Go to the first tab and click submit  Result:  WicketRuntimeException: unable to find component with path form on stateless page [Page class = test.Page2, id = 0, version = 0]  It looks like the 2 pages are created with the same id in 2 different pagemaps, but when I submit the form, it goes to the second pagemap and finds the second page (with no form on it).$$patch1-wicket-8ee095bf_Developer_PatchNaturalnessYe$$Add missing constructor .. $$1
wicket-a79ed51e$$WebPageRenderer should honor RedirectPolicy.ALWAYS_REDIRECT more consistently$$In WebPageRenderer shouldPreserveClientUrl() currently has precedence over RedirectPolicy.ALWAYS_REDIRECT.  This can lead to confusion or unexpected behavior when RedirectPolicy.ALWAYS_REDIRECT is explicitely set, but for some reason shouldPreserveClientUrl() returns true and thus no redirect is performed due to the logic in WebPageRenderer.  A fix for this particular problem could be implemented in  WebPageRenderer as of Wicket 6.12.0 by changing line 211 to:                  || (shouldPreserveClientUrl && getRedirectPolicy() != RedirectPolicy.ALWAYS_REDIRECT)) //   Note that this problem is slightly related to WICKET-5484. Both fixes combined the line could look like this:                  || (shouldPreserveClientUrl && !isAjax && getRedirectPolicy() != RedirectPolicy.ALWAYS_REDIRECT)) //$$patch1-wicket-a79ed51e_Developer_PatchNaturalnessYe$$Preserve client URL on redirect to another page when the same target url already exists .. $$1
wicket-7e7ab76c$$TextField ingnores convertEmptyInputStringToNull = true property when the String type is set$$I posted this patch on WICKET-3269, but the discussion on this ticket is about an improvement request, not a bug. I opened this one for the bug.$$patch1-wicket-7e7ab76c_Developer_PatchNaturalnessYe$$Removed unused imports. Added missing documentation. Fixed null / empty string handling in AbstractTextComponent. remove debug code. revert 8e704f627c7b03427ab1b71bb. $$1
wicket-ea4a3f8a$$PageParameters construced with keyValuePairs does not handle array values$$The PageParameters constructor that takes a "keyValuePairs" argument does not convert repeated keys into an array of values.  For example:  {code} // specify three comma delimited values for the "a" parameters PageParameters parameters = new PageParameters("a=1,a=2,a=3"); String[] a = parameters.getStringArray("a"); assertEquals(3, a.length); // fails because a.length == 1 {code}  Issue first described on the user's list: http://www.nabble.com/PageParameters-with-String-array-question-to22540294.html$$patch1-wicket-ea4a3f8a_Developer_PatchNaturalnessYe$$revert 149 change. remove unused imports. made ValueMap constructor parameter nullable. ValueMap # add ( String , Object ). add putAll with Map < ? extends String , ? extends Object >. wicket + use leftovers in ValueMap. Remove unused code. improve value map. add missing try catch block. add missing throwing. improve value map. made final. add final modifier to getAsDuration ( String ). made final. add final modifier to getAsTime ( ). made getTimeProvider ( ) allow override of default value. allow override of default value for enum implementation .. wicket added missing value for parameter ' pos '. $$1
wicket-3feb0e3a$$MarkupContainer.removeAll() does not detach models recursively$$ML thread at: http://markmail.org/message/ybdfd2ts4i3j2b72$$patch1-wicket-3feb0e3a_Developer_PatchNaturalnessYe$$Fix MarkupContainer ' s detach method. $$1
wicket-beb9086d$$setResponsePage in AjaxLink goes always to localhost:8080 instead to the right host and port$$setResponsePage in an AjaxLink in Wicket 1.4 redirects with a relative path to the response page. Wicket 1.5 takes the absolute path "localhost:8080/path to the response page" even when the host and port are different. (e.g. with Apache2 a virtual host is created with server name www.mycompany.com, setResponce wil go to "localhost:8080/path to page" instead of  "www.mycompany.com/path to page")$$patch1-wicket-beb9086d_Developer_PatchNaturalnessYe$$Remove getAbsoluteURL from ServletWebResponse. $$1
wicket-e6582c52$$URL with a previous page version ignores requested page based on mount path$$See discussion on http://mail-archives.apache.org/mod_mbox/wicket-users/201203.mbox/browser  With 2 mounts /page1 and /page2 to stateful pages and the following sequence: 1-With a new session, user visits "/page1". Displayed URL is "/page1?0" 2-Whatever, without expiring session 3-User requests URL "/page2?0" because it was bookmarked, received via email, etc. 4-Rendered page is "/page1?0" which was stored in the page map. The actual URL displayed is "/wicket/bookmarkable/com.mycompany.Page1?0"  If a requested page id exists but does not match the page class mounted on the actual requested url, Wicket should not use the old page version. This is very counter-intuitive for users having bookmarks to stateful pages or exchanging links.$$patch1-wicket-e6582c52_Developer_PatchNaturalnessYe$$Fixed a compile error in PageProvider . getStoredPage ( ). $$1
wicket-9e6efa61$$The DateTimeField.onBeforeRender() method does not format the fields correctly.$$The current implementation relies on the org.joda.time.MutableDateTime instance to format the date, hours, amOrPm, and minutes fields. Unfortunately, the MutableDateTime constructor is not provided with the client's TimeZone value (assuming it is set). As a result, the joda library uses the JVM's default timezone. If the defaul timezone differs from the client's timezone, the formatted fields may turn out to be incorrect.$$patch1-wicket-9e6efa61_Developer_PatchNaturalnessYe$$Fixed client timezone in DateTimeField .. $$1
wicket-7da4ad17$$EnumChoiceRenderer misbehaves with anonymous enum classes$$Please find attached testcase reproducing the problem.  Proper fix is to do return object.getDeclaringClass().getSimpleName() + "." + object.name()  instead of return object.getClass().getSimpleName() + "." + object.name()  in EnumChoiceRenderer.resourceKey$$patch1-wicket-7da4ad17_Developer_PatchNaturalnessYe$$Change EnumChoiceRenderer resource key from < T > to < T > .. $$1
wicket-8fbdc68f$$Component markup caching inconsistencies$$In WICKET-3891 we found that Component#markup field is not being reset between requests. The problem is that this field is transient and it is null-ified only when the page is read from the second level page cache (see https://cwiki.apache.org/confluence/x/qIaoAQ). If the page instance is read from first level cache (http session) then its non-serialized version is used and the markup field value is still non-null.  In WICKET-3891 this looked like a minor issue with the markup caching in development mode but actually this problem is valid even in production mode. See the attached application. When the panel's variation is changed every MarkupContainer inside still uses its old markup.$$patch1-wicket-8fbdc68f_Developer_PatchNaturalnessYe$$Updating old import. add internalDetach ( ) , so that we can now notify the detach listener. Remove the cached markup on the client. Fix typo. add missing import. wicket - add inline enclosure class. Remove early setOutputMarkupId because it won ' t be used .. Fix error in inline enclosures ' getMarkup. $$1
wicket-0f8a6d75$$ResourceUtils.getLocaleFromFilename cannot handle filenames with classifiers$$When I try to get PathLocale with ResourceUtils, than get wrong values, when the files contains '.' in name.  Example: 'jquery.test.js'  PathLocale.path=jquery, PathLocale.locale = null  or jquery.test_hu.js'. PathLocale.path=jquery, PathLocale.locale = null  That's why I'd like to use  jquery.test_hu.js' as resource, the ResourceStreamLocator try to find  jquery.test_hu_hu_HU.js, jquery.test_hu_hu.js, and after  jquery.test_hu.js. Because the  ResourceStreamLocator.locate  		PathLocale data = ResourceUtils.getLocaleFromFilename(path); 		if ((data != null) && (data.locale != null)) 		{ 			path = data.path; 			locale = data.locale; 		} doesn't work in this case.  Should change the  ResourceUtils  	public static PathLocale getLocaleFromFilename(String path) { 		int pos = path.indexOf('.'); ---------------- To                 int pos = path.lastIndexOf('.');$$patch1-wicket-0f8a6d75_Developer_PatchNaturalnessYe$$wicket util should contain ISO 639 - 2. Put back lastIndexOf. Add extension to default path locale implementation. $$1
wicket-ddf7e8a2$$Links with multiple parameters are wrongly generated$$If you have a PageParameters, with multiple params, then the resulting link will be something like this /url?id=123&amp;sid=456, so for some reason the & sign is encoded to &amp; which will result in the following parameters on the receiving page: id=[123], amp;sid=[456] See the attached quickstart for example.$$patch1-wicket-ddf7e8a2_Developer_PatchNaturalnessYe$$Removed unused imports. Don ' t replace HTML attributes with HTML attributes. Removed url replace replace with ""&"" in HTML tag. wicket - utils - replace replace with /. $$1
wicket-66bfc885$$Handling of semicolons in form action URLs$$What I expect to happen when there is no semicolon support in Wicket is that a URL in a form like below stays intact and will not be cut off at the position of the first semicolon:  <form action="http://localhost:8080/dor/abc_1234:56;023:456_def_78;90.html" method="post"><input type="submit" value="Submit" /></form>  In my application the part abc_1234:56;023:456_def_78;90.html is "named1" in the mapping below:  mount(new MountedMapper("dor/#{named1}", TestPage.class, new MyPageParametersEncoder()));  and parsed in MyPageParametersEncoder.  The officially intended use of semicolons in URLs seems to be specified in "RFC 1808 - Relative Uniform Resource Locators, 2.4.5." (http://www.faqs.org/rfcs/rfc1808.html). But thats not what Im looking for.  If I had not some pages running on this syntax, I could easily swap the semicolon with another symbol. Nevertheless and if Im correctly informed, I think those URLs should not be cut off.  (Quotation from the mailing list)  The quickstart can be tested with the following URLs:  http://localhost:8080/dor/abc_1234:56;023:456_def_78;90.html http://localhost:8080/dor/abc_1234:56%3B023:456_def_78%3B90.html http://localhost:8080/dor/?abc=1234:56%3B023:456&def=78%3B90  The crucial part is the action attribute in the form in the pages source code, which contains i.e. "./abc_1234:56?-1.IFormSubmitListener-form".$$patch1-wicket-66bfc885_Developer_PatchNaturalnessYe$$Missing import. remove null check. $$1
wicket-53bcb78d$$Multipart Form and AjaxSubmitLink will result in invalid redirect after user session expires$$Hi,  I have hit an issue similar to this one:  https://issues.apache.org/jira/browse/WICKET-3141  I do not receive any errors from Wicket itself to help clarify, so I will try to explain using an example.  The example below with which I could recreate the issue uses the default SignInPanel (in my LoginPage.clas) and AuthenticatedWebSession to authenticate the user and store the session:  	protected Class<? extends WebPage> getSignInPageClass() 	{ 		return LoginPage.class; 	}  If the authentiation is succesfull then the user is redirect back to the test page:  			protected void onSignInSucceeded() { 				setResponsePage(Test.class); 			}  So far so good. However if I use a form with setMultiPart(true) in combination with an AjaxSubmitLink as shown in the following piece of code:  import org.apache.wicket.ajax.AjaxRequestTarget; import org.apache.wicket.ajax.markup.html.form.AjaxSubmitLink; import org.apache.wicket.authroles.authorization.strategies.role.annotations.AuthorizeInstantiation; import org.apache.wicket.markup.html.WebPage; import org.apache.wicket.markup.html.form.Form;  @AuthorizeInstantiation("USER") public class Test extends WebPage {  	public Test() 	{ 		super(); 		 		final Form testForm =  				new Form("testForm"); 	 		testForm.setMultiPart(true); 		 		testForm.add(new AjaxSubmitLink("testButton", testForm) { 			 			@Override 			protected void onSubmit(AjaxRequestTarget target, Form form) { 				super.onSubmit(); 			}; 			 			@Override 			protected void onError(AjaxRequestTarget target, Form form) { 				 			}; 		}); 				 		add(testForm); 	} }  And have selected the option "Remember credentials" in the SignInPanel, clicking on the testButton AFTER the session has expired will result in:  http://localhost:8080/PaladinWicket/?3-1.IBehaviorListener.0-testForm-testButton&wicket-ajax=true&wicket-ajax-baseurl=.  which displays this in the browser:  This XML file does not appear to have any style information associated with it. The document tree is shown below. <ajax-response> <redirect> <![CDATA[ .?1 ]]> </redirect> </ajax-response>$$patch1-wicket-53bcb78d_Developer_PatchNaturalnessYe$$removed unused import. add missing import. wicket - mistake remove query param from data . postParameters. $$1
wicket-97514205$$In wicket 1.5 urlFor returns incorrect string for package mounted pages$$Attached two quickstart projects for 1.4 and 1.5.  Then access http://localhost:8080/app/Page1 and see 1.5 returns wrong address.$$patch1-wicket-97514205_Developer_PatchNaturalnessYe$$Remove unneeded import. add missing super call to mountPackage ( String , Class ). Removed package separator from package url in case we had source class that was not in the package. $$1
wicket-f5f802c5$$NumberTextField doesn't accept values <=0 for Double and Float$$The org.apache.wicket.util.lang.Numbers class defines the method : public static Number getMinValue(Class<? extends Number> numberType)  This method return the MatchingNumberTypeClass.MIN_VALUE. But for Double.MIN_VALUE and Float.MIN_VALUE return the smallest positive number, not the smallest negative number like for the other number classes.  One side effect is that by default you can't enter a negative value, or a 0 in a NumberTextField<Double> or NumberTextField<Float>.$$patch1-wicket-f5f802c5_Developer_PatchNaturalnessYe$$Fixed formatting. Fixed formatting issue. Fixed no - min default value for Double. $$1
wicket-eccb3b11$$JavaScriptReference escapes given URL$$while trying to integrate gmaps3 in our webapp i had issues with the wicketstuff-gmap3 stuff ( - we need a client-id for our request) ...  so i have: {noformat} public static final String GMAP_API_URL = "%s://maps.google.com/maps/api/js?v=3&sensor=%s&client-id=%s";  response.render(JavaScriptHeaderItem.forUrl(String.format(GMAP_API_URL, schema, sensor, clientid))); {noformat}  the rendered result of this is: {noformat} <script type="text/javascript" src="http://maps.google.com/maps/api/js?v=3&amp;sensor=false&amp;client-id=...."></script> {noformat}  so the requestparameters are encoded  which is happening in the JavaScriptUtils Helper: {noformat} public static void writeJavaScriptUrl(final Response response, final CharSequence url, final String id, boolean defer, String charset) {         response.write("<script type=\"text/javascript\" ");         if (id != null)         {             response.write("id=\"" + Strings.escapeMarkup(id) + "\" ");         }         if (defer)         {             response.write("defer=\"defer\" ");         }         if (charset != null)         {             response.write("charset=\"" + Strings.escapeMarkup(charset) + "\" ");         }         response.write("src=\"");         response.write(Strings.escapeMarkup(url));         response.write("\"></script>");         response.write("\n"); } {noformat} but ... is this right to escape the url?  when i open the above mentioned script, google tells me i have no parameter "sensor" ... which i can understand as ther is only a parameter amp ...$$patch1-wicket-eccb3b11_Developer_PatchNaturalnessYe$$encode HTML entities properly. encode HTML id in JSUtils. $$1
wicket-e2d88568$$AjaxPreprocessingCallDecorator calls the delegate decorator before itself (same behavior as AjaxPostprocessingCallDecorator)$$AjaxPreprocessingCallDecorator calls the delegate decorator before itself (same behavior as AjaxPostprocessingCallDecorator), when it should call itself before the delegate.$$patch1-wicket-e2d88568_Developer_PatchNaturalnessYe$$Fix AJAX - 661. Fix whitespace. Fix AJAX error in AJAX request inner closure. $$1
wicket-2b6da516$$SimpleTree example not working with CryptoMapper$$Adding the following lines to WicketExampleApplication.java causes the SimpleTree example to break. There are no expand icons anymore and there is no way to expand the tree. Even the expand link will not work.  Add to WicketExampleApplication.java   IRequestMapper cryptoMapper = new CryptoMapper(getRootRequestMapper(), this); setRootRequestMapper(cryptoMapper);   Comment out in WicketExampleApplication.java   //getSecuritySettings().setCryptFactory(new ClassCryptFactory(NoCrypt.class, ISecuritySettings.DEFAULT_ENCRYPTION_KEY));  Without the CryptoMapper everythings works fine.$$patch1-wicket-2b6da516_Developer_PatchNaturalnessYe$$Remove debug code. Fixed nit on length of encrypted URL , closes # 1796. remove debug code. add new segments from browser. $$1
wicket-145da021$$SecuritySettings.setEnforceMounts() does not work when the mounted mapper is not in the root compound mapper$$BookmarkableMapper.isPageMounted() assumes that all mounted mappers are in Application.getRootRequestMapperAsCompound(). Sometimes the mappers make a tree structure, with multiple compounds existing, sometimes separated by wrappers, like HttpsMapper and CryptoMapper.  Because of this, BookmarkableMapper fails to realise that a page is mounted and so does not enforce mounting.$$patch1-wicket-145da021_Developer_PatchNaturalnessYe$$add missing import. fix # 5094 - only allow mount if page is mounted. wicket - api / org / wicket / request / mapper / AbstractBookmarkableMapper. $$1
wicket-15477252$$ajax not working due to bugs in resource handling$$A couple of bugs were found that were preventing .js resources to be returned to the client correctly. One bug was returning the jar file size as the content length of the resource if it is in a jar file. The other was copying past a source buffer into the response.  After fixing these bugs, the ajax functions in the trunk seems to be working.  A patch is provided. Test cases included.$$patch1-wicket-15477252_Developer_PatchNaturalnessYe$$Fix bug in AbstractResource . write ( ). Fix NPE. Log if / else with long form workflow. $$1
wicket-e1953357$$Confusion between a form component's wicket:id and a PageParameter in Wicket 1.5.x$$A Form has a strange behavior when a component has the same wicket:id than a page parameter.  To create a Bookmarkable link after a form is submited, setResponsePage is called, and a PageParameter object is given as a parameter :  			PageParameters params = new PageParameters(); 			params.add("searchString", searchField.getValue()); 			setResponsePage(SomePage.class, params);  In Wicket 1.5, if "searchString" is also a form-component's wicket:id, the form will only be submitted once :  searchField.getValue() will always return the first value entered by the user.   Here's an example :   public class SearchPanel extends Panel {  	public SearchPanel(String id) { 		super(id); 		add(new SearchForm("searchForm")); 	}  	private class SearchForm extends Form<String> {  		private static final long serialVersionUID = 1L; 		private TextField<String> searchField;  		public SearchForm(String id) { 			super(id); 			searchField = new TextField<String>("searchString", new Model<String>("")); 			add(searchField); 		}  		@Override 		public void onSubmit() { 			PageParameters params = new PageParameters(); 			params.add("searchString", searchField.getValue()); 			setResponsePage(ListContactsPage.class, params); 		} 	} }   I tested the same application with Wicket 1.4.17 and it was fine. I only had this problem in Wicket 1.5.2 and 1.5.3.$$patch1-wicket-e1953357_Developer_PatchNaturalnessYe$$include page . getPageParameters ( ) in url for embedded wicket request. $$1
wicket-7a162f77$$org.apache.wicket.validation.ValidatorAdapter class causes problem with validator properties to be loaded$$PROBLEM: <e1nPL> hi I am having such problem:  <e1nPL> I have implemented validator by implementing IValidator<T> interface <e1nPL> and I have impelemnted the same validator by extending AbstractValidator<T> class  CODE:     ===================== VALIDATOR EXTENDED FROM AbstractValidator =====================     package com.mycompany;           import java.util.regex.Pattern;     import org.apache.wicket.IClusterable;     import org.apache.wicket.util.lang.Classes;     import org.apache.wicket.validation.IValidatable;     import org.apache.wicket.validation.IValidator;     import org.apache.wicket.validation.ValidationError;     import org.apache.wicket.validation.validator.AbstractValidator;           /**      *      * @author e1n      */     public class PasswordPolicyValidator<T> extends AbstractValidator<T> {               private static final Pattern UPPER = Pattern.compile("[A-Z]");         private static final Pattern LOWER = Pattern.compile("[a-z]");         private static final Pattern NUMBER = Pattern.compile("[0-9]");                 @Override         public void onValidate(IValidatable<T> validatable) {             final String password = (String)validatable.getValue();                         if (!NUMBER.matcher(password).find()) {                 error(validatable, "no-digit");             }             if (!LOWER.matcher(password).find()) {                 error(validatable, "no-lower");             }             if (!UPPER.matcher(password).find()) {                 error(validatable, "no-upper");             }               }                 @Override         public void error(IValidatable<T> validatable, String errorKey) {             ValidationError err = new ValidationError();             err.addMessageKey(Classes.simpleName(getClass()) + "." + errorKey);             validatable.error(err);         }             }                 =============== VALIDATOR directly implementing IValidator interfce ====================     package com.mycompany;           import java.util.regex.Pattern;     import org.apache.wicket.IClusterable;     import org.apache.wicket.util.lang.Classes;     import org.apache.wicket.validation.IValidatable;     import org.apache.wicket.validation.IValidator;     import org.apache.wicket.validation.ValidationError;     import org.apache.wicket.validation.validator.AbstractValidator;           /**      *      * @author e1n      */     public class PasswordPolicyValidator<T> implements IValidator<T> {               private static final Pattern UPPER = Pattern.compile("[A-Z]");         private static final Pattern LOWER = Pattern.compile("[a-z]");         private static final Pattern NUMBER = Pattern.compile("[0-9]");               public void validate(IValidatable<T> validatable) {             final String password = (String)validatable.getValue();                         if (!NUMBER.matcher(password).find()) {                 error(validatable, "no-digit");             }             if (!LOWER.matcher(password).find()) {                 error(validatable, "no-lower");             }             if (!UPPER.matcher(password).find()) {                 error(validatable, "no-upper");             }               }                 public void error(IValidatable<T> validatable, String errorKey) {             ValidationError err = new ValidationError();             err.addMessageKey(Classes.simpleName(getClass()) + "." + errorKey);             validatable.error(err);         }             }    <e1nPL> I also have properties file which is named after validator class <e1nPL> and placed in the same package <e1nPL> my problem is that when i use to validate my form field validator which implements IValidator interface it is not capable of loading error messages from properties file <e1nPL> but when i am using validator which is extending AbstractValidator class <e1nPL> properties file with error msgs gets loaded POSSIBLE FIX: <e1nPL> ok i have found class which is responsible for my problem and it is probably a bug <e1nPL> org.apache.wicket.validation.ValidatorAdapter <e1nPL> which wraps classes that directly implements IValidator interface <e1nPL> then when resources are loaded, and properties file are searched in class path etc., loaders search in wrong path that is build against org.apache.wicket.validation.ValidatorAdapter  PLACE WHER FIX SHOULD OCCOUR org.apache.wicket.resource.loader.ValidatorStringResourceLoader::loadStringResource(java.lang.Class,java.lang.String,java.util.Locale,java.lang.String,java.lang.String)$$patch1-wicket-7a162f77_Developer_PatchNaturalnessYe$$add missing import. Allow a validator to override its own scope .. Allow a validator to be scope by default. $$1
wicket-ad849602$$Redirect to HTTPS is using wrong port 80 if HttpsConfig with default ports 80/443 is used$$HttpsMapper#mapHandler() doesn't set the Url's port, if the desired protocol uses the standard port.  This leads to UrlRenderer choosing to the request's port as fallback (which is 80 before switching to https).$$patch1-wicket-ad849602_Developer_PatchNaturalnessYe$$Remove unnecessary check for non - standard port. $$1
wicket-c86b972a$$Set an request parameter on Wicket tester do not add it in the request URL$$When submitting an form, the parameters set in request do not get appended to the URL query string. Initial impression is that UrlRender should append query parameters in the base URL on relatives URL.$$patch1-wicket-c86b972a_Developer_PatchNaturalnessYe$$Fixed issue with query parameters being empty string ( or sometimes empty string ). $$1
wicket-9dab1bb5$$bug in Duration.toString(Locale locale)$$Duration.toString(Locale locale) misses milliseconds in line 529$$patch1-wicket-9dab1bb5_Developer_PatchNaturalnessYe$$Fix TimeUnit format. $$1
wicket-5e1bf8d8$$Do not use the parsed PageParameters when re-creating an expired page$$WICKET-4014 and WICKET-4290 provided functionality to re-create an expired page if there is a mount path in the current request's url. There is a minor problem with that because the page parameters are passed to the freshly created page. I.e. parameters for a callback behavior are now set as page construction parameters. Since the execution of the behavior is ignored for the recreated page these parameters should be ignored too.$$patch1-wicket-5e1bf8d8_Developer_PatchNaturalnessYe$$ignore pageParameters for stateful pages. $$1
wicket-d450acb0$$Errors reported from Form#onValidateModelObjects() are ignored$$None$$patch1-wicket-d450acb0_Developer_PatchNaturalnessYe$$Avoid NPE in Form when error is found in submitted form .. $$1
wicket-5837817c$$OnChangeAjaxBehavior should listen for both 'inputchange' and 'change' events for TextField and TextArea$$WICKET-5603 introduced a regression that a TextField using OnChangeAjaxBehavior doesn't work anymore when used as date picker, or Select2. The problem is that usually extensions like DatePicker and Select2 will fire 'change' event when they update the text input.  OnChangeAjaxBehavior should use both 'inputchange" and "change" events for TextField and TextArea components.$$patch1-wicket-5837817c_Developer_PatchNaturalnessYe$$Added ChangeAjaxBehavior event name and code for ' inputchange '. wicket - form - > ' change ' event name. $$1
wicket-5ad32df9$$Component's markup cannot be found in Ajax requests if the parent is transparent$$When TransparentWebMarkupContainer is used an inner markup container cannot find its markup on Ajax updates. The problem seems to be caused by the fact that ComponentResolvers#resolve() is not executed and since there is transparent container involved Markup.find(String) cannot find the markup for non-transparent markup containers. I'll commit a disabled test case that shows the problem.$$patch1-wicket-5ad32df9_Developer_PatchNaturalnessYe$$Remove unused imports. Allow null to be empty string ( or empty string ). add missing import. using getMarkup ( ) on MarkupContainer is not a async API. add missing import. Allow null to be included in a component. Added Error message for failed builds. Allow null to be added to aMarkupContainer. $$1
wicket-96330447$$DebugBar throws an java.lang.ExceptionInInitializerError when Tomcat is restarted$$I have just added the DebugBar to our base page, and since then when Tomcat is restarted and session would be reloaded by this it throws this exception:  1    ERROR org.apache.catalina.session.ManagerBase  - Exception loading sessions from persistent storage java.lang.ExceptionInInitializerError 	at sun.misc.Unsafe.ensureClassInitialized(Native Method) 	at sun.reflect.UnsafeFieldAccessorFactory.newFieldAccessor(UnsafeFieldAccessorFactory.java:25) 	at sun.reflect.ReflectionFactory.newFieldAccessor(ReflectionFactory.java:122) 	at java.lang.reflect.Field.acquireFieldAccessor(Field.java:918) 	at java.lang.reflect.Field.getFieldAccessor(Field.java:899) 	at java.lang.reflect.Field.getLong(Field.java:528) 	at java.io.ObjectStreamClass.getDeclaredSUID(ObjectStreamClass.java:1614) 	at java.io.ObjectStreamClass.access 700(ObjectStreamClass.java:52) 	at java.io.ObjectStreamClass 2.run(ObjectStreamClass.java:425) 	at java.security.AccessController.doPrivileged(Native Method) 	at java.io.ObjectStreamClass.<init>(ObjectStreamClass.java:413) 	at java.io.ObjectStreamClass.lookup(ObjectStreamClass.java:310) 	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:547) 	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1583) 	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1496) 	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1583) 	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1496) 	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1732) 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329) 	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1667) 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1323) 	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1947) 	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1871) 	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1753) 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329) 	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1947) 	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:480) 	at org.apache.wicket.Component.readObject(Component.java:4469) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) 	at java.lang.reflect.Method.invoke(Method.java:597) 	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:974) 	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1849) 	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1753) 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329) 	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1667) 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1323) 	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1947) 	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1871) 	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1753) 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329) 	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:351) 	at java.util.concurrent.CopyOnWriteArrayList.readObject(CopyOnWriteArrayList.java:845) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) 	at java.lang.reflect.Method.invoke(Method.java:597) 	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:974) 	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1849) 	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1753) 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329) 	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1947) 	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1871) 	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1753) 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329) 	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1947) 	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1871) 	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1753) 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329) 	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1947) 	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1871) 	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1753) 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329) 	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1667) 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1323) 	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1947) 	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:480) 	at org.apache.wicket.Page.readPageObject(Page.java:1349) 	at org.apache.wicket.Component.readObject(Component.java:4465) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) 	at java.lang.reflect.Method.invoke(Method.java:597) 	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:974) 	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1849) 	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1753) 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329) 	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:351) 	at org.apache.wicket.protocol.http.SecondLevelCacheSessionStore SecondLevelCachePageMap.readObject(SecondLevelCacheSessionStore.java:412) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) 	at java.lang.reflect.Method.invoke(Method.java:597) 	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:974) 	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1849) 	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1753) 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329) 	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:351) 	at org.apache.catalina.session.StandardSession.readObject(StandardSession.java:1407) 	at org.apache.catalina.session.StandardSession.readObjectData(StandardSession.java:931) 	at org.apache.catalina.session.StandardManager.doLoad(StandardManager.java:394) 	at org.apache.catalina.session.StandardManager.load(StandardManager.java:321) 	at org.apache.catalina.session.StandardManager.start(StandardManager.java:637) 	at org.apache.catalina.core.ContainerBase.setManager(ContainerBase.java:432) 	at org.apache.catalina.core.StandardContext.start(StandardContext.java:4160) 	at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1014) 	at org.apache.catalina.core.StandardHost.start(StandardHost.java:736) 	at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1014) 	at org.apache.catalina.core.StandardEngine.start(StandardEngine.java:443) 	at org.apache.catalina.core.StandardService.start(StandardService.java:448) 	at org.apache.catalina.core.StandardServer.start(StandardServer.java:700) 	at org.apache.catalina.startup.Catalina.start(Catalina.java:552) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) 	at java.lang.reflect.Method.invoke(Method.java:597) 	at org.apache.catalina.startup.Bootstrap.start(Bootstrap.java:295) 	at org.apache.catalina.startup.Bootstrap.main(Bootstrap.java:433) Caused by: org.apache.wicket.WicketRuntimeException: There is no application attached to current thread main 	at org.apache.wicket.Application.get(Application.java:178) 	at org.apache.wicket.devutils.debugbar.DebugBar.getContributors(DebugBar.java:146) 	at org.apache.wicket.devutils.debugbar.DebugBar.registerContributor(DebugBar.java:140) 	at org.apache.wicket.devutils.debugbar.DebugBar.registerStandardContributors(DebugBar.java:152) 	at org.apache.wicket.devutils.debugbar.DebugBar.<clinit>(DebugBar.java:65) 	... 109 more$$patch1-wicket-96330447_Developer_PatchNaturalnessYe$$Fix typo in debug bar. Added a static modifier to debug bar contributors. Added missing toString method. $$1
wicket-4ee5ad1f$$Stack overflow when render malformed html.$$Stack overflow when render malformed html.  Please, note that </HEAD> element is inserted after </body>.  HTML: <html> <head> <body> Malformed HTML </body> </head> </html>  Java: package com.mycompany;  import org.apache.wicket.markup.html.WebPage; public class Test1 extends WebPage { 	private static final long serialVersionUID = -4267477971499123852L;  }   Thanks.$$patch1-wicket-4ee5ad1f_Developer_PatchNaturalnessYe$$Removed unused imports. Added ignoreTheRest flag to HtmlHeaderSectionHandler. wicket - 917 - updated parser course - I am using the component model in the < head. Added wicket - note - that < body > is not valid in < HEAD > tag. $$1
wicket-0e70ce39$$"isPrimary" check is not applied to beans in parent contexts$$see this comment in WICKET-2771: https://issues.apache.org/jira/browse/WICKET-2771?focusedCommentId=12872246&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12872246$$patch1-wicket-0e70ce39_Developer_PatchNaturalnessYe$$Added missing import. fixed a similar issue with AnnotProxyFieldValueFactory. getBeanDefinition ( AbstractApplicationContext ) - > getBeanDefinition ( ). Added support for field type in AnnotProxyFieldValueFactory. Added missing import. Fix possible NPE in SpringBeanLocator. Fix broken build. Added support for bean names with @ deprecated javadoc tags. $$1
wicket-614e3b50$$improve wicket's handling of empty / null page parameters$$- DefaultPageFactory#newPage() should be sure to not pass 'null' to a page constructor with page parameters$$patch1-wicket-614e3b50_Developer_PatchNaturalnessYe$$wicket - request . ajax . isEmpty ( ) disable ajax optimization. $$1
wicket-6cceff44$$DefaultPropertyResolver does not respect JavaBean conventions$$The property name code should handle the isPropertyName pattern  if(getterName.startsWith("get")) { 	name = getterName.substring(3, 4).toLowerCase() + getterName.substring(4); } else { 	name = getterName.substring(2, 3).toLowerCase() + getterName.substring(3); }  Workaround: providing my own property resolver.$$patch1-wicket-6cceff44_Developer_PatchNaturalnessYe$$Added missing import. Allow property getters to be specified with the correct name .. $$1
wicket-925cae5c$$UrlRenderer renders invalid relative URLs if first segment contains colon$$Seen on Wicket 1.5.3.  If a relative url of a link starts with a path segment containing a colon then the whole uri will be regarded as absolute uri, so typically browsers will complain that there is no handle for the protocol foo in foo:bar/dee/per.  See also the attached quickstart. The start page contains three links, one relative with colon, one absolute and one to a mounted page without colon for comparison. The application also has a static switch to add an extended urlrenderer, prepending "./" if needed. This fix is merely a quick shot and there might be better alternatives.$$patch1-wicket-925cae5c_Developer_PatchNaturalnessYe$$Remove wicket - 4260 from the url because it is not legal to send a redirect. $$1
wicket-0d4d1df7$$Session should be bound when adding messages to it$$When using the Sessions info(), error() and success() methods, and the session is temporary, the messages can be dropped silently. This happens when on stateless pages and a redirect happens in the same request during which a session message is added.  The fix for this could be to make sure the session is bound and call Session#bind() automatically when a session message is added.  Email thread: http://wicket-users.markmail.org/thread/zd72s4gwnlp5d7ch$$patch1-wicket-0d4d1df7_Developer_PatchNaturalnessYe$$Remove unused imports. add code to check for session feedback messages. $$1
wicket-9a6a06be$$NullPointerException in org.apache.wicket.markup.html.form.ValidationErrorFeedback$$org.apache.wicket.markup.html.form.ValidationErrorFeedback throws a NPE in the following situation:  - Form with a TextField<Integer> that has a RangeValidator - value outside range is entered - form is submitted  See attached quickstart.$$patch1-wicket-9a6a06be_Developer_PatchNaturalnessYe$$Adding default message if one is not set .. $$1
wicket-8e518d88$$CDI integration fails in Glassfish 4.0 with WELD-000070$$When CDI is configured in the Application and a page has a non-static inner class the page throws exception, regardless of whether there are any injected fields.  Caused by: org.jboss.weld.exceptions.DefinitionException: WELD-000070 Simple bean [EnhancedAnnotatedTypeImpl] private  class com.inversebit.HomePage AForm cannot be a non-static inner class 	at org.jboss.weld.injection.producer.BasicInjectionTarget.checkType(BasicInjectionTarget.java:81) 	at org.jboss.weld.injection.producer.BasicInjectionTarget.<init>(BasicInjectionTarget.java:69) 	at org.jboss.weld.injection.producer.BeanInjectionTarget.<init>(BeanInjectionTarget.java:52) 	at org.jboss.weld.manager.InjectionTargetFactoryImpl.createInjectionTarget(InjectionTargetFactoryImpl.java:95) 	at org.jboss.weld.manager.InjectionTargetFactoryImpl.createInjectionTarget(InjectionTargetFactoryImpl.java:78) 	... 65 more$$patch1-wicket-8e518d88_Developer_PatchNaturalnessYe$$Allow non - static inner classes to be processed by the UI implementation. $$1
wicket-ccd74641$$The default exception mapper is replying cacheable exceptional responses$$The problem is that some common URLs in the application like to a page instance are responding the cached exception page rather than hitting the server for the page instance being requested. It happens because at some moment in the past a exception page were replied and cached for a request in this URL.$$patch1-wicket-ccd74641_Developer_PatchNaturalnessYe$$Remove unused imports. disable caching for exceptional responses in WebResponse. $$1
wicket-4624ab3d$$Ajax link reports weird error when session is expired$$Reproducing steps:  1. Put below simple page into a Wicket application and get it mounted:  TestPage.java:  import org.apache.wicket.ajax.AjaxRequestTarget; import org.apache.wicket.ajax.markup.html.AjaxLink; import org.apache.wicket.markup.html.WebPage;  @SuppressWarnings("serial") public class TestPage extends WebPage { 	 	public TestPage() { 		 		add(new AjaxLink<Void>("test") {  			@Override 			public void onClick(AjaxRequestTarget target) { 			} 			 		}); 		 	} 	 }  TestPage.html:  <!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"> <?xml version="1.0" encoding="UTF-8"?> <html xmlns="http://www.w3.org/1999/xhtml"> 	<head> 		<title>Test Page</title> 	</head> 	<body> 		<a wicket:id="test">test</a> 	</body> </html>  2. Access the page in browser via mounted url, the page will display a link.   3. Wait until current session is expired (do not refresh the page or click the link while waiting).   4. Hit the link and below exception will be thrown: Message: Cannot find behavior with id: 0 on component: [ [Component id = test]]. Perhaps the behavior did not properly implement getStatelessHint() and returned 'true' to indicate that it is stateless instead of returning 'false' to indicate that it is stateful.  5. In wicket 1.5.0, this results in a PageExpiredException which is more comprehensive.$$patch1-wicket-4624ab3d_Developer_PatchNaturalnessYe$$Add isNewInstance to PageProvider. Added missing logic from wicket - store. throw exception if page instance is old .. add missing import. add log level listener interface request logger. Allow new page instances for listener interface. $$1
wicket-8b7946d8$$CachingResourceLocator lookup key doesn't take strict into account$$CachingResourceLocator uses a CacheKey to store lookups for resources.   With e.g.  - b_nl.js - b.js  When e.g. a strict resource lookup for b.js with locale "us" is performed, it will store the not-found for locale "us" under the cache key. The cache key consists of resource name, locale, style and variation. However when you search non-strict for locale "us", the resource locator should find the non-localized resource "b.js", but since a matching key for the lookup was stored for this particular resource, it will fail.$$patch1-wicket-8b7946d8_Developer_PatchNaturalnessYe$$enable cache by default. Updating cache key. wicket - cache - miss. $$1
wicket-48454f4d$$Ajax behaviors are failing in stateless pages$$Stateless ajax behaviors are not working in stateless pages in 1.5-RC4.2. I verified it with the stateless demo project of Martin Grigorov (https://github.com/martin-g/wicket-stateless), when changing the dropdown on the start page an exception is thrown (clicking the increment link causes a similar exception):   org.apache.wicket.behavior.InvalidBehaviorIdException: Cannot find behavior with id: 0 on component: [DropDownChoice [Component id = c]]  At first glance the reason may be located in org.apache.wicket.Behaviors.getBehaviorById() which does not create the ID list if missing (getBehaviorsIdList(false) in line 286 instead of getBehaviorsIdList(true)), because this error does not occur when getBehaviorId() was manually called in the page constructor to force creation of the list.$$patch1-wicket-48454f4d_Developer_PatchNaturalnessYe$$fix ( wicket ) add stateless behavior to the list. Added missing method. $$1
wicket-21a47387$$Resource bundles are not resolved on PriorityHeaderItems$$If a bundle X provides resource A, and resource A is rendered as priority header item, the resource A is rendered, not bundle X.$$patch1-wicket-21a47387_Developer_PatchNaturalnessYe$$Fix whitespace. wicket - util - wicket - util - wicket - util - wicket - util - wicket - util. I wrapped the header item up until we get the real response. $$1
wicket-01a3dd66$$AjaxFormChoiceComponentUpdatingBehavior affects checkboxes even if component uses radios and vice-versa$$I have a form with two radio buttons.  Depending which radio the user selects, I show one form or another form.  I'm using an AjaxFormChoiceComponentUpdatingBehavior attached to the RadioGroup.  One of the forms has a checkbox.  The checkbox triggers an ajax update--even though the AjaxFormChoiceComponentUpdatingBehavior is attached to a RadioGroup.  AjaxFormChoiceComponentUpdatingBehavior should only affect the appropriate controls based on whether it is attached to a choice component that uses radios or checkboxes.  If a developer really wants both, then he can use two AjaxFormChoiceComponentUpdatingBehavior instances.  I've attached a patch.$$patch1-wicket-01a3dd66_Developer_PatchNaturalnessYe$$Added note to comply with the new API. added missing if ( in JSP could find a match with JSP. Added to radio to outputMarkupId = true for radios .. throw exception if radio group is not found. Added throw error for radio group components .. addedMarkupId. Added outputMarkupId to Check .. Added missing check group method. removed unused code. added idAttribute for radio choice. $$1
wicket-ed780cc7$$Parantheses problem with UrlValidator$$One of our users got an error message when trying to add a new URL:  'http://en.wikipedia.org/wiki/Genus_(mathematics)' is not a valid URL  I just created very quickly a junit test and it fails:  String[] schemes = {"http"}; UrlValidator urlValidator = new UrlValidator(schemes); assertTrue(urlValidator.isValid("http://en.wikipedia.org/wiki/Genus_(mathematics)"));$$patch1-wicket-ed780cc7_Developer_PatchNaturalnessYe$$add missing backslash. $$1
wicket-6f0863f4$$URL rendering regression$$The way URLs are encoded was changed (WICKET-4645) and now the first request (with ;jsessionid in path) generates invalid internal links: My page is mounted to "/Home/" and I get redirected to "/Home/;jsessionid=1234?0" (fine). There's a Link  on the page and the generated URL for it is "../Home;jsessionid=1234?0-1.ILinkListener-link". Note the missing "/". This results in a 404 and breaks basically all of my system tests.  I'll attach a simple quickstart which demonstrates the problem. It's important to delete the jsessionid cookie before accessing the page.$$patch1-wicket-6f0863f4_Developer_PatchNaturalnessYe$$Remove duplicated StringMode enum. add missing fields.  wicket - request does not include unicode characters. do not strip the first non - folder segment from the relative url. add missing closing parenthesis. Fix url normalize bug. Fix whitespace in Url class. $$1
wicket-294b0b2f$$When using Servlet 3.0 filter Wicket calculates filter path wrong$$When using a servlet 3.0 filter with annotations Wicket calculates the filter path wrong causing it to not match any pages other than the home page.  e.g.  {code} @WebFilter(value = "/web/*", initParams = {@WebInitParam(name = "applicationClassName", value = "com.example.CheesrApplication")}) public class CheesrFilter extends WicketFilter { } {code}  Will cause Wicket to create a filter path of /web/*/ instead of the expected /web.$$patch1-wicket-294b0b2f_Developer_PatchNaturalnessYe$$Removed the last char from the pattern parameter in the URL .. $$1
wicket-b92591f6$$Queueing component in autocomponent$$There is an exception when a component is added to queue when its parent is an auto component  <body> 		<a href="panier.html"> 			<span wicket:id="inlink"></span> 		</a> 	</body>   Last cause: Unable to find component with id 'inlink' in [TransparentWebMarkupContainer [Component id = wicket_relative_path_prefix_1]] 	Expected: 'wicket_relative_path_prefix_1:inlink'. 	Found with similar names: ''$$patch1-wicket-b92591f6_Developer_PatchNaturalnessYe$$made html header resolver public ( I think ). Removed erroneous @. wicket - 11727 FixRelativePathPrefixHandler. Added check for insideHead. Added missing javadoc. wicket - 7733 - RelativePathPrefixHandler now allows queued children components. Reduced check for isHeadTag ( ). Added auto component factory method to MarkupContainer .. Add note about component resolvers thread - safe. Updating Deprecated warning in ComponentTag. added missing MarkupContainer arg. Added missing method. $$1
wicket-ebe56869$$Dynamically adding component via an IComponentResolver fails within an enclosure for versions after 1.4.1$$We have been using an IComponentResolver implementation for a long time to allow the inclusion of certain panels to be determined by the markup. Some panels are included inside enclosures and some are not. Both cases worked fine in wicket 1.4.1 but in versions 1.4.2 and later a 'Tag expected' error occurs if the component is wrapped inside a wicket enclosure.  A quickstart example has been included to demonstrate the problem.$$patch1-wicket-ebe56869_Developer_PatchNaturalnessYe$$Removed unused imports. Reduce visibility for enclosed WebMarkupContainer. added missing "" childComponent "" method. wicket - 20066 fixed nullability check. Removed erroneous check for child component. Updating enclosure resolve method. Suppress onAfterRender Javadoc warning. Added missing import. Added Error message for failed add ( ). added code style fixes. Added resolveByApplication ( ) method that attempts to resolve a component by application - level resolver. Added resolveByComponentHierarchy method. add toString ( ) to BufferedWebResponse. $$1
wicket-be97d017$$javascript with a less than character ("<") fails to execute when added through a header contribution in ajax response$$This is adapted from a wicket users post I made (links are to the same thread in two archive systems):  http://markmail.org/search/?q=wicket%20users%20wicket-ajax.js#query:wicket%20users%20wicket-ajax.js+page:1+mid:rfts3ar3upffhbbt+state:results  http://mail-archives.apache.org/mod_mbox/wicket-users/201102.mbox/%3CAANLkTi=EkmTA0RnA+GyJE-CQWmkCxRLsjp+z8jwv-Aw9@mail.gmail.com%3E  The problem:  I have a panel with this:      <wicket:head> 	<script> 		if (someVariable < 0) { 			someVariable = 0;		 		} 	</script>     </wicket:head>  This script fails to execute when the panel is loaded by ajax.  If I replace the less than character "<" with equals "==", then it executes (but of course, this is not what I need).  I tested this in Firefox 4.0b10 and Chrome 8.  After some debugging, it seems to me that this needs to be corrected in wicket-ajax.js. The header contribution is sent to the browser inside of a CDATA section so the "<" character arrives to javascript intact. However, in parsing the script tag, the "<" seems to signal the beginning of an HTML tag that then is considered malformed.   Possible workarounds for apps:   - Invert the logic so a greater-than is used. In my example, this would be: "if (0 > someVariable) {"  - Put the code into a separate JS file (the downside is it requires another network hop from the browser)  - Embed the script in <wicket:panel> rather than <wicket:head> (the disadvantage is the script will be re-sent with the panel content when the panel is re-used on the same page)$$patch1-wicket-be97d017_Developer_PatchNaturalnessYe$$Remove unused imports. Restored interface to DefaultPageStore. $$1
wicket-961f2477$$URL query parameter values containing equals sign get cut off$$When calling a page with a query parameter like 'param1=val1=val2' the value of 'param1' obtained from PageParameters will be 'val1'. Everything after the equals sign inside the parameter value gets cut off.$$patch1-wicket-961f2477_Developer_PatchNaturalnessYe$$remove debug code. $$1
wicket-e62ded51$$discrepancy between JavaDoc and code in MarkupContainer#visitChildren()$$The JavaDoc for  MarkupContainer#visitChildren() states that  "@param clazz The class of child to visit, or null to visit all children"  The parameter clazz is used to create a new ClassVisitFilter which in its visitObject() does not check for clazz == null, leading to a NPE.$$patch1-wicket-e62ded51_Developer_PatchNaturalnessYe$$null check in ClassVisitFilter. $$1
wicket-9cb617ae$$MockHttpServletResponse.addCookie(Cookie) adds duplicate cookies$$org.apache.wicket.protocol.http.mock.MockHttpServletResponse.addCookie(Cookie) makes a bad check whether the cookie to be added is already in the list of cookies. Since javax.servlet.http.Cookie doesn't implement #equals() "cookies.remove(cookie)" wont remove the previous cookie because the identity is different.  According to http://www.ietf.org/rfc/rfc2109.txt, p.4.3.3 :   If a user agent receives a Set-Cookie response header whose NAME is    the same as a pre-existing cookie, and whose Domain and Path    attribute values exactly (string) match those of a pre-existing    cookie, the new cookie supersedes the old.  However, if the Set-    Cookie has a value for Max-Age of zero, the (old and new) cookie is    discarded.  Otherwise cookies accumulate until they expire (resources    permitting), at which time they are discarded.  I.e. the equality is on the name, path and domain.$$patch1-wicket-9cb617ae_Developer_PatchNaturalnessYe$$add missing import. remove unnecessary loop. $$1
wicket-e350f19e$$PropertyValidator ignoring groups with the @NotNull annotation only$$When using groups in your JSR303 compliant classes, Wicket does not honor the groups for the @NotNull annotation.$$patch1-wicket-e350f19e_Developer_PatchNaturalnessYe$$Updated javadoc. Added missing imports. Fix copy / paste error. Added TODO. If property ' s required flag is set , set the required flag on the component .. workflow improvements - added workflow improvements. Fix typo .. Fix typo in wicket - bean - validation. add missing context parameter. add locale parameter. $$1
wicket-f1c9cef2$$MarkupContainer.toString(true) fails with MarkupNotFoundException if the call is made in the component constructor$$org.apache.wicket.MarkupContainer.toString(boolean) uses "if (getMarkup() != null)" to decide whether to write something for the markup but since recently Component#getMarkup() throws MarkupNotFoundException when there is no markup and doesn't return null.$$patch1-wicket-f1c9cef2_Developer_PatchNaturalnessYe$$MarkupContainer doesn ' t add children to the toString method , but I ' m getting an. Remove detailed component lookup in findPage ( ). applied patch for {. $$1
wicket-6cefb9f8$$Behaviors#internalAdd(Behavior) erroneously gets id for stateless behaviors$$see http://markmail.org/thread/jtd4zn527r343jbm$$patch1-wicket-6cefb9f8_Developer_PatchNaturalnessYe$$"Revert "" remove statelessHint ( Component ) """. $$1
wicket-6a1b2f61$$StringValidator.exactLength has wrong variable in ErrorMessage$$In error message for StringValidator.exactLength is variable  {exact} , but in StringValidator.decorate is added variable length to map and not exact.   Exception when is error message interpolate for show in feedback.  Caused by: java.lang.IllegalArgumentException: Value of variable [[exact]] could not be resolved while interpolating [[' {label}' is not exactly  {exact} characters long.]]  property from application. StringValidator.exact=' {label}' is not exactly  {exact} characters long.  When I added same property in my own properties and change exact to length, it works.$$patch1-wicket-6a1b2f61_Developer_PatchNaturalnessYe$$Fix error in AbstractRangeValidator. Add EXACT validation rule. $$1
wicket-a3a5a40f$$onBeforeRender called too early on stateless page$$I'm having a problem with a ListView that displays an outdated list. In my test, the ListView uses a Model that returns a static variable just to make sure the model is independent from any page instance. As far as I can tell, this problem has nothing to do with the model, but with the way Wicket prepares for a request listener invocation.  The exact setup is this: - the page contains a ListView and (outside of the list) a Link that adds an item to the list in its onClick(). The list itself is stored in a static variable. - the page is stateless - the page's components are created in onInitialize()  Result: The list doesn't show the most recently added item. Reloading the original page shows the correct list. Note that by "reloading" I mean entering the page's original URL since the browser's address bar now contains the request listener URL due to the page being stateless.  This is how I think is happens: - Initially rendering the page works fine. The page is then discarded since it's stateless. - Clicking on the link creates a new page instance to invoke the link's request listener. - IPageAndComponentProvider.getComponent() cannot find the link yet since it is not created until onInitialize() has been called. - as a consequence, it calls page.internalInitialize() and internalPrepareForRender(false) - this creates the link, but it also creates the ListView and prepares it for rendering. This in turn polls the ListView's model and creates list items. It also marks the ListView as "prepared for render", which is the crucial point. - The link's request listener runs and adds an item to the list. - After the request listener handler, the page render handler runs - That handler renders the page, including the ListView - ... but it doesn't call onBeforeRender() on the ListView anymore, because it's already marked as "prepared for render"! So it doesn't pick up the new, up-to-date list from its model.  I'm not sure if I'm "doing it wrong", but then it doesn't seem quite right that onBeforeRender() gets called before invoking the listener, but not actually before rendering. There's probably some kind of logic behind the decision to run onBeforeRender() only when this hasn't yet happened, right? Is there a general way to "unprepare" the component in onClick()?  --- Re: #internalPrepareForRender(false) should not mark the page as rendered (thus the false parameter).  The problem is, I think, not that the component is being marked as *rendered*, but as *prepared for render*. From class Component:  protected void onBeforeRender() {   setFlag(FLAG_PREPARED_FOR_RENDER, true);   onBeforeRenderChildren();   setRequestFlag(RFLAG_BEFORE_RENDER_SUPER_CALL_VERIFIED, true); }  Note the first line. This causes subsequent invocations of internalBeforeRender() to skip the relevant part.$$patch1-wicket-a3a5a40f_Developer_PatchNaturalnessYe$$revert 488d532feb227afa77f5dfc7c. $$1
wicket-f3d7565c$$MountedMapper.mapHandler ruins Links inside mounted pages appending parameters wicket-ajax and wicket-ajax-baseurl$$With the last commit n 1166194 method mapHandler has been added to MountedMapper class in order to solve WICKET-4014. Unfortunately this method seems to ruin Link url inside mounted page (for example home page) if this page uses AJAX. mapHandler modifies Link url appending parameters 'wicket-ajax' and 'wicket-ajax-baseur'l to it. In this way when we click Link we get an error from browser like this:         " This XML file does not appear to have any style information associated with it. The document tree is shown below.       <ajax-response><redirect>wicket/page?41</redirect></ajax-response> "  The error message is the same for Firefox and Chromium. See attached quickstart.  Warning: as I'm writing this issue, Wicket snapshot is not affected yet by this bug, so you have to run quickstart with the last source from repository.$$patch1-wicket-f3d7565c_Developer_PatchNaturalnessYe$$wicket - ajax .. $$1
wicket-e1168a57$$g/apache/wicket/protocol/http/request/UserAgent matches method is not correct$$In the UserAgent Enum matches method, the loop over detectionStrings is at most executed once:      for (List<String> detectionGroup : detectionStrings)     {       for (String detectionString : detectionGroup)       {         if (!userAgent.contains(detectionString))         {           return false;         }       }        return true;     }  It returns true after only processing the first element in the detectionStrings list. It never looks at any of the other elements of the list.$$patch1-wicket-e1168a57_Developer_PatchNaturalnessYe$$"fix for Eclipse "" return false """. $$1
wicket-246d53c5$$adding (and querying) feedback messages at construction time fails.$$See http://www.nabble.com/error%28...%29-No-page-found-for-component-tf3497125.html  Currently, adding (and querying) feedback messages fails whenever it is done on components that are not yet added to a page (or were removed from them due to component replacement).  There are two ways to fix this. The first fix is attached as a patch, and basically uses a thread local to temporarily store the messages and distribute them to the relevant page instances just in time or when rendering starts. The advantage of this method is that it is completely back wards compatible.  The other way to fix this is to store all messages, whether component specific or not, in the session, and pull them from there. We need to be careful about how/ when to clean these error messages up though. We can use this issue to think about it a little bit more.$$patch1-wicket-246d53c5_Developer_PatchNaturalnessYe$$Remove unused imports. Add logging object to prevent Eclipse from giving an error due to missing log level. remove copy on close. added missing assignment. Fix getConverter ( Class /* < ? > */ ). removed null check. add comment. remove removeRenderedFeedbackMessages. Added cleanupFeedbackMessages method. remove redundant cleanupRenderedFeedbackMessages ( ). remove warning. Added missing text. Added missing @ return for feedback messages sorter. Remove redundant code. Added missing /** * * * @ see wicket . model . IModel # setObject (. Added missing /** * * * @ see wicket . model . IDetachable # detach (. Resolve # 1796. Resolve # 685. Resolve # 696. Using Session to get feedback message from component. using Session to avoid NPE in Controller. Fix error message not found in repository. Remove old behaviour. suppress warn in IDE. add missing import. remove warning. Add missing import. Added missing comment. fix hide - > show error messages in feedback form. removed unused import wicket . Session. Fix error when form is submitted .. Remove unused import. Added a constant to Page . LATEST_VERSION which is used by both forms of Page. Remove unused static final flag .. Remove unused private fields. suppress logging of components that were not rendered. Allow access to the session on any component. Add ability to get ajax version number from workflow .. Fixing failing tests .. removed redundant check. Added ignoreVersionMerge method. Remove unused setter from Page # setPageStateless ( ). Do not set componentStateChanging in IFeedback , closes # 1707. Don ' t reset the pagemap in a piece. add missing redirect after creating new version. Fixing debug in Controller and IDE files. Added missing import. Added a method to add feedback messages .. Allow authors to specify which page to clear messages from. remove empty loop. Added iterator over stored messages. Add size method to FeedbackMessages. trimToSize ( ) is a no - op if ( messages instanceof ArrayList ). remove unused method. trimToSize implements FeedbackMessages . iterator. $$1
wicket-71499e17$$UrlAttributes are encoded incorrectly when style is null but variation is not$$AbstractResourceReferenceMapper.encodeResourceReferenceAttributes() method generates the same "-foo" output for these two different inputs: {locale = null, style = "foo", variation = null} and {locale = null, style = null, variation = "foo"}. For the second input it should generate "--foo" (double dash prefix).$$patch1-wicket-71499e17_Developer_PatchNaturalnessYe$$wicket - request . mapper . AbstractResourceReferenceMapper doesn ' t ignore style empty string in JS. $$1
wicket-0578d6ee$$Invalid javascript when setStripJavascriptCommentsAndWhitespace is enabled$$When setStripJavascriptCommentsAndWhitespace is enabled (for example in deployment mode), some javascript files get corrupted. For example, the following line (notice the 2 spaces after 'return') return  this.__unbind__(type, fn); is compacted to return this.__unbind__(type, fn); which does not execute the unbind function.$$patch1-wicket-0578d6ee_Developer_PatchNaturalnessYe$$Add @ author tag. Add js comments removal. Add newline in whitespace. Allow one - line ifs. Allow underscores in JS stripper. Allow whitespace to be ignored after stripper state = WHITE_SPACE. $$1
wicket-986848f7$$FormTester doesn't correctly submit a form when a FileUploadField was not set (which is not required)$$FormTester doesn't correctly submit a form when  a FileUploadField was not set. This file is not required.  So it is impossible to create a real test because I am forced to always set a File to check to whole form.  There was discussion about this problem here: http://www.nabble.com/FormTester-and-FileUploadField-td18566869.html   I will be very grateful if you can fix it :) Artur$$patch1-wicket-986848f7_Developer_PatchNaturalnessYe$$Removed redundant read method. $$1
wicket-d78132be$$CryptoMapper ignores original queryString parameters$$When an AjaxRequest with parameters (e.g.: Autocomplete.getChoices()) arrives and CryptoMapper decrypts it, original queryString parameters dissapears.  Debugging CryptoMapper, I've checked that this method: private Url decryptUrl(final Request request, final Url encryptedUrl) {         ... }  receives querystrings parameters (on field url.parameter from "request" parameter) and the new Url returned by the method never adds them to its own list.$$patch1-wicket-d78132be_Developer_PatchNaturalnessYe$$add WICKET - 4923 additional parameters. $$1
wicket-f30bd1cb$$onremove() in RefreshingView.onPopulate$$file a bug with a quickstart. onremove() should be called on all removed components.  -igor  On Fri, Feb 18, 2011 at 5:38 AM, Benedikt Rothe <benedikt.rothe@qleo.de> wrote: > > Hi > > > > Are the existing children of a RepeatingView/RefreshingView being informed, > > when > > the View is newly populated (RefreshingView.onPopulate). > > > > I'd like to clean some internal references in this case. > > I tried: > > - aChild.onRemove is not called in this situation > > - aChild.setParent(null) is called. I treid to override setParent it. But > > setParent is private. > > > > Any suggestions? > > Benedikt$$patch1-wicket-f30bd1cb_Developer_PatchNaturalnessYe$$Added missing internalOnRemove ( ) to MarkupContainer .. $$1
wicket-2b1ce91d$$Page not mounted with WebApplication#mountPackage$$A bookmarkable page FormPage is mounted via WebApplication#mountPackage().  If this page is opened via IModel model; setResponsePage(new FormPage(IModel model)); then the URL is /wicket/page?0 which is not mounted.  If the page is mounted via WebApplication#mountPage() then the URL is mounted as expected.  If the page is not mounted then the users get PageExpiredException which in this case is unrecoverable.$$patch1-wicket-2b1ce91d_Developer_PatchNaturalnessYe$$Removed erroneous blank line. $$1
wicket-a255bbca$$BaseWicketTester#startComponentInPage fails for pages with <wicket:header-items></wicket:header> placeholder$$I am using the {{[BaseWicketTester.html#startComponentInPage(C)|https://ci.apache.org/projects/wicket/apidocs/6.x/org/apache/wicket/util/tester/BaseWicketTester.html#startComponentInPage(C)]}} to validate individual components/panels.  I am overriding the {{[BaseWicketTester.html#createPage()|https://ci.apache.org/projects/wicket/apidocs/6.x/org/apache/wicket/util/tester/BaseWicketTester.html#createPage()]}} and {{[BaseWicketTester.html#createPageMarkup(java.lang.String)|https://ci.apache.org/projects/wicket/apidocs/6.x/org/apache/wicket/util/tester/BaseWicketTester.html#createPageMarkup(java.lang.String)]}} methods to return a dummy page that contains a placeholder for components-to-be-tested. The dummy page extends my base page class.  My base page class makes use of the [<wicket:header-items/>|https://cwiki.apache.org/confluence/display/WICKET/Wicket's+XHTML+tags#Wicket'sXHTMLtags-Elementwicket:header-items] placeholder tag.  When attempting to use {{[BaseWicketTester.html#createPage()|https://ci.apache.org/projects/wicket/apidocs/6.x/org/apache/wicket/util/tester/BaseWicketTester.html#createPage()]}} method, the method fails with the following error message: |Error while parsing the markup for the autogenerated page: More than one <wicket:header-items/> detected in the <head> element. Only one is allowed.  If I remove the {{[<wicket:header-items/>|https://cwiki.apache.org/confluence/display/WICKET/Wicket's+XHTML+tags#Wicket'sXHTMLtags-Elementwicket:header-items]}} placeholder tag from my base page class, the test runs successfully.  The test only fails when using the {{[BaseWicketTester.html#startComponentInPage(C)|https://ci.apache.org/projects/wicket/apidocs/6.x/org/apache/wicket/util/tester/BaseWicketTester.html#startComponentInPage(C)]}}, which only accepts one argument. If I use the {{[BaseWicketTester.html#startComponentInPage(C, org.apache.wicket.markup.IMarkupFragment)|https://ci.apache.org/projects/wicket/apidocs/6.x/org/apache/wicket/util/tester/BaseWicketTester.html#startComponentInPage(C, org.apache.wicket.markup.IMarkupFragment)]}} and pass in the {{MarkupFragment}} of the test class as the second argument, then the test runs successfully, e.g.  {code} tester.startComponentInPage(new MyPanel(DummyPanelPage.TEST_PANEL_ID),  new MyDummyPanelPage(new PageParameters()).getMarkup()); {code}  It would seem that the {{[<wicket:header-items/>|https://cwiki.apache.org/confluence/display/WICKET/Wicket's+XHTML+tags#Wicket'sXHTMLtags-Elementwicket:header-items]}} placeholder tag clashes with the {{[ContainerInfo|https://ci.apache.org/projects/wicket/apidocs/6.x/org/apache/wicket/markup/ContainerInfo.html]}} class used by the testing framework, but this is by no means my area of expertise.  I am attaching a quick-start app with a {{TestHomePage}} test class that reproduces the issue.  Thank you in advance!$$patch1-wicket-a255bbca_Developer_PatchNaturalnessYe$$add missing import. Allow < head > tag to be opened / closed. $$1
wicket-a0150366$$AppendingStringBuffer.insert  infinite loop$$When trying to insert a StringBuffer into an AppendingStringBuffer, the method   public AppendingStringBuffer insert(final int offset, final Object obj)  will call itself repeatedly generating an infinite loop.  The fix would be to call toString() method if the object is a StringBuffer   public AppendingStringBuffer insert(final int offset, final Object obj) 	{ 		if (obj instanceof AppendingStringBuffer) 		{ 			AppendingStringBuffer asb = (AppendingStringBuffer)obj; 			return insert(offset, asb.value, 0, asb.count); 		} 		else if (obj instanceof StringBuffer) 		{ 			//return insert(offset, obj);                        return insert(offset, obj.toString());  		} 		return insert(offset, String.valueOf(obj)); 	}$$patch1-wicket-a0150366_Developer_PatchNaturalnessYe$$Add StringBuffer to framework class. wicket AddingStringBuffer ' s insert method supports StringBuilder / StringBuffer. Added StringBuilder javadoc. Employs API for StringBuffer . insert ( ). $$1
wicket-518c933b$$Url#toString(StringMode.FULL) throws exception if a segment contains two dots$$When invoking toString(StringMode.FULL) for a URL like /mountPoint/whatever.../ an IllegalStateException is thrown with message: Cannot render this url in FULL mode because it has a `..` segment: /mountPoint/whatever.../  The method does not actually check for `..` segments but rather checks whether path.contains("..")$$patch1-wicket-518c933b_Developer_PatchNaturalnessYe$$Allow url to be rendered in full mode because it has a .. $$1
wicket-ecdfc124$$WebPageRenderer must not render full page in Ajax requests$$WebPageRenderer renders the full page when WebRequest#shouldPreserveClientUrl() is true or RedirectStrategy.NEVER_REDIRECT is configured.  For Ajax request this means that wicket-ajax-js will not be able to parse the HTML response.$$patch1-wicket-ecdfc124_Developer_PatchNaturalnessYe$$add code to ignore ajax requests. $$1
wicket-0374c040$$AjaxRequestAttributes extra parameters aren't properly handled in getCallbackFunction()$$extra parameters of an Ajax behaviour can be accessed by getRequest().getRequestParameters().getParameterValue(key)  but if one uses getCallbackFunction() of an AbstractDefaultAjaxBehavior, these parameters get screwed and can no longer be accessed in the same manner.   the problem seems to be the merge in attrs.ep = Wicket.merge(attrs.ep, params);$$patch1-wicket-0374c040_Developer_PatchNaturalnessYe$$Allow missing JSON for extra parameters. $$1
wicket-a382917f$$TimeOfDay.valueOf(Calendar, Time) and TimeOfDay.valueOf(Time) incorrectly use 12-hour clock$$TimeOfDay.valueOf(Calendar, Time) is implemented as:     return militaryTime(time.getHour(calendar), time.getMinute(calendar), time.getSecond(calendar));  This is flawed because Time.getHour() is implemented as:     return get(calendar, Calendar.HOUR); and Calendar.HOUR is for the 12-hour clock. The result is that TimeOfDay.valueOf(Calendar, Time) incorrectly only returns 12-hour results, not 24-hour results. This affects TimeOfDay.valueOf(Time) as well since it is implemented in terms of the previously-named method.  One fix would be to change Time.getHour() to use Calendar.HOUR_OF_DAY. Since Time doesn't have an am/pm indicator this seems reasonable. An alternate, more localized fix would be to change TimeOfDay.valueOf(Calendar, Time) to call time.get(Calendar.HOUR_OF_DAY) to get the hour value.$$patch1-wicket-a382917f_Developer_PatchNaturalnessYe$$Fixed getHour ( ). $$1
wicket-fd910746$$FormComponents remain invalid forever if there is no feedback panel$$if there is no feedback panel the error messages are not removed in ondetach and form component re-validation is skipped so the form component, once marked as invalid, will remain invalid forever or at least until its error messages are rendered.  the error messages should be dropped and the form component should be re-validated on every form submit.$$patch1-wicket-fd910746_Developer_PatchNaturalnessYe$$wicket - 11648 - Remove a redundant validate in Form. $$1
wicket-cd3b9234$$Feedback messages not cleared for invisible/disabled form components on submit.$$Having:  - IFeedbackMessageFilter.NONE used as the default application's feedback message cleanup filter (in order to make feedback messages not to disappear after page refresh, i.e. persistent) - form with validatable component whose enabled/visible state may be dynamically changed by user (e.g. checkbox "send me email" and text field "email")  First, user tries to submit form having invalid value - as the result validation error occurs.  Then user makes that component invisible and retries form submitting - as the result no validation errors, but form wasn't submitted.  This happens because that component still has error feedback message got from first submit. Note that when using default application's feedback message cleanup filter, form is successfully submitted.  Probably, feedback messages should be cleared for invisible/disabled form components on submit, as it done for visible/enabled components in FormComponent.validate()$$patch1-wicket-cd3b9234_Developer_PatchNaturalnessYe$$@@ 1145 - cleaned up code. Remove warning. Remove unused static final modifier .. remove newline. $$1
wicket-6e794ad0$$404 Error on Nested ModalWindows in IE7 and IE8$$When opening a ModalWindow inside a ModalWindow, the inner ModalWindow generates a 404 error.  Both windows use a PageCreator for content.  To replicate, you must use an actual IE 7 or IE 8 browser, as this does not replicate using developer tools and setting the document and brower to IE 7.  The problem can be seen at http://www.wicket-library.com/wicket-examples/ajax/modal-window.  I will attach a Quickstart as well.$$patch1-wicket-6e794ad0_Developer_PatchNaturalnessYe$$Updating old import. Fixed url segmentation. make exception if the homepage was not mounted. wicket / bookmarkable / com .. $$1
wicket-4b7367ef$$Problems with cookies disabled when using 301/302 and also 303 (even with cookies)$$As mentioned in the mailing list by Martin, i open this as a bug...  Its not possible to use 303 as redirect (SC_SEE_OTHER) because thats not supported, only 302 and 301 are supported but this is defined in RFC HTTP 1.1 from 1997.   301 will add the Location header - which works as expected when disabling cookies. But a 302 (which is what i prefer) will redirect to the same page because the Location header is missing. When i enable cookies, its working.  Example can be found here: https://github.com/olze/WicketRedirect$$patch1-wicket-4b7367ef_Developer_PatchNaturalnessYe$$wicket - request should throw exception with status code 303. fixed context - relative url for redirect URLs. wicket - request doesn ' t set the status code we sent to the client .. $$1
wicket-5729ed90$$AbstractMarkupParser doesn't remove Comments correctly$$AbstractMarkupParser removeComment(...) doesn't remove Comments correctly  if two html comments stand to close together <!-- foo --> <!-- bar --> foo will be removed but not bar.  see:  https://github.com/mafulafunk/wicketComments  git@github.com:mafulafunk/wicketComments.git$$patch1-wicket-5729ed90_Developer_PatchNaturalnessYe$$Removed unused imports. Removed unnecessary modifier .. Allow ' <!-- ' tag to be parsed as HTML fragment , probably by SpringLoaded .. $$1
wicket-ceac38b1$$Component Use Check always fails for visible components inside an invisible border body$$None$$patch1-wicket-ceac38b1_Developer_PatchNaturalnessYe$$add missing import. added TODO. $$1
wicket-c66cf607$$Link always causes Page to become stateful, regardless of visibility$$Despite the changes made in WICKET-4468 , an invisible Link still causes a Page to become stateful.   The problem seems to be that Component#isStateless does this before even checking the visibility:   		if (!getStatelessHint()) 		{ 			return false; 		}   ... and Link#getStatelessHint() just contains just "return false" .$$patch1-wicket-c66cf607_Developer_PatchNaturalnessYe$$Remove stateless Hint from Controller. Added missing if (. $$1
wicket-6c5083b4$$missing base64/ URL encoding$$yesterday i showed the concept of omponents to a friend and stumled into something i dont understand and think it might be a bug.    I have a small panelcompoment that holds a searchform (textfield + submit) nothing special here, the code behind looks like:     @Override         public void onSubmit()          {             String suchFeld = getSuchfeld();             if(suchFeld.length()>0)             {                 PageParameters params = new PageParameters();                 params.add("finde",suchFeld);                 setResponsePage(Suche.class,params);             }             else             {                 setResponsePage(getPage().getClass());             }         }   the component is put into a "BasePage":    public BasePage() {         ....             add(bar);         add(new SuchPanel("SuchPanel"));         ..... }   wich is then extended by the real page:   public class Foo extends BasePage{          /** Creates a new instance of Zigarren */     public Foo() {         }   wich works all fine, however if the class name contains non ascii letters (e.g:    etc.) it gives me a bug if nothing is entered into the search and the part   public class Zubehr extends BasePage{          /** Creates a new instance of Zubehr */     public Zubehr() {     }   "setResponsePage(getPage().getClass());" comes to action, the trouble is that the page might have the URL: ?wicket:bookmarkablePage=:de.pages.Zubeh%C3%B6r but the form tries to go to :  wicket:bookmarkablePage=:de.pages.Zubeh%F6r   wich results in a CODE 404 in the App Server$$patch1-wicket-6c5083b4_Developer_PatchNaturalnessYe$$encodeURIComponent ( String ) and remove backslash. $$1
wicket-aa1d177a$$DataTable row groups are present in markup even when they contain no rows.$$As per the HTML spec : "When present, each THEAD, TFOOT, and TBODY contains a row group. Each row group must contain at least one row, defined by the TR element."  There is no check in place to remove the row group tags from the output if they don't contain any row.$$patch1-wicket-aa1d177a_Developer_PatchNaturalnessYe$$add missing import. addDT - 7744 - Missing two axes constructor. add toolbar to the container. Added a constructor that implements interface to IDE .. Do not go deeper if the data table doesn ' t exist. $$1
wicket-9c8f658a$$AjaxFormChoiceComponentUpdatingBehavior fails for choices containing other invalid FormComponents$$If a TextField inside a RadioGroup has a ValidationError, processing of AjaxFormChoiceComponentUpdatingBehavior will erroneously update the group's model:  - RadioGroup#validate() does not convert the input, because #isValid() returns false (since the nested textfield has an error message) - the behavior tests #hasErrorMessage() on the group, which returns false (since the group itself doesn't have an error message) - the behavior continues processing with a null value$$patch1-wicket-9c8f658a_Developer_PatchNaturalnessYe$$added log level exception. Fixed a typo in AJAX form componentUpdatingBehavior. add warning. revert error handling. Added onError ( target , null ) to invalid form components .. $$1
wicket-19e7c1cd$$XmlPullParser fails to properly parse from String with encoding declaration$$When parsing from a string, XmlPullParser fails if the encoding from the XML declaration is different than the system's file encoding.  Examples:    -Dfile.encoding=ISO-8859-1    parser.parse("<?xml encoding='UTF-8' ?><span id='umlaut-'></span>");     -Dfile.encoding=UTF-8    parser.parse("<?xml encoding='ISO-8859-1' ?><span id='umlaut-'></span>");  Both fail because the string is read with the system's file encoding while the parser expects the stream to be encoded in the declarated encoding.$$patch1-wicket-19e7c1cd_Developer_PatchNaturalnessYe$$Removed unused imports. Removed unused private field. Removed unused method .. Allow null string input. Added missing @ parse javadoc. Add note about closed input stream after parsing .. Fix warning. $$1
wicket-184e51e9$$WicketTester MockHttpRequest.getCookies very slow / OutOfMemory$$We have an extensive set of WicketTester tests. Recently, the wicket RELEASE in the maven repository changed to 6.7.0. After the new version, our tests got very slow.  When profiling, I discovered that the MockHttpRequest.getCookies() was taking up a lot of time. Also, tests failed because of OutOfMemory exceptions. My guess is that somehow a lot of objects are created at such speeds that the GC cannot clean them  I will investigate further, but switching back to 6.6.0 solved the issue.   [Edit] The tests are run with TestNG and using 'mvn test'$$patch1-wicket-184e51e9_Developer_PatchNaturalnessYe$$Adding missing method .. add missing internalInitialize ( ). Remove unecessary bind in AbstractPageManager. Added a try / catch block for better logging of failed workflow. Fixed a try / finally. Fixed Javadoc warning. disable caching on WebResponse. Fix whitespace. Remove unused local variable .. $$1
wicket-a08562a7$$wicket:border: inconsistency between add() and remove()$$Assuming c1 is a Border and c2 is some component, the following sequence crashes with duplicate addition:  c1.add(c2); c1.remove(c2); c1.add(c2);  The reason for this is that remove() doesn't remove the object from the bodycontainer. The sequence can be made to work by changing the middle line to:  c1.getBodyContainer().remove(c2);  That remove() doesn't look the component from the same container as add() adds it to, seems to violate the principle of least astonishment. Unfortunately the Component structure manipulation API has more methods, such as swap(), size(), get(), etc. which are final, and can't be overridden by Border as they are. It could be best to force all users to use c1.getBodyContainer().add() instead of c1.add(), because consistent operation is probably easier to deal with in the long run than behavior that conforms to initial assumptions but has flaws elsewhere.  This ticket suggests removing the overload of add() and documenting the difference in migration guide.$$patch1-wicket-a08562a7_Developer_PatchNaturalnessYe$$add missing import. wicket + add inline header for IE9. $$1
wicket-56169634$$Page mount with an optional named parameter overtakes a mount with more specific path$$See the discussion in http://markmail.org/thread/sgpiku27ah2tmcim  Having:   mountPage("/all/sindex",Page1.class);   mountPage("/all/#{exp}", Page2.class);  Request to /all/sindex will be handled by Page2.  Compatibility score for optional parameters should be lower than mandatory parameters which should be lower than exact value.$$patch1-wicket-56169634_Developer_PatchNaturalnessYe$$update score for pathSegments. $$1
wicket-ab1856db$$RequestCycleListenerCollection.onException should not throw an exception when multiple listeners handle the exception$$When multiple listeners handle the exception, RequestCycleListenerCollection should simple take the first handler. The current approach makes it impossible to add a listener that handles all exceptions and later add listeners for specific exceptions. Simply removing the 'if (handlers.size() > 1)' should suffice.$$patch1-wicket-ab1856db_Developer_PatchNaturalnessYe$$Added TODO. Added missing javadoc. Log exception if multiple exception handlers were found. $$1
wicket-1b7afefc$$AjaxEventBehavior#onEvent is invoked on disabled behavior$$Security bug  AjaxEventBehavior#onEvent is invoked on disabled behavior. It should not be - it is really dangerous, can you fix it.  I think it is security bug.$$patch1-wicket-1b7afefc_Developer_PatchNaturalnessYe$$Allow default behavior to be ignored .. $$1
wicket-a88882f7$$Wicket example 'forminput' is broken due to bad url for IOnChangeListener$$http://localhost:8080/forminput (wicket-examples) doesn't change the locale of the labels when the locale select is changed. The reason seems to be the produced url: './.?5-1.IOnChangeListener-inputForm-localeSelect'  This is parsed to a Url with one empty segment and thus HomePageMapper doesn't match it and doesn't handle it.$$patch1-wicket-a88882f7_Developer_PatchNaturalnessYe$$wicket - request url does not contain a .. $$1
wicket-31c88569$$Queuing a component in head$$Queuing a component which is in the head section doesn't work : <head> 	<meta charset="utf-8" /> 	<title wicket:id="titre">Test</title> </head>$$patch1-wicket-31c88569_Developer_PatchNaturalnessYe$$ wicket - 7733 - missing import. Added static final class to HtmlHeaderSectionHandler. Set auto component factory to avoid copy and paste error. Set the auto component factory on tag close. Removed unused AutoComponentFactory on head .. Added auto - component factory to html header section handler. $$1
wicket-aa82ccfc$$A 404 error occurs when using a CryptoMapper$$Under certain prerequisites a 404 error occurs.  The prerequisites are: - A _CryptoMapper_ is used as _RequestMapper_ - _SecuritySettings.enforceMounts_ is set to true - Class _SomePage_ is *not* annotated with _@MountPath_  Reason: In _BookmarkableMapper.parseRequest_ (called indirectly by _CryptoMapper.mapRequest_) the method _matches_ returns _false_, as _reverseUrl_ is the *encrypted URL* (_rootRequestMapper_ is a _CryptoMapper_) but _BookmarkableMapper.matches_ expects a *decrypted URL*.  _BookmarkableMapper_ - lines 132 ff.: {code} Url reverseUrl = application.getRootRequestMapper().mapHandler( 	new RenderPageRequestHandler(new PageProvider(pageClass))); if (!matches(request.cloneWithUrl(reverseUrl))) { 	return null; } {code} 	 As a result _BookmarkableMapper.mapRequest_ and hence _CryptoMapper.mapRequest_ returns _null_ resulting in a 404 error.$$patch1-wicket-aa82ccfc_Developer_PatchNaturalnessYe$$England - 221 added check for page class in PackageMapper. Removed unused imports. Fixed WICKET - 5094 issue. fix # 1296. $$1
wicket-081cdeb2$$ResourceMapper throws IllegalStateException when attempting to map a request to a URL ending in a empty segment (directory)$$ResourceMapper.mapRequest() calls ResourceMapper.removeCachingDecoration() which, throws IllegalStateException if the URL's last segment is an empty string.  URLs like: path/to/my/non/wicket/directory/ end in a empty segment.   We must change the behaviour to not attempt to undecorate a URL ending in an empty segment.$$patch1-wicket-081cdeb2_Developer_PatchNaturalnessYe$$wicket - request . mapper . ResourceMapper should not ignore empty filenames. $$1
wicket-1f128536$$Using an IValidator on an AjaxEditableLabel causes ClassCastException$$AjaxEditableLabel<Integer> label = new AjaxEditableLabel<Integer>("label", new PropertyModel<Integer>(this, "value")); form.add(label); label.setRequired(true); label.add(new RangeValidator<Integer>(1, 10));  Using a RangeValidator<Integer> on an AjaxEditableLabel<Integer>  causes an ClassCastException after editing the label.   java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.String  This can be avoided by setting the type explicit on the AjaxEditableLabel.  label.setType(Integer.class);  But this wasn't necessary in Wicket 1.4.19. In this version all works fine without setting the type explicit.  I found out, that AbstractTextComponent.resolveType() is not able to get the type of the DefaultModel of the AjaxEditableLabel in Wicket 1.5.3.  I will attach two QuickStarts to demonstrate the bug. One with wicket 1.4.19 and the other with Wicket 1.5.3$$patch1-wicket-1f128536_Developer_PatchNaturalnessYe$$add missing import. - 392 Adding missing editor instance in label and editor for IE8 ( GROO. - 406 - EditableLabel should set a default model. - - avoids NPE in WICKET - 1919. Added missing method. $$1
wicket-12124902$$RequestHandler listeners are called with null handler$$When running RequestCycleListeners with WicketTester, they get called with null for the requesthandler in onScheduleHandler(). This is an artifact of the wicket tester requestcycle, which I think should normally not occur in Wicket processing. The parameters are not marked as optional, so it is IMO safe to assume that handler should never be null.  I propose to modify scheduleRequestHandlerAfterCurrent(IRequestHandler handler) to not invoke the listeners when the handler is null, but only to clear the current scheduled handler.$$patch1-wicket-12124902_Developer_PatchNaturalnessYe$$only forward requests to the listeners when handler is null. $$1
wicket-b772ff87$$Null model for AttributeAppender should not render empty attribute$$I can't think of a reason this would be valid, but passing in null model renders <span class="">Test</span>.  If previous and new attribute are both null, the component should render cleanly like <span>Test</span>.$$patch1-wicket-b772ff87_Developer_PatchNaturalnessYe$$Fixed a bug where AttributeAppender . isEmpty ( ) returns null when the value is empty .. $$1
wicket-747bccb5$$WicketTester does not follow absolute redirects$$Wicket tester does not follow absolute redirects:  This is a problem when using HttpsMapper. For example when requesting a page over http:// with an forced redirect to https:// for secure access will make wicket tester return 'null' for the last renderer page instead of the rendered page instance. In general all kinds of absolute redirects to another page will not be tracked by wicket tester. So this potentially a problem for all kinds of tests that rely on absolute redirects.$$patch1-wicket-747bccb5_Developer_PatchNaturalnessYe$$add missing import. parse url string to an object with query string. cleaned up documentation. strtolower the protocol name. Fix NPE in Url class. Fix Url . getDefaultPortForProtocol ( String ). Remove old comment. $$1
wicket-a4a3a9a6$$AbstractNumberConverter issue when used with NumberFormat#getCurrencyInstance$$Summary of the discussion on users@:  There is an issue when using AbstractNumberConverter when #getNumberFormat returns NumberFormat#getCurrencyInstance() I think the problem is due to AbstractNumberConverter#parse(Object, double, double, Locale):  if (value instanceof String) {         // Convert spaces to no-break space (U+00A0) to fix problems with         // browser conversions.         // Space is not valid thousands-separator, but no-br space is.         value = ((String)value).replace(' ', '\u00A0'); }  Which replace spaces, so a string like "1,5 " is invalid while being parsed.  public class CurrencyConverter extends AbstractNumberConverter<Double> {     private static final long serialVersionUID = 1L;      public CurrencyConverter()     {     }      @Override     protected Class<Double> getTargetType()     {         return Double.class;     }      @Override     public NumberFormat getNumberFormat(Locale locale)     {         return NumberFormat.getCurrencyInstance(locale);     }      @Override     public Double convertToObject(String value, Locale locale)     {         locale = Locale.FRANCE;          return this.parse(value, Double.MIN_VALUE, Double.MAX_VALUE, locale);  //        This does work: //        final NumberFormat format = this.getNumberFormat(locale); //        return this.parse(format, value, locale);     } }  As Sven indicates, there is (yet another) issue in Java currency formating (space as thousand separator) http://matthiaswessendorf.wordpress.com/2007/12/03/javas-numberformat-bug/ http://bugs.sun.com/view_bug.do?bug_id=4510618  So will I let you decide whether or not you wish to fix it (the space before the currency symbol).  Thanks & best regards, Sebastien.$$patch1-wicket-a4a3a9a6_Developer_PatchNaturalnessYe$$Remove spurious blank line. $$1
math-c0b49542$$"RegulaFalsiSolver" failure$$The following unit test: {code} @Test public void testBug() {     final UnivariateRealFunction f = new UnivariateRealFunction() {             @Override             public double value(double x) {                 return Math.exp(x) - Math.pow(Math.PI, 3.0);             }         };      UnivariateRealSolver solver = new RegulaFalsiSolver();     double root = solver.solve(100, f, 1, 10); } {code} fails with {noformat} illegal state: maximal count (100) exceeded: evaluations {noformat}  Using "PegasusSolver", the answer is found after 17 evaluations.$$patch1-math-c0b49542_Developer_PatchNaturalnessYe$$Fix typo in PegasusSolver. Add missing period. Add missing param for PegasusSolver constructor .. Add missing params. bump pegasus solver version. Fix typo in IllinoisSolver. Add missing period. added missing params. Add missing params .. Updated comment. added note on the updated Secant solver. Add missing period to argument. Add missing params. Add missing params. Added missing import. Added missing params .. Add missing params. Prevent infinite loops in BaseSecantSolver. $$1
math-8dd22390$$Wrong parameter for first step size guess for Embedded Runge Kutta methods$$In a space application using DOP853 i detected what seems to be a bad parameter in the call to the method  initializeStep of class AdaptiveStepsizeIntegrator.  Here, DormandPrince853Integrator is a subclass for EmbeddedRungeKuttaIntegrator which perform the call to initializeStep at the beginning of its method integrate(...)  The problem comes from the array "scale" that is used as a parameter in the call off initializeStep(..)  Following the theory described by Hairer in his book "Solving Ordinary Differential Equations 1 : Nonstiff Problems", the scaling should be :  sci = Atol i + |y0i| * Rtoli  Whereas EmbeddedRungeKuttaIntegrator uses :  sci = Atoli  Note that the Gragg-Bulirsch-Stoer integrator uses the good implementation "sci = Atol i + |y0i| * Rtoli  " when he performs the call to the same method initializeStep(..)  In the method initializeStep, the error leads to a wrong step size h used to perform an  Euler step. Most of the time it is unvisible for the user. But in my space application the Euler step with this wrong step size h (much bigger than it should be)  makes an exception occur (my satellite hits the ground...)   To fix the bug, one should use the same algorithm as in the rescale method in GraggBulirschStoerIntegrator For exemple :   final double[] scale= new double[y0.length];;                      if (vecAbsoluteTolerance == null) {               for (int i = 0; i < scale.length; ++i) {                 final double yi = Math.max(Math.abs(y0[i]), Math.abs(y0[i]));                 scale[i] = scalAbsoluteTolerance + scalRelativeTolerance * yi;               }             } else {               for (int i = 0; i < scale.length; ++i) {                 final double yi = Math.max(Math.abs(y0[i]), Math.abs(y0[i]));                 scale[i] = vecAbsoluteTolerance[i] + vecRelativeTolerance[i] * yi;               }             }                      hNew = initializeStep(equations, forward, getOrder(), scale,                            stepStart, y, yDotK[0], yTmp, yDotK[1]);    Sorry for the length of this message, looking forward to hearing from you soon  Vincent Morand$$patch1-math-8dd22390_Developer_PatchNaturalnessYe$$removing unused import. Fix NPE in EmbeddedRungeKuttaIntegator .. $$1
math-607c9ec6$$In RealVector, dotProduct and outerProduct return wrong results due to misuse of sparse iterators$$In class {{RealVector}}, the default implementation of {{RealMatrix outerProduct(RealVector)}} uses sparse iterators on the entries of the two vectors. The rationale behind this is that {{0d * x == 0d}} is {{true}} for all {{double x}}. This assumption is in fact false, since {{0d * NaN == NaN}}.  Proposed fix is to loop through *all* entries of both vectors. This can have a significant impact on the CPU cost, but robustness should probably be preferred over speed in default implementations.  Same issue occurs with {{double dotProduct(RealVector)}}, which uses sparse iterators for {{this}} only.  Another option would be to through an exception if {{isNaN()}} is {{true}}, in which case caching could be used for both {{isNaN()}} and {{isInfinite()}}.$$patch1-math-607c9ec6_Developer_PatchNaturalnessYe$$Fix a bug in RealVector . outerProduct ( ). $$1
math-8e5867ed$$Incorrect rounding of float$$package org.apache.commons.math3.util  example of usage of round functions of Precision class:  Precision.round(0.0f, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float)0.0, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float) 0.0, 2) = 0.0 Precision.round(0.0, 2, BigDecimal.ROUND_UP) = 0.0  Seems the reason is usage of extending float to double inside round functions and getting influence of memory trash as value.  I think, same problem will be found at usage of other round modes.$$patch1-math-8e5867ed_Developer_PatchNaturalnessYe$$Fix precision test. Fix rounding method name. $$1
math-4cc9a49d$$Dangerous code in "PoissonDistributionImpl"$$In the following excerpt from class "PoissonDistributionImpl":  {code:title=PoissonDistributionImpl.java|borderStyle=solid}     public PoissonDistributionImpl(double p, NormalDistribution z) {         super();         setNormal(z);         setMean(p);     } {code}  (1) Overridable methods are called within the constructor. (2) The reference "z" is stored and modified within the class.  I've encountered problem (1) in several classes while working on issue 348. In those cases, in order to remove potential problems, I copied/pasted the body of the "setter" methods inside the constructor but I think that a more elegant solution would be to remove the "setters" altogether (i.e. make the classes immutable). Problem (2) can also create unexpected behaviour. Is it really necessary to pass the "NormalDistribution" object; can't it be always created within the class?$$patch1-math-4cc9a49d_Developer_PatchNaturalnessYe$$Remove unused javadoc. Deprecated method setMean ( ) now implements PoissonDistribution. Remove some unused methods from PoissonDistribution. Remove unused import. Set defaultsEPSILON and MAX_ITERATIONS for PoissonDistributionImpl .. Throw an exception if p < 0 || p > 1e6. Set epsilon as max iterations. Deprecated constructor in PoissonDistributionImpl .. Remove deprecated method .. Deprecated method in PoissonDistributionImpl .. $$1
math-996c0c16$$EnumeratedRealDistribution.inverseCumulativeProbability returns values not in the samples set$$The method EnumeratedRealDistribution.inverseCumulativeProbability() sometimes returns values that are not in the initial samples domain... I will attach a test to exploit this bug.$$patch1-math-996c0c16_Developer_PatchNaturalnessYe$$Add missing import. Add inverseCumulativeProbability ( ) to EnumeratedRealDistribution. $$1
math-a4ffd393$$EigenDecomposition.Solver should consider tiny values 0 for purposes of determining singularity$$EigenDecomposition.Solver tests for singularity by comparing eigenvalues to 0 for exact equality. Elsewhere in the class and in the code, of course, very small values are considered 0. This causes the solver to consider some singular matrices as non-singular.  The patch here includes a test as well showing the behavior -- the matrix is clearly singular but isn't considered as such since one eigenvalue are ~1e-14 rather than exactly 0.  (What I am not sure of is whether we should really be evaluating the *norm* of the imaginary eigenvalues rather than real/imag components separately. But the javadoc says the solver only supports real eigenvalues anyhow, so it's kind of moot since imag=0 for all eigenvalues.)$$patch1-math-a4ffd393_Developer_PatchNaturalnessYe$$EigenDecomposition isNonSingular ( ) now returns false for all 0 eigenvalues .. EigenDecomposition is using the inverse of the eigenvalue in EigenDecomposition . java. $$1
math-ebc61de9$$"RegulaFalsiSolver" failure$$The following unit test: {code} @Test public void testBug() {     final UnivariateRealFunction f = new UnivariateRealFunction() {             @Override             public double value(double x) {                 return Math.exp(x) - Math.pow(Math.PI, 3.0);             }         };      UnivariateRealSolver solver = new RegulaFalsiSolver();     double root = solver.solve(100, f, 1, 10); } {code} fails with {noformat} illegal state: maximal count (100) exceeded: evaluations {noformat}  Using "PegasusSolver", the answer is found after 17 evaluations.$$patch1-math-ebc61de9_Developer_PatchNaturalnessYe$$Remove an obsolete code line. $$1
math-185e3033$$GammaDistribution cloning broken$$Serializing a GammaDistribution and deserializing it, does not result in a cloned distribution that produces the same samples.  Cause: GammaDistribution inherits from AbstractRealDistribution, which implements Serializable. AbstractRealDistribution has random, in which we have a Well19937c instance, which inherits from AbstractWell. AbstractWell implements Serializable. AbstractWell inherits from BitsStreamGenerator, which is not Serializable, but does have a private field 'nextGaussian'.  Solution: Make BitStreamGenerator implement Serializable as well.  This probably affects other distributions as well.$$patch1-math-185e3033_Developer_PatchNaturalnessYe$$Fixed a javadoc bug in BitsStreamGenerator. $$1
math-b148046a$$getKernel fails for buckets with only multiple instances of the same value in random.EmpiricalDistribution$$After loading a set of values into an EmpericalDistribution, assume that there's a case where a single bin ONLY contains multiple instances of the same value.  In this case the standard deviation will equal zero.  This will fail when getKernel attempts to create a NormalDistribution.  The other case where stddev=0 is when there is only a single value in the bin, and this is handled by returning a ConstantRealDistribution rather than a NormalDistrbution.  See: https://issues.apache.org/jira/browse/MATH-984$$patch1-math-b148046a_Developer_PatchNaturalnessYe$$Added note about K ( x ) being a constant distribution. EmpiricalDistribution k ( x ) cannot be used as a constant real distribution. $$1
math-e2dc384d$$LevenburgMaquardt switched evaluation and iterations$$None$$patch1-math-e2dc384d_Developer_PatchNaturalnessYe$$Fix slightly changed symbol. Fix default value if ( ( value < 0 ) || ( value < 1 ). $$1
math-1d635088$$BitsStreamGenerator#nextBytes(byte[]) is wrong$$Sequential calls to the BitsStreamGenerator#nextBytes(byte[]) must generate the same sequence of bytes, no matter by chunks of which size it was divided. This is also how java.util.Random#nextBytes(byte[]) works.  When nextBytes(byte[]) is called with a bytes array of length multiple of 4 it makes one unneeded call to next(int) method. This is wrong and produces an inconsistent behavior of classes like MersenneTwister.  I made a new implementation of the BitsStreamGenerator#nextBytes(byte[]) see attached code.$$patch1-math-1d635088_Developer_PatchNaturalnessYe$$changed bits stream generator. Fixed bug with AbstractRandomGenerator . nextBytes ( ) .. $$1
math-ad252a8c$$EigenDecomposition fails for certain matrices$$The Schurtransformation of the following matrix fails, which is a preliminary step for the Eigendecomposition:  RealMatrix m = MatrixUtils.DEFAULT_FORMAT.parse("{{0.184944928,-0.0646971046,0.0774755812,-0.0969651755,-0.0692648806,0.3282344352,-0.0177423074,0.206313634},{-0.0742700134,-0.028906303,-0.001726946,-0.0375550146,-0.0487737922,-0.2616837868,-0.0821201295,-0.2530000167},{0.2549910127,0.0995733692,-0.0009718388,0.0149282808,0.1791878897,-0.0823182816,0.0582629256,0.3219545182},{-0.0694747557,-0.1880649148,-0.2740630911,0.0720096468,-0.1800836914,-0.3518996425,0.2486747833,0.6257938167},{0.0536360918,-0.1339297778,0.2241579764,-0.0195327484,-0.0054103808,0.0347564518,0.5120802482,-0.0329902864},{-0.5933332356,-0.2488721082,0.2357173629,0.0177285473,0.0856630593,-0.35671263,-0.1600668126,-0.1010899621},{-0.0514349819,-0.0854319435,0.1125050061,0.006345356,-0.2250000688,-0.220934309,0.1964623477,-0.1512329924},{0.0197395947,-0.1997170581,-0.1425959019,-0.274947791,-0.0969467073,0.060368852,-0.2826905192,0.1794315473}}");$$patch1-math-ad252a8c_Developer_PatchNaturalnessYe$$Updated SchurTransformer with one more root - > one more .. Fix the for - loop of SchurTransformer .. Fix a bug in SchurTransformer where the norm of the vector is not greater than the norm. Fix final SchurTransformer . abs ( ) .. Removing epsilon from SchurTransformer . java. Fix optimisation on SchurTransformer .. Fix SchurTransformer . java. Fix SchurTransformer . equals ( ) , added missing if (. Fix SchurTransformer . java. Fix the for loop. $$1
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch1-math-f6dd42b4_Developer_PatchNaturalnessYe$$Added missing error message. Added a message to the error message. Added missing bracketing message .. $$1
math-7cfbc0da$$arcs set split covers full circle instead of being empty$$When splitting an arcs set using an arc very close to one of the boundaries (but not at the boundary), the algorithm confuses cases for which end - start = 2pi from cases for which end - start = epsilon.  The following test case shows such a failure: {code}     @Test     public void testSplitWithinEpsilon() {         double epsilon = 1.0e-10;         double a = 6.25;         double b = a - 0.5 * epsilon;         ArcsSet set = new ArcsSet(a - 1, a, epsilon);         Arc arc = new Arc(b, b + FastMath.PI, epsilon);         ArcsSet.Split split = set.split(arc);         Assert.assertEquals(set.getSize(), split.getPlus().getSize(),  epsilon);         Assert.assertNull(split.getMinus());     } {code}  The last assertion (split.getMinus() being null) fails, as with current code split.getMinus() covers the full circle from 0 to 2pi.$$patch1-math-7cfbc0da_Developer_PatchNaturalnessYe$$Updated arcs set synced start / end check. added missing closing bracket. syncedEnd > MathUtils . TWO_PI + arcLength + arcOffset. Fixed issue with added method .. $$1
math-b9ca51f0$$Need range checks for elitismRate in ElitisticListPopulation constructors.$$There is a range check for setting the elitismRate via ElitisticListPopulation's setElitismRate method, but not via the constructors.$$patch1-math-b9ca51f0_Developer_PatchNaturalnessYe$$Improve doc string .. ElitisticListPopulation constructor should set the elitism rate before setting the population .. ElitisticListPopulation can set the elitism rate to the constructor .. $$1
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch1-math-596ccd59_Developer_PatchNaturalnessYe$$Fixing membership issue in FuzzyKMeansClusterer. $$1
math-bfe4623c$$StatUtils.sum returns NaN for zero-length arrays$$StatUtils.sum returns NaN for zero-length arrays, which is:  1. inconsistent with the mathematical notion of sum: in maths, sum_{i=0}^{N-1} a_i will be 0 for N=0. In particular, the identity  sum_{i=0}^{k-1} a_i + sum_{i=k}^{N-1} = sum_{i=0}^{N-1}  is broken for k = 0, since NaN + x = NaN, not x.  2. introduces hard to debug erros (returning a NaN is one of the worst forms of reporting an exceptional condition, as NaNs propagate silently and require manual tracing during the debugging)  3. enforces "special case" handling when the user expects that the summed array can have a zero length.  The correct behaviour is, in my opinion, to return 0.0, not NaN in the above case.$$patch1-math-bfe4623c_Developer_PatchNaturalnessYe$$Fix a typo in sumOfSquares .. Fix sum of squares constructor .. Remove redundant code. Fix sum of squares statistic. Added missing boolean check. Fix a typo in the javadoc. Fix NaN in Product. Remove redundant code. Fix double precision thingie in Product. Fix a typo in Product . evaluate ( ). Fix a typo in Product . evaluate ( ). Fix a typo in summaries of NaN values .. Fix NaN sum implementation. Remove redundant code. Fix sum implementation. Added missing boolean check. Added missing arguments. Fix a typo in Sum class .. Fix documentation. Remove useless if / else .. Remove Double . NaN from SumOfLogs class. Added boolean test. $$1
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch1-math-328513f3_Developer_PatchNaturalnessYe$$removed unused import. Added missing javadoc. removed try catch block. $$1
math-a49e443c$$Not expected UnboundedSolutionException$$SimplexSolver throws UnboundedSolutionException when trying to solve minimization linear programming problem. The number of exception thrown depends on the number of variables.  In order to see that behavior of SimplexSolver first try to run JUnit test setting a final variable ENTITIES_COUNT = 2 and that will give almost good result and then set it to 15 and you'll get a massive of unbounded exceptions. First iteration is runned with predefined set of input data with which the Solver gives back an appropriate result.  The problem itself is well tested by it's authors (mathematicians who I believe know what they developed) using Matlab 10 with no unbounded solutions on the same rules of creatnig random variables values.  What is strange to me is the dependence of the number of UnboundedSolutionException exceptions on the number of variables in the problem.  The problem is formulated as min(1*t + 0*L) (for every r-th subject) s.t. -q(r) + QL >= 0 x(r)t - XL >= 0 L >= 0 where  r = 1..R,  L = {l(1), l(2), ..., l(R)} (vector of R rows and 1 column), Q - coefficients matrix MxR X - coefficients matrix NxR$$patch1-math-a49e443c_Developer_PatchNaturalnessYe$$Fix the for loop. added another optimization step. $$1
math-2b852d79$$SpearmansCorrelation fails when using NaturalRanking together with NaNStrategy.REMOVED$$As reported by Martin Rosellen on the users mailinglist:  Using a NaturalRanking with a REMOVED NaNStrategy can result in an exception when NaN are contained in the input arrays.  The current implementation just removes the NaN values where they occur, without taken care to remove the corresponding values in the other array.$$patch1-math-2b852d79_Developer_PatchNaturalnessYe$$added missing import. Added missing documentation. added missing rankTransform call. added missing rankTransform call. added missing final modifier to method computeCorrelationMatrix. Fix NaN strategy matching in rankTransform ( ) .. $$1
math-48dde378$$DerivativeStructure.atan2(y,x) does not handle special cases properly$$The four special cases +/-0 for both x and y should give the same values as Math.atan2 and FastMath.atan2. However, they give NaN for the value in all cases.$$patch1-math-48dde378_Developer_PatchNaturalnessYe$$Fix DSCompiler . atan2 ( x , y ). $$1
math-73605560$$Line.revert() is imprecise$$Line.revert() only maintains ~10 digits for the direction. This becomes an issue when the line's position is evaluated far from the origin. A simple fix would be to use Vector3D.negate() for the direction.  Also, is there a reason why Line is not immutable? It is just comprised of two vectors.$$patch1-math-73605560_Developer_PatchNaturalnessYe$$revert line direction. $$1
math-7994d3ee$$"HarmonicFitter.ParameterGuesser" sometimes fails to return sensible values$$The inner class "ParameterGuesser" in "HarmonicFitter" (package "o.a.c.m.optimization.fitting") fails to compute a usable guess for the "amplitude" parameter.$$patch1-math-7994d3ee_Developer_PatchNaturalnessYe$$Add missing import. Fix a MathIllegalStateException in HarmonicFitter .. Add a throw if the 2D value is zero .. remove erroneous space. $$1
math-c640932d$$weight versus sigma in AbstractLeastSquares$$In AbstractLeastSquares, residualsWeights contains the WEIGHTS assigned to each observation.  In the method getRMS(), these weights are multiplicative as they should. unlike in getChiSquare() where it appears at the denominator!   If the weight is really the weight of the observation, it should multiply the square of the residual even in the computation of the chi2.   Once corrected, getRMS() can even reduce   public double getRMS() {return Math.sqrt(getChiSquare()/rows);}$$patch1-math-c640932d_Developer_PatchNaturalnessYe$$Update the AbstractLeastSquaresOptimizer . getChiSquare ( ) method to return the value of. Added missing semi - colon .. Remove erroneous comment. Add a comment about the singular value decomposition .. Fix EigenDecompositionImpl . checkInfiniteLoops. $$1
math-b12610d3$$KendallsCorrelation suffers from integer overflow for large arrays.$$For large array size (say, over 5,000), numPairs > 10 million. in line 258, (numPairs - tiedXPairs) * (numPairs - tiedYPairs) possibly > 100 billion, which will cause an integer overflow, resulting in a negative number, which will result in the end result in a NaN since the square-root of that number is calculated. This can easily be solved by changing line 163 to final long numPairs = ((long)n) * (n - 1) / 2; // to avoid overflow$$patch1-math-b12610d3_Developer_PatchNaturalnessYe$$KendallsCorrelation uses long instead of int. Fix KendallsCorrelation concordant minus discordant .. $$1
math-fc409e88$$Remove "assert" from "MathUtils.equals"$$The "assert" in methods "equals(double,double,int)" and "equals(float,float,int)" is not necessary.$$patch1-math-fc409e88_Developer_PatchNaturalnessYe$$Remove an obsolete assert. Remove an obsolete assert. $$1
math-c979a6f0$$EigenDecomposition.Solver should consider tiny values 0 for purposes of determining singularity$$EigenDecomposition.Solver tests for singularity by comparing eigenvalues to 0 for exact equality. Elsewhere in the class and in the code, of course, very small values are considered 0. This causes the solver to consider some singular matrices as non-singular.  The patch here includes a test as well showing the behavior -- the matrix is clearly singular but isn't considered as such since one eigenvalue are ~1e-14 rather than exactly 0.  (What I am not sure of is whether we should really be evaluating the *norm* of the imaginary eigenvalues rather than real/imag components separately. But the javadoc says the solver only supports real eigenvalues anyhow, so it's kind of moot since imag=0 for all eigenvalues.)$$patch1-math-c979a6f0_Developer_PatchNaturalnessYe$$added a looping over all values to reduce the likelihood of a singularity matrix being found .. $$1
math-350f726c$$"BrentOptimizer" not always reporting the best point$${{BrentOptimizer}} (package "o.a.c.m.optimization.univariate") does not check that the point it is going to return is indeed the best one it has encountered. Indeed, the last evaluated point might be slightly worse than the one before last.$$patch1-math-350f726c_Developer_PatchNaturalnessYe$$Fixing the relative and absolute threshold for Brent in the constructor .. Fix a false positive in BrentOptimizer . java. Added one more test to see if they are null .. $$1
math-b2f3f6db$$NaN singular value from SVD$$The following jython code Start code  from org.apache.commons.math.linear import *   Alist = [[1.0, 2.0, 3.0],[2.0,3.0,4.0],[3.0,5.0,7.0]]   A = Array2DRowRealMatrix(Alist)   decomp = SingularValueDecompositionImpl(A)   print decomp.getSingularValues()  End code  prints array('d', [11.218599757513008, 0.3781791648535976, nan]) The last singular value should be something very close to 0 since the matrix is rank deficient.  When i use the result from getSolver() to solve a system, i end  up with a bunch of NaNs in the solution.  I assumed i would get back a least squares solution.  Does this SVD implementation require that the matrix be full rank?  If so, then i would expect an exception to be thrown from the constructor or one of the methods.$$patch1-math-b2f3f6db_Developer_PatchNaturalnessYe$$Add singular values from NDJ - 159 ( was missing in 1 . 5 ) .. Remove erroneous loop. $$1
math-fbbb96eb$$Vector3D.crossProduct is sensitive to numerical cancellation$$Cross product implementation uses the naive formulas (y1 z2 - y2 z1, ...). These formulas fail when vectors are almost colinear, like in the following example: {code} Vector3D v1 = new Vector3D(9070467121.0, 4535233560.0, 1); Vector3D v2 = new Vector3D(9070467123.0, 4535233561.0, 1); System.out.println(Vector3D.crossProduct(v1, v2)); {code}  The previous code displays { -1, 2, 0 } instead of the correct answer { -1, 2, 1 }$$patch1-math-fbbb96eb_Developer_PatchNaturalnessYe$$better handle for cross product. $$1
math-59a0da9c$$Matrix's "OutOfBoundException" in SimplexSolver$$Hi all, This bug is somehow related to incident MATH-286, but not necessarily...  Let's say I have an LP and I solve it using SimplexSolver. Then I create a second LP similar to the first one, but with "stronger" constraints. The second LP has the following properties: * the only point in the feasible region for the second LP is the solution returned for the first LP * the solution returned for the first LP is also the (only possible) solution to the second LP  This shows the problem:  {code:borderStyle=solid} LinearObjectiveFunction f = new LinearObjectiveFunction(new double[] { 0.8, 0.2, 0.7, 0.3, 0.4, 0.6}, 0 ); Collection<LinearConstraint> constraints = new ArrayList<LinearConstraint>(); constraints.add(new LinearConstraint(new double[] { 1, 0, 1, 0, 1, 0 }, Relationship.EQ, 30.0)); constraints.add(new LinearConstraint(new double[] { 0, 1, 0, 1, 0, 1 }, Relationship.EQ, 30.0)); constraints.add(new LinearConstraint(new double[] { 0.8, 0.2, 0.0, 0.0, 0.0, 0.0 }, Relationship.GEQ, 10.0)); constraints.add(new LinearConstraint(new double[] { 0.0, 0.0, 0.7, 0.3, 0.0, 0.0 }, Relationship.GEQ, 10.0)); constraints.add(new LinearConstraint(new double[] { 0.0, 0.0, 0.0, 0.0, 0.4, 0.6 }, Relationship.GEQ, 10.0));  RealPointValuePair solution = new SimplexSolver().optimize(f, constraints, GoalType.MAXIMIZE, true);  double valA = 0.8 * solution.getPoint()[0] + 0.2 * solution.getPoint()[1]; double valB = 0.7 * solution.getPoint()[2] + 0.3 * solution.getPoint()[3]; double valC = 0.4 * solution.getPoint()[4] + 0.6 * solution.getPoint()[5];  f = new LinearObjectiveFunction(new double[] { 0.8, 0.2, 0.7, 0.3, 0.4, 0.6}, 0 ); constraints = new ArrayList<LinearConstraint>(); constraints.add(new LinearConstraint(new double[] { 1, 0, 1, 0, 1, 0 }, Relationship.EQ, 30.0)); constraints.add(new LinearConstraint(new double[] { 0, 1, 0, 1, 0, 1 }, Relationship.EQ, 30.0)); constraints.add(new LinearConstraint(new double[] { 0.8, 0.2, 0.0, 0.0, 0.0, 0.0 }, Relationship.GEQ, valA)); constraints.add(new LinearConstraint(new double[] { 0.0, 0.0, 0.7, 0.3, 0.0, 0.0 }, Relationship.GEQ, valB)); constraints.add(new LinearConstraint(new double[] { 0.0, 0.0, 0.0, 0.0, 0.4, 0.6 }, Relationship.GEQ, valC));  solution = new SimplexSolver().optimize(f, constraints, GoalType.MAXIMIZE, true); {code}   Instead of returning the solution, SimplexSolver throws an Exception:  {noformat} Exception in thread "main" org.apache.commons.math.linear.MatrixIndexException: no entry at indices (0, 7) in a 6x7 matrix 	at org.apache.commons.math.linear.Array2DRowRealMatrix.getEntry(Array2DRowRealMatrix.java:356) 	at org.apache.commons.math.optimization.linear.SimplexTableau.getEntry(SimplexTableau.java:408) 	at org.apache.commons.math.optimization.linear.SimplexTableau.getBasicRow(SimplexTableau.java:258) 	at org.apache.commons.math.optimization.linear.SimplexTableau.getSolution(SimplexTableau.java:336) 	at org.apache.commons.math.optimization.linear.SimplexSolver.doOptimize(SimplexSolver.java:182) 	at org.apache.commons.math.optimization.linear.AbstractLinearOptimizer.optimize(AbstractLinearOptimizer.java:106){noformat}   I was too optimistic with the bug MATH-286 ;-)$$patch1-math-59a0da9c_Developer_PatchNaturalnessYe$$Add column labels to the SimplexTableau class .. Fix a warning. Fix removing column labels in SimplexTableau , closes # 77 .. Fix a bug in the SimplexTableau code. Fix warning. remove unused method. Fix numDecisionVariables for SimplexTableau. Fix restore warning. $$1
math-784e4f69$$Inconsistent result from Levenberg-Marquardt$$Levenberg-Marquardt (its method doOptimize) returns a VectorialPointValuePair.  However, the class holds the optimum point, the vector of the objective function, the cost and residuals.  The value returns by doOptimize does not always corresponds to the point which leads to the residuals and cost$$patch1-math-784e4f69_Developer_PatchNaturalnessYe$$Added missing fields. Fix the jacobian reduction issue .. Update the QTy function to 1 . 0 . 1. Fix the bug in LevenbergMarquardtOptimizer . java. Added updateResidualsAndCost ( ) if the orthotol is not reached .. Fix a bug in the LevenbergMarquardtOptimizer . determineLMParameter ( ) method where. Fix grammar error in LevenbergMarquardtOptimizer . java. Remove unused variable. Added a check for convergence. Fixes the case for Levenberg - Marquardt optimization .. Remove unused variable .. $$1
math-a197ba85$$NPE in BSPTree#fitToCell()$$Hello,  I faced a NPE using  BSPTree#fitToCell() from the SVN trunk. I fixed the problem using a small patch I will attach to the ticket.$$patch1-math-a197ba85_Developer_PatchNaturalnessYe$$removed null check. $$1
math-5a6ccd58$$Brent optimizer doesn't use the Base optimizer iteration counter$$BrentOptimizer uses "iter" defined in "doOptimize"  to count iterations. It should ideally use the iteration counter defined for the BaseOptimizer.$$patch1-math-5a6ccd58_Developer_PatchNaturalnessYe$$removed unused local variable. Fix a regression in BrentOptimizer with the help of a few thousand dollars per test .. Added incrementIterationCount ( ) so that it can be used as a final step. $$1
math-8a83581e$$BigFraction.doubleValue() returns Double.NaN for large numerators or denominators$$The current implementation of doubleValue() divides numerator.doubleValue() / denominator.doubleValue().  BigInteger.doubleValue() fails for any number greater than Double.MAX_VALUE.  So if the user has 308-digit numerator or denominator, the resulting quotient fails, even in cases where the result would be well inside Double's range.  I have a patch to fix it, if I can figure out how to attach it here I will.$$patch1-math-8a83581e_Developer_PatchNaturalnessYe$$Add exponent shift for BigFraction precision. Added a note that BigFraction . floatValue ( ) may err on overflow .. $$1
math-97b440fc$$Division by zero$$In class {{Complex}}, division by zero always returns NaN. I think that it should return NaN only when the numerator is also {{ZERO}}, otherwise the result should be {{INF}}. See [here|http://en.wikipedia.org/wiki/Riemann_sphere#Arithmetic_operations].$$patch1-math-97b440fc_Developer_PatchNaturalnessYe$$Fix documentation. Fix MATH - 657. $$1
math-262fe4c0$$Maximal number of iterations (540) exceeded$$I have a matrix of size 49x19 and when I apply SVD on this matrix it raises the following exception. The problem which I am facing is that SVD works for some matrix and doesn't work for others. I have no clue what is the possible reason.  Exception:: CorrespondenceAnalysis: org.apache.commons.math.MaxIterationsExceededException: Maximal number of iterations (540) exceeded  [org.apache.commons.math.linear.EigenDecompositionImpl.processGeneralBlock(EigenDecompositionImpl.java:881), org.apache.commons.math.linear.EigenDecompositionImpl.findEigenvalues(EigenDecompositionImpl.java:651), org.apache.commons.math.linear.EigenDecompositionImpl.decompose(EigenDecompositionImpl.java:243), org.apache.commons.math.linear.EigenDecompositionImpl.<init>(EigenDecompositionImpl.java:202), org.apache.commons.math.linear.SingularValueDecompositionImpl.<init>(SingularValueDecompositionImpl.java:114),   RealMatrix m = [[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 0.94999999, 0.95833331, 1.00000000, 1.00000000, 0.99107143, 0.94583333, 1.00000000, 0.95000000, 0.98333333, 0.92106681, 0.97368419, 1.00000000, 0.95357142, 0.95238096, 1.00000000, 0.93333334, 0.96428573],[1.00000000, 1.00000000, 0.94999999, 0.95833331, 1.00000000, 1.00000000, 0.99107143, 0.94583333, 1.00000000, 0.95000000, 0.98333333, 0.92106681, 0.97368419, 1.00000000, 0.95357142, 0.95238096, 1.00000000, 0.93333334, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 0.94999999, 0.95833331, 1.00000000, 1.00000000, 0.99107143, 0.94583333, 1.00000000, 0.95000000, 0.98333333, 0.92106681, 0.97368419, 1.00000000, 0.95357142, 0.95238096, 1.00000000, 0.93333334, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 0.94999999, 0.95833331, 1.00000000, 1.00000000, 0.99107143, 0.94583333, 1.00000000, 0.95000000, 0.98333333, 0.92106681, 0.97368419, 1.00000000, 0.95357142, 0.95238096, 1.00000000, 0.93333334, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 0.94999999, 0.95833331, 1.00000000, 1.00000000, 0.99107143, 0.94583333, 1.00000000, 0.95000000, 0.98333333, 0.92106681, 0.97368419, 1.00000000, 0.95357142, 0.95238096, 1.00000000, 0.93333334, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 0.94999999, 0.95833331, 1.00000000, 1.00000000, 0.99107143, 0.94583333, 1.00000000, 0.95000000, 0.98333333, 0.92106681, 0.97368419, 1.00000000, 0.95357142, 0.95238096, 1.00000000, 0.93333334, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 0.94999999, 0.95833331, 1.00000000, 1.00000000, 0.99107143, 0.94583333, 1.00000000, 0.95000000, 0.98333333, 0.92106681, 0.97368419, 1.00000000, 0.95357142, 0.95238096, 1.00000000, 0.93333334, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 0.94999999, 0.95833331, 1.00000000, 1.00000000, 0.99107143, 0.94583333, 1.00000000, 0.95000000, 0.98333333, 0.92106681, 0.97368419, 1.00000000, 0.95357142, 0.95238096, 1.00000000, 0.93333334, 0.96428573],[1.00000000, 1.00000000, 0.94999999, 0.95833331, 1.00000000, 1.00000000, 0.99107143, 0.94583333, 1.00000000, 0.95000000, 0.98333333, 0.92106681, 0.97368419, 1.00000000, 0.95357142, 0.95238096, 1.00000000, 0.93333334, 0.96428573]]  RealMatrix rcp = MatrixUtils.createRealMatrix(CP);	 SingularValueDecomposition svd = new SingularValueDecompositionImpl(rcp);		  RealMatrix U = svd.getU(); RealMatrix S = svd.getS(); RealMatrix Vt = svd.getVT(); double[] singularValues = svd.getSingularValues();$$patch1-math-262fe4c0_Developer_PatchNaturalnessYe$$Added missing number. Added hidden vars to SingularValueDecompositionImpl .. Fix NPE in A * T * SVD .. Add logging. Added rank method for SingularValueDecompositionImpl .. Fix singular valueDecompositionImpl .. $$1
math-f64b6a90$$Incomplete beta function I(x, a, b) is inaccurate for large values of a and/or b$$This was first reported in MATH-718. The result of the current implementation of the incomplete beta function I(x, a, b) is inaccurate when a and/or b are large-ish.     I've skimmed through [slatec|http://www.netlib.org/slatec/fnlib/betai.f], GSL, [Boost|http://www.boost.org/doc/libs/1_38_0/libs/math/doc/sf_and_dist/html/math_toolkit/special/sf_beta/ibeta_function.html] as well as NR. At first sight, neither uses the same method to compute this function. I think [TOMS-708|http://www.netlib.org/toms/708] is probably the best option.    _Issue moved from MATH project on January 27, 2018 (concerned implementation was moved to module {{commons-numbers-gamma}} of "Commons Numbers")._$$patch1-math-f64b6a90_Developer_PatchNaturalnessYe$$Fix license info in fortran pdf. Added beta function , with the same coefficients as the series expansion .. Remove unused parameter warning .. Fix logBeta ( ). $$1
math-a94ff90a$$FastMath.exp may return NaN for non-NaN arguments$$I have observed that FastMath.exp(709.8125) returns NaN. However, the exponential function must never return NaN (if the argument is not NaN). The result must always be non-negative or positive infinity.$$patch1-math-a94ff90a_Developer_PatchNaturalnessYe$$FastMath . pow ( z , y , z ) returns a NaN. $$1
math-061f5017$$ODE integrator goes past specified end of integration range$$End of integration range in ODE solving is handled as an event. In some cases, numerical accuracy in events detection leads to error in events location. The following test case shows the end event is not handled properly and an integration that should cover a 60s range in fact covers a 160s range, more than twice the specified range. {code}   public void testMissedEvent() throws IntegratorException, DerivativeException {           final double t0 = 1878250320.0000029;           final double t =  1878250379.9999986;           FirstOrderDifferentialEquations ode = new FirstOrderDifferentialEquations() {                          public int getDimension() {                 return 1;             }                          public void computeDerivatives(double t, double[] y, double[] yDot)                 throws DerivativeException {                 yDot[0] = y[0] * 1.0e-6;             }         };          DormandPrince853Integrator integrator = new DormandPrince853Integrator(0.0, 100.0,                                                                                1.0e-10, 1.0e-10);          double[] y = { 1.0 };         integrator.setInitialStepSize(60.0);         double finalT = integrator.integrate(ode, t0, y, t, y);         Assert.assertEquals(t, finalT, 1.0e-6);     }  {code}$$patch1-math-061f5017_Developer_PatchNaturalnessYe$$Fixed a bug in EmbeddedRungeKuttaIntegator . evaluateStep ( ) .. added note on a small step. added note on a small step. Don ' t truncate a step if the step is too small. $$1
math-dbdff075$$SimplexSolver not working as expected?$$I guess (but I could be wrong) that SimplexSolver does not always return the optimal solution, nor satisfies all the constraints...  Consider this LP:  max: 0.8 x0 + 0.2 x1 + 0.7 x2 + 0.3 x3 + 0.6 x4 + 0.4 x5; r1: x0 + x2 + x4 = 23.0; r2: x1 + x3 + x5 = 23.0; r3: x0 >= 10.0; r4: x2 >= 8.0; r5: x4 >= 5.0;  LPSolve returns 25.8, with x0 = 10.0, x1 = 0.0, x2 = 8.0, x3 = 0.0, x4 = 5.0, x5 = 23.0;  The same LP expressed in Apache commons math is:  LinearObjectiveFunction f = new LinearObjectiveFunction(new double[] { 0.8, 0.2, 0.7, 0.3, 0.6, 0.4 }, 0 ); Collection<LinearConstraint> constraints = new ArrayList<LinearConstraint>(); constraints.add(new LinearConstraint(new double[] { 1, 0, 1, 0, 1, 0 }, Relationship.EQ, 23.0)); constraints.add(new LinearConstraint(new double[] { 0, 1, 0, 1, 0, 1 }, Relationship.EQ, 23.0)); constraints.add(new LinearConstraint(new double[] { 1, 0, 0, 0, 0, 0 }, Relationship.GEQ, 10.0)); constraints.add(new LinearConstraint(new double[] { 0, 0, 1, 0, 0, 0 }, Relationship.GEQ, 8.0)); constraints.add(new LinearConstraint(new double[] { 0, 0, 0, 0, 1, 0 }, Relationship.GEQ, 5.0));  RealPointValuePair solution = new SimplexSolver().optimize(f, constraints, GoalType.MAXIMIZE, true);  that returns 22.20, with x0 = 15.0, x1 = 23.0, x2 = 8.0, x3 = 0.0, x4 = 0.0, x5 = 0.0;  Is it possible SimplexSolver is buggy that way? The returned value is 22.20 instead of 25.8, and the last constraint (x4 >= 5.0) is not satisfied...  Am I using the interface wrongly?$$patch1-math-dbdff075_Developer_PatchNaturalnessYe$$Added a method to get the row that the variable is basic in .. Fix the bit between 0 and 1 in SimplexTableau . getSolution ( ). Added getNegativeDecisionVariableOffset ( ) to the SimplexTableau instance .. $$1
math-4ebd967c$$Beta, LogNormalDistribution, WeibullDistribution give slightly wrong answer for extremely small args due to log/exp inaccuracy$$Background for those who aren't familiar: math libs like Math and FastMath have two mysterious methods, log1p and expm1. log1p(x) = log(1+x) and expm1(x) = exp(x)-1 mathetmatically, but can return a correct answer even when x was small, where floating-point error due to the addition/subtraction introduces a relatively large error.  There are three instances in the code that can employ these specialized methods and gain a measurable improvement in accuracy. See patch and tests for an example -- try the tests without the code change to see the error.$$patch1-math-4ebd967c_Developer_PatchNaturalnessYe$$Remove logBeta from exp ( ) .. LogNormalDistribution is using expm1 instead of exp ( - 1 ). Change scale to 1 . 0 for weibull distribution .. $$1
math-fb007815$$Incorrect Kendall Tau calc due to data type mistmatch$$The Kendall Tau calculation returns a number from -1.0 to 1.0  due to a mixing of ints and longs, a mistake occurs on large size columns (arrays) passed to the function. an array size of > 50350 triggers the condition in my case - although it may be data dependent  the ver 3.5 library returns 2.6 as a result (outside of the defined range of Kendall Tau)  with the cast to long below - the result reutns to its expected value   commons.math3.stat.correlation.KendallsCorrelation.correlation   here's the sample code I used: I added the cast to long of swaps in the   			int swaps = 1077126315; 			 final long numPairs = sum(50350 - 1); 			    long tiedXPairs = 0; 		        long tiedXYPairs = 0; 		        long tiedYPairs = 0; 		         		  final long concordantMinusDiscordant = numPairs - tiedXPairs - tiedYPairs + tiedXYPairs - 2 * (long) swaps; 	        final double nonTiedPairsMultiplied = 1.6e18; 	        double myTest = concordantMinusDiscordant / FastMath.sqrt(nonTiedPairsMultiplied);$$patch1-math-fb007815_Developer_PatchNaturalnessYe$$KendallsCorrelation had a long instead of int. $$1
math-5e638976$$Integer overflow in OpenMapRealMatrix$$computeKey() has an integer overflow. Since it is a sparse matrix, this is quite easily encountered long before heap space is exhausted. The attached code demonstrates the problem, which could potentially be a security vulnerability (for example, if one was to use this matrix to store access control information).  Workaround: never create an OpenMapRealMatrix with more cells than are addressable with an int.$$patch1-math-5e638976_Developer_PatchNaturalnessYe$$Add missing import. Add one throw. $$1
math-c44bfe00$$Exception thrown in ode for a pair of close events$$When two discrete events occur closer to each other than the convergence threshold used for locating them, this sometimes triggers a NumberIsTooLargeException.  The exception happens because the EventState class think the second event is simply a numerical artifact (a repetition of the already triggerred first event) and tries to skip past it. If there are no other event in the same step later on, one interval boundary finally reach step end and the interval bounds are reversed.$$patch1-math-c44bfe00_Developer_PatchNaturalnessYe$$Fix edge case. $$1
math-d4f978dd$$Percentile Computation errs$$In the following test, the 75th percentile is _smaller_ than the 25th percentile, leaving me with a negative interquartile range.  {code:title=Bar.java|borderStyle=solid} @Test public void negativePercentiles(){          double[] data = new double[]{                 -0.012086732064244697,                  -0.24975668704012527,                  0.5706168483164684,                  -0.322111769955327,                  0.24166759508327315,                  Double.NaN,                  0.16698443218942854,                  -0.10427763937565114,                  -0.15595963093172435,                  -0.028075857595882995,                  -0.24137994506058857,                  0.47543170476574426,                  -0.07495595384947631,                  0.37445697625436497,                  -0.09944199541668033         };         DescriptiveStatistics descriptiveStatistics = new DescriptiveStatistics(data);          double threeQuarters = descriptiveStatistics.getPercentile(75);         double oneQuarter = descriptiveStatistics.getPercentile(25);          double IQR = threeQuarters - oneQuarter;                  System.out.println(String.format("25th percentile %s 75th percentile %s", oneQuarter, threeQuarters ));                  assert IQR >= 0;              } {code}$$patch1-math-d4f978dd_Developer_PatchNaturalnessYe$$Fix MATH - 1129 - Percentile sort method. $$1
math-8b418000$$numerical problems in rotation creation$$building a rotation from the following vector pairs leads to NaN: u1 = -4921140.837095533, -2.1512094250440013E7, -890093.279426377 u2 = -2.7238580938724895E9, -2.169664921341876E9, 6.749688708885301E10 v1 = 1, 0, 0 v2 = 0, 0, 1  The constructor first changes the (v1, v2) pair into (v1', v2') ensuring the following scalar products hold:  <v1'|v1'> == <u1|u1>  <v2'|v2'> == <u2|u2>  <u1 |u2>  == <v1'|v2'>  Once the (v1', v2') pair has been computed, we compute the cross product:   k = (v1' - u1)^(v2' - u2)  and the scalar product:   c = <k | (u1^u2)>  By construction, c is positive or null and the quaternion axis we want to build is q = k/[2*sqrt(c)]. c should be null only if some of the vectors are aligned, and this is dealt with later in the algorithm.  However, there are numerical problems with the vector above with the way these computations are done, as shown by the following comparisons, showing the result we get from our Java code and the result we get from manual computation with the same formulas but with enhanced precision:  commons math:   k = 38514476.5,            -84.,                           -1168590144 high precision: k = 38514410.36093388...,  -0.374075245201180409222711..., -1168590152.10599715208...  and it becomes worse when computing c because the vectors are almost orthogonal to each other, hence inducing additional cancellations. We get: commons math    c = -1.2397173627587605E20 high precision: c =  558382746168463196.7079627...  We have lost ALL significant digits in cancellations, and even the sign is wrong!$$patch1-math-8b418000_Developer_PatchNaturalnessYe$$Updated erroneous test. Removed erroneous line. Fix euclidean quaternion product. Fix euclidean quaternion product. $$1
math-a56d4998$$bracket function gives up too early$$In UnivariateSolverUtils.bracket(...) the search ends prematurely if a = lowerBound, which ignores some roots in the interval.$$patch1-math-a56d4998_Developer_PatchNaturalnessYe$$Fix a bug in UnivariateSolverUtils. $$1
math-962315ba$$Bugs in "BrentOptimizer"$$I apologize for having provided a buggy implementation of Brent's optimization algorithm (class "BrentOptimizer" in package "optimization.univariate"). The unit tests didn't show that there was something wrong, although (from the "changes.xml" file) I discovered that, at the time, Luc had noticed something weird in the implementation's behaviour. Comparing with an implementation in Python, I could figure out the fixes. I'll modify "BrentOptimizer" and add a test. I also propose to change the name of the unit test class from "BrentMinimizerTest" to "BrentOptimizerTest".$$patch1-math-962315ba_Developer_PatchNaturalnessYe$$Added a protected method to perform the optimization .. BrentOptimizer now uses the same type as the other one .. BrentOptimizer no longer uses 1 . 5 as the objective function is not minimizable .. Remove unused variable. improve performance of Brent optimization. increment iterations counter after maximal iterations reached .. Added a throw inside the doOptimize ( ) method that implements the IDE to the interface. Removed unnecessary optimisation from ConvergingAlgorithmImpl . incrementIterationsCounter ( ). $$1
math-334c01e6$$"RegulaFalsiSolver" failure$$The following unit test: {code} @Test public void testBug() {     final UnivariateRealFunction f = new UnivariateRealFunction() {             @Override             public double value(double x) {                 return Math.exp(x) - Math.pow(Math.PI, 3.0);             }         };      UnivariateRealSolver solver = new RegulaFalsiSolver();     double root = solver.solve(100, f, 1, 10); } {code} fails with {noformat} illegal state: maximal count (100) exceeded: evaluations {noformat}  Using "PegasusSolver", the answer is found after 17 evaluations.$$patch1-math-334c01e6_Developer_PatchNaturalnessYe$$Added a throw new MathInternalError .. $$1
math-03178c8b$$NormalDistribution.cumulativeProbability() suffers from cancellation$$I see the following around line 194: {noformat}         return 0.5 * (1 + Erf.erf(dev / (standardDeviation * SQRT2))); {noformat}  When erf() returns a very small value, this cancels in the addition with the "1.0" which leads to poor precision in the results.  I would suggest changing this line to read more like: {noformat} return 0.5 * Erf.erfc( -dev / standardDeviation * SQRT2 ); {noformat}   Should you want some test cases for "extreme values" (one might argue that within 10 standard deviations isn't all that extreme) then you can check the following: http://www.jstatsoft.org/v52/i07/ then look in the v52i07-xls.zip at replication-01-distribution-standard-normal.xls  I think you will also find that evaluation of expressions such as {noformat}NormalDistribution( 0, 1 ).cumulativeProbability( -10.0 );{noformat} are pretty far off.$$patch1-math-03178c8b_Developer_PatchNaturalnessYe$$Fix erroneous conversion to Erf . erf ( - dev ). $$1
math-26e878ab$$FastMath.pow(double, long) enters an infinite loop with Long.MIN_VALUE$$FastMath.pow(double, long) enters an infinite loop with Long.MIN_VALUE. It cannot be negated, so unsigned shift (>>>) is required instead of a signed one (>>).$$patch1-math-26e878ab_Developer_PatchNaturalnessYe$$FastMath . exponent ( beware , there is no exclusion ). FastMath . java was using bitwise multiplication. $$1
math-d270055e$$NPE when calling SubLine.intersection() with non-intersecting lines$$When calling SubLine.intersection() with two lines that not intersect, then a NullPointerException is thrown in Line.toSubSpace(). This bug is in the twod and threed implementations.  The attached patch fixes both implementations and adds the required test cases.$$patch1-math-d270055e_Developer_PatchNaturalnessYe$$Fixed a crash in SubLine . intersection ( ) .. Added missing return. $$1
math-495f04bc$$NaN in "equals" methods$$In "MathUtils", some "equals" methods will return true if both argument are NaN. Unless I'm mistaken, this contradicts the IEEE standard.  If nobody objects, I'm going to make the changes.$$patch1-math-495f04bc_Developer_PatchNaturalnessYe$$Fix semantics of equals ( double , double , int ) .. Removed @ deprecated javadoc file. Remove deprecated method. Fix MathUtils round up case. Fix MathUtils round up. Fix MathUtils # nextAfter ( BigDecimal , Double ). removed unused method. $$1
math-51aa6e6c$$Miscellaneous issues concerning the "optimization" package$$Revision 990792 contains changes triggered the following issues: * [MATH-394|https://issues.apache.org/jira/browse/MATH-394] * [MATH-397|https://issues.apache.org/jira/browse/MATH-397] * [MATH-404|https://issues.apache.org/jira/browse/MATH-404]  This issue collects the currently still unsatisfactory code (not necessarily sorted in order of annoyance): # "BrentOptimizer": a specific convergence checker must be used. "LevenbergMarquardtOptimizer" also has specific convergence checks. # Trying to make convergence checking independent of the optimization algorithm creates problems (conceptual and practical):  ** See "BrentOptimizer" and "LevenbergMarquardtOptimizer", the algorithm passes "points" to the convergence checker, but the actual meaning of the points can very well be different in the caller (optimization algorithm) and the callee (convergence checker).  ** In "PowellOptimizer" the line search ("BrentOptimizer") tolerances depend on the tolerances within the main algorithm. Since tolerances come with "ConvergenceChecker" and so can be changed at any time, it is awkward to adapt the values within the line search optimizer without exposing its internals ("BrentOptimizer" field) to the enclosing class ("PowellOptimizer"). # Given the numerous changes, some Javadoc comments might be out-of-sync, although I did try to update them all. # Class "DirectSearchOptimizer" (in package "optimization.direct") inherits from class "AbstractScalarOptimizer" (in package "optimization.general"). # Some interfaces are defined in package "optimization" but their base implementations (abstract class that contain the boiler-plate code) are in package "optimization.general" (e.g. "DifferentiableMultivariateVectorialOptimizer" and "BaseAbstractVectorialOptimizer"). # No check is performed to ensure the the convergence checker has been set (see e.g. "BrentOptimizer" and "PowellOptimizer"); if it hasn't there will be a NPE. The alternative is to initialize a default checker that will never be used in case the user had intended to explicitly sets the checker. # "NonLinearConjugateGradientOptimizer": Ugly workaround for the checked "ConvergenceException". # Everywhere, we trail the checked "FunctionEvaluationException" although it is never used. # There remains some duplicate code (such as the "multi-start loop" in the various "MultiStart..." implementations). # The "ConvergenceChecker" interface is very general (the "converged" method can take any number of "...PointValuePair"). However there remains a "semantic" problem: One cannot be sure that the list of points means the same thing for the caller of "converged" and within the implementation of the "ConvergenceChecker" that was independently set. # It is not clear whether it is wise to aggregate the counter of gradient evaluations to the function evaluation counter. In "LevenbergMarquartdOptimizer" for example, it would be unfair to do so. Currently I had to remove all tests referring to gradient and Jacobian evaluations. # In "AbstractLeastSquaresOptimizer" and "LevenbergMarquardtOptimizer", occurences of "OptimizationException" were replaced by the unchecked "ConvergenceException" but in some cases it might not be the most appropriate one. # "MultiStartUnivariateRealOptimizer": in the other classes ("MultiStartMultivariate...") similar to this one, the randomization is on the firts-guess value while in this class, it is on the search interval. I think that here also we should randomly choose the start value (within the user-selected interval). # The Javadoc utility raises warnings (see output of "mvn site") which I couldn't figure out how to correct. # Some previously existing classes and interfaces have become no more than a specialisation of new "generics" classes; it might be interesting to remove them in order to reduce the number of classes and thus limit the potential for confusion.$$patch1-math-51aa6e6c_Developer_PatchNaturalnessYe$$Fix a warning. Remove some redundant code. $$1
math-bfbb156d$$CMAESOptimizer with bounds fits finely near lower bound and coarsely near upper bound.$$When fitting with bounds, the CMAESOptimizer fits finely near the lower bound and coarsely near the upper bound.  This is because it internally maps the fitted parameter range into the interval [0,1].  The unit of least precision (ulp) between floating point numbers is much smaller near zero than near one.  Thus, fits have much better resolution near the lower bound (which is mapped to zero) than the upper bound (which is mapped to one).  I will attach a example program to demonstrate.$$patch1-math-bfbb156d_Developer_PatchNaturalnessYe$$Fix a comment. CMAESOptimizer does not encode the start point .. Fix incorrect decoding function in CMAESOptimizer. Fix CAMEL - 229 by adding a new one .. remove erroneous if / else. remove intermittent inverse of inputSigma. Fix decoding of fixed objective functions .. Fix CMAESOptimizer ' s isFeasible method .. Fix CMAESOptimizer ' s fix .. $$1
math-2c8a114f$$RandomDataImpl.nextPoisson fails for means in range 6.0 - 19.99$$math.random.RandomDataImpl.nextPoisson(double mean) fails frequently (but not always) for values of mean between 6.0 and 19.99 inclusive. For values below 6.0 (where I see there is a branch in the logic) and above 20.0 it seems to be okay (though I've only randomly sampled the space and run a million trials for the values I've tried)  When it fails, the exception is as follows (this for a mean of 6.0)  org.apache.commons.math.MathRuntimeException 4: must have n >= 0 for n!, got n = -2 	at org.apache.commons.math.MathRuntimeException.createIllegalArgumentException(MathRuntimeException.java:282) 	at org.apache.commons.math.util.MathUtils.factorialLog(MathUtils.java:561) 	at org.apache.commons.math.random.RandomDataImpl.nextPoisson(RandomDataImpl.java:434)   ie MathUtils.factorialLog is being called with a negative input  To reproduce:      JDKRandomGenerator random = new JDKRandomGenerator();     random.setSeed(123456);     RandomData randomData = new RandomDataImpl(random);      for (int i=0; i< 1000000; i++){         randomData.nextPoisson(6.0);     }$$patch1-math-2c8a114f_Developer_PatchNaturalnessYe$$Added a note about using a computer generation of Poisson variates , as explained. Bump the pivot point to 40 . 0d .. Handle the case of exp ( - ) in Gaussian distributions .. $$1
math-a4b1948b$$MathUtils.equals(double, double) does not work properly for floats$$MathUtils.equals(double, double) does not work properly for floats.  There is no equals(float,float) so float parameters are automatically promoted to double. However, that is not necessarily appropriate, given that the ULP for a double is much smaller than the ULP for a float.  So for example:  {code} double oneDouble = 1.0d; assertTrue(MathUtils.equals(oneDouble, Double.longBitsToDouble(1 + Double.doubleToLongBits(oneDouble)))); // OK float oneFloat = 1.0f; assertTrue(MathUtils.equals(oneFloat, Float.intBitsToFloat(1 + Float.floatToIntBits(oneFloat)))); // FAILS float  f1 = 333.33334f; double d1 = 333.33334d; assertTrue(MathUtils.equals(d1, f1)); // FAILS {code}  I think the equals() methods need to be duplicated with the appropriate changes for floats to avoid any problems with the promotion of floats.$$patch1-math-a4b1948b_Developer_PatchNaturalnessYe$$Add missing constant .. Fix a bug in MathUtils . equalsIncludingNaN. $$1
math-91d280b7$$ArrayIndexOutOfBoundsException in MathArrays.linearCombination$$When MathArrays.linearCombination is passed arguments with length 1, it throws an ArrayOutOfBoundsException. This is caused by this line:  double prodHighNext = prodHigh[1];  linearCombination should check the length of the arguments and fall back to simple multiplication if length == 1.$$patch1-math-91d280b7_Developer_PatchNaturalnessYe$$Revert to scalar multiplication .. $$1
math-a7363a2a$$Bug in MonotoneChain: a collinear point landing on the existing boundary should be dropped (patch)$$The is a bug on the code in MonotoneChain.java that attempts to handle the case of a point on the line formed by the previous last points and the last point of the chain being constructed. When `includeCollinearPoints` is false, the point should be dropped entirely. In common-math 3,3, the point is added, which in some cases can cause a `ConvergenceException` to be thrown.  In the patch below, the data points are from a case that showed up in testing before we went to production.  {code:java} Index: src/main/java/org/apache/commons/math3/geometry/euclidean/twod/hull/MonotoneChain.java =================================================================== --- src/main/java/org/apache/commons/math3/geometry/euclidean/twod/hull/MonotoneChain.java	(revision 1609491) +++ src/main/java/org/apache/commons/math3/geometry/euclidean/twod/hull/MonotoneChain.java	(working copy) @@ -160,8 +160,8 @@                  } else {                      if (distanceToCurrent > distanceToLast) {                          hull.remove(size - 1); +                        hull.add(point);                      } -                    hull.add(point);                  }                  return;              } else if (offset > 0) { Index: src/test/java/org/apache/commons/math3/geometry/euclidean/twod/hull/ConvexHullGenerator2DAbstractTest.java =================================================================== --- src/test/java/org/apache/commons/math3/geometry/euclidean/twod/hull/ConvexHullGenerator2DAbstractTest.java	(revision 1609491) +++ src/test/java/org/apache/commons/math3/geometry/euclidean/twod/hull/ConvexHullGenerator2DAbstractTest.java	(working copy) @@ -204,6 +204,24 @@      }        @Test +    public void testCollinnearPointOnExistingBoundary() { +        final Collection<Vector2D> points = new ArrayList<Vector2D>(); +        points.add(new Vector2D(7.3152, 34.7472)); +        points.add(new Vector2D(6.400799999999997, 34.747199999999985)); +        points.add(new Vector2D(5.486399999999997, 34.7472)); +        points.add(new Vector2D(4.876799999999999, 34.7472)); +        points.add(new Vector2D(4.876799999999999, 34.1376)); +        points.add(new Vector2D(4.876799999999999, 30.48)); +        points.add(new Vector2D(6.0959999999999965, 30.48)); +        points.add(new Vector2D(6.0959999999999965, 34.1376)); +        points.add(new Vector2D(7.315199999999996, 34.1376)); +        points.add(new Vector2D(7.3152, 30.48)); + +        final ConvexHull2D hull = generator.generate(points); +        checkConvexHull(points, hull); +    } + +    @Test      public void testIssue1123() {            List<Vector2D> points = new ArrayList<Vector2D>(); {code}$$patch1-math-a7363a2a_Developer_PatchNaturalnessYe$$removed extraneous line. $$1
math-32b0f733$$Division by zero$$In class {{Complex}}, division by zero always returns NaN. I think that it should return NaN only when the numerator is also {{ZERO}}, otherwise the result should be {{INF}}. See [here|http://en.wikipedia.org/wiki/Riemann_sphere#Arithmetic_operations].$$patch1-math-32b0f733_Developer_PatchNaturalnessYe$$Add isZero to Complex .. Fixed a bug where Complex . isNaN ( ) and isInfinite ( ) were acting as zero fields. Added missing closing Javadoc. Fix divide by zero logic .. Fix divide by zero error in Complex .. $$1
math-1352a70f$$BitStreamGenerators (MersenneTwister, Well generators) do not clear normal deviate cache on setSeed$$The BitStream generators generate normal deviates (for nextGaussian) in pairs, caching the last value generated. When reseeded, the cache should be cleared; otherwise seeding two generators with the same value is not guaranteed to generate the same sequence.$$patch1-math-1352a70f_Developer_PatchNaturalnessYe$$added ISAACRandom . clear ( ) .. $$1
math-ba62c59d$$2.0 equal to -2.0$$The following test fails:  {code}     @Test     public void testMath1127() {         Assert.assertFalse(Precision.equals(2.0, -2.0, 1));     } {code}$$patch1-math-ba62c59d_Developer_PatchNaturalnessYe$$Added precision bits to precision class .. updated Precision class link. updated Precision class link. Fix bug in equals ( ) .. Added link to precision class Precision. Fix bug in equals ( ) .. $$1
math-dd6cefb0$$BSPTree class and recovery of a Euclidean 3D BRep$$New to the work here. Thanks for your efforts on this code.  I create a BSPTree from a BoundaryRep (Brep) my test Brep is a cube as represented by a float array containing 8 3D points in(x,y,z) order and an array of indices (12 triplets for the 12 faces of the cube). I construct a BSPMesh() as shown in the code below. I can construct the PolyhedronsSet() but have problems extracting the faces from the BSPTree to reconstruct the BRep. The attached code (BSPMesh2.java) shows that a small change to 1 of the vertex positions causes/corrects the problem.  Any ideas?$$patch1-math-dd6cefb0_Developer_PatchNaturalnessYe$$Fixed a bug in PolygonsSet .. $$1
math-a6f96306$$Convergence Checker Fixes$$None$$patch1-math-a6f96306_Developer_PatchNaturalnessYe$$Fix bit of code. Fix false positives in LevenbergMarquardtOptimizer . evaluate ( ) .. Fix LeastSquaresFactory constructor to copy the point before adding it to the copy constructor. removed final. Change the entry in the column vector in the same way as the others .. $$1
math-95d15eff$$[math] Complex Tanh for "big" numbers$$Hi,  In Complex.java the tanh is computed with the following formula:  tanh(a + bi) = sinh(2a)/(cosh(2a)+cos(2b)) + [sin(2b)/(cosh(2a)+cos(2b))]i  The problem that I'm finding is that as soon as "a" is a "big" number, both sinh(2a) and cosh(2a) are infinity and then the method tanh returns in the real part NaN (infinity/infinity) when it should return 1.0.  Wouldn't it be appropiate to add something as in the FastMath library??:  if (real>20.0){       return createComplex(1.0, 0.0); } if (real<-20.0){       return createComplex(-1.0, 0.0); }   Best regards,  JBB$$patch1-math-95d15eff_Developer_PatchNaturalnessYe$$Fix typo in Complex class. Fix tan. fix tan from MathUtils. Fix typo in Complex class. Fix tanh reference to NPE in Complex . java. Fix tanh case. $$1
math-96eb80ef$$SimplexSolver returning wrong answer from optimize$$SimplexSolver fails for the following linear program:  min 2x1 +15x2 +18x3  Subject to    -x1 +2x2  -6x3 <=-10             x2  +2x3 <= 6    2x1      +10x3 <= 19     -x1  +x2       <= -2     x1,x2,x3 >= 0  Solution should be x1 = 7 x2 = 0 x3 = 1/2 Objective function = 23  Instead, it is returning x1 = 9.5 x2 = 1/8 x3 = 0 Objective function = 20.875  Constraint number 1 is violated by this answer$$patch1-math-96eb80ef_Developer_PatchNaturalnessYe$$Add missing import. Added a DimensionMismatchException if the dimension of the constraints doesn ' t match the objective function. Add missing import. Add a DimensionMismatchException if the dimension of the linear objective function does not match the dimension .. Add a check for dimension of the objective function. Add a helper method for checking that the dimension of the objective function and constraints match .. $$1
math-49444ee6$$stat.correlation.Covariance should allow one-column matrices$$Currently (rev 1453206), passing 1-by-M matrix to the Covariance constructor throws IllegalArgumentException. For consistency, the Covariance class should work for a single-column matrix (i.e., for a N-dimensional random variable with N=1) and it should return 1-by-1 covariance matrix with the variable's variance in its only element.$$patch1-math-49444ee6_Developer_PatchNaturalnessYe$$Missing import. add missing throws. Make Covariance constructor public. Added a comment to Covariance constructor. Fix checkstyle. Fix Covariance constructor to be compatible with Java 8 restrictions. Fix checkstyle. allow override of computeCovarianceMatrix ( ). Fix Covariance. Fix check for data size , also include zero - length cols. $$1
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch1-math-9e0c5ad4_Developer_PatchNaturalnessYe$$Fix Gamma abssampling .. $$1
math-abe53a53$$CMAESOptimizer does not enforce bounds$$The CMAESOptimizer can exceed the bounds passed to optimize.  Looking at the generationLoop in doOptimize(), it does a bounds check by calling isFeasible() but if checkFeasableCount is zero (the default) then isFeasible() is never even called.  Also, even with non-zero checkFeasableCount it may give up before finding an in-bounds offspring and go forward with an out-of-bounds offspring.  This is against svn revision 1387637.  I can provide an example program where the optimizer ends up with a fit outside the prescribed bounds if that would help.$$patch1-math-abe53a53_Developer_PatchNaturalnessYe$$Add missing import. Fix CMAESOptimizer so that it can be repaired after decoding .. Added decode method. $$1
math-b2e24119$$inverseCumulativeDistribution fails with cumulative distribution having a plateau$$This bug report follows MATH-692. The attached unit test fails. As required by the definition in MATH-692, the lower-bound of the interval on which the cdf is constant should be returned. This is not so at the moment.$$patch1-math-b2e24119_Developer_PatchNaturalnessYe$$Remove unused import. Fix a potential NPE in AbstractRealDistribution .. $$1
math-66dece12$$Fix and then deprecate isSupportXxxInclusive in RealDistribution interface$$The conclusion from [1] was never implemented. We should deprecate these properties from the RealDistribution interface, but since removal will have to wait until 4.0, we should agree on a precise definition and fix the code to match it in the mean time.  The definition that I propose is that isSupportXxxInclusive means that when the density function is applied to the upper or lower bound of support returned by getSupportXxxBound, a finite (i.e. not infinite), not NaN value is returned.  [1] http://markmail.org/message/dxuxh7eybl7xejde$$patch1-math-66dece12_Developer_PatchNaturalnessYe$$Fix UniformRealDistribution . isSupportUpperBoundInclusive. Fix FDistribution . isSupportLowerBoundInclusive. Fix deprecated warnings. $$1
math-98556fed$$AbstractRandomGenerator nextInt() and nextLong() default implementations generate only positive values$$The javadoc for these methods (and what is specified in the RandomGenerator interface) says that all int / long values should be in the range of these methods.  The default implementations provided in this class do not generate negative values.$$patch1-math-98556fed_Developer_PatchNaturalnessYe$$Removed dependency of Double nextDouble in AbstractRandomGenerator. Updated AbstractRandomGenerator . nextLong ( ). $$1
math-41f29780$$Interval class upper and lower check$$In class Interval, which is in the package org.apache.commons.math4.geometry.euclidean.oned it is possible to pass the value for variable upper  less than the value of variable lower, which is logically incorrect and  also causes the method getSize() to return negative value.  For example:   @Test   public void test1()  throws Throwable  {       Interval interval0 = new Interval(0.0, (-1.0));       double double0 = interval0.getSize();       assertEquals((-1.0), double0, 0.01D);   }$$patch1-math-41f29780_Developer_PatchNaturalnessYe$$Missing import. Bound two endpoints at the same time .. $$1
math-0596e314$$nextExponential parameter check bug - patch supplied$$Index: src/main/java/org/apache/commons/math/random/RandomDataImpl.java =================================================================== --- src/main/java/org/apache/commons/math/random/RandomDataImpl.java	(revision 830102) +++ src/main/java/org/apache/commons/math/random/RandomDataImpl.java	(working copy) @@ -462,7 +462,7 @@       * @return the random Exponential value       */      public double nextExponential(double mean) { -        if (mean < 0.0) { +        if (mean <= 0.0) {              throw MathRuntimeException.createIllegalArgumentException(                    "mean must be positive ({0})", mean);          }$$patch1-math-0596e314_Developer_PatchNaturalnessYe$$Fixed a typo in ControllerTest. $$1
math-5fe9b36c$$ConvergenceException in NormalDistributionImpl.cumulativeProbability()$$I get a ConvergenceException in  NormalDistributionImpl.cumulativeProbability() for very large/small parameters including Infinity, -Infinity. For instance in the following code:  	@Test 	public void testCumulative() { 		final NormalDistribution nd = new NormalDistributionImpl(); 		for (int i = 0; i < 500; i++) { 			final double val = Math.exp(i); 			try { 				System.out.println("val = " + val + " cumulative = " + nd.cumulativeProbability(val)); 			} catch (MathException e) { 				e.printStackTrace(); 				fail(); 			} 		} 	}  In version 2.0, I get no exception.   My suggestion is to change in the implementation of cumulativeProbability(double) to catch all ConvergenceException (and return for very large and very small values), not just MaxIterationsExceededException.$$patch1-math-5fe9b36c_Developer_PatchNaturalnessYe$$Fix a bug in NormalDistributionImpl . cumulativeProbability ( ). $$1
math-6eb46555$$In RealVector, dotProduct and outerProduct return wrong results due to misuse of sparse iterators$$In class {{RealVector}}, the default implementation of {{RealMatrix outerProduct(RealVector)}} uses sparse iterators on the entries of the two vectors. The rationale behind this is that {{0d * x == 0d}} is {{true}} for all {{double x}}. This assumption is in fact false, since {{0d * NaN == NaN}}.  Proposed fix is to loop through *all* entries of both vectors. This can have a significant impact on the CPU cost, but robustness should probably be preferred over speed in default implementations.  Same issue occurs with {{double dotProduct(RealVector)}}, which uses sparse iterators for {{this}} only.  Another option would be to through an exception if {{isNaN()}} is {{true}}, in which case caching could be used for both {{isNaN()}} and {{isInfinite()}}.$$patch1-math-6eb46555_Developer_PatchNaturalnessYe$$Fix Red Sox .. Fix ArrayRealVector dotProduct .. RealVector . dotProduct ( v ) removed someSparseEntries. $$1
math-ce2badf0$$EmpiricalDistribution cumulativeProbability can return NaN when evaluated within a constant bin$$If x belongs to a bin with no variance or to which a ConstantRealDistribution kernel has been assigned, cumulativeProbability(x) can return NaN.$$patch1-math-ce2badf0_Developer_PatchNaturalnessYe$$EmpiricalDistribution k ( x ) < kernel . getNumericalMean ( ). $$1
math-f4c926ea$$twod.PolygonsSet.getSize produces NullPointerException if BSPTree has no nodes$$org.apache.commons.math3.geometry.euclidean.twod.PolygonsSet.getSize() uses a tree internally:  final BSPTree<Euclidean2D> tree = getTree(false);  However, if that tree contains no data, it seems that the reference returned is null, which causes a subsequent NullPointerException.  Probably an exception with a message ("tree has no data") would clarify that this is an API usage error.$$patch1-math-f4c926ea_Developer_PatchNaturalnessYe$$Added a note about limited class size. Fixed a bug in PolygonsSet . boxBoundary ( ). $$1
math-2f2a2dda$$UniformIntegerDistribution should make constructer a exclusive bound or made parameter check more relax$$UniformIntegerDistribution constructer  public UniformIntegerDistribution(RandomGenerator rng,                                       int lower,                                       int upper)  the lower and the upper all inclusive. but the parameter check made a   if (lower >= upper) {             throw new NumberIsTooLargeException(                             LocalizedFormats.LOWER_BOUND_NOT_BELOW_UPPER_BOUND,                             lower, upper, false); check, i think it is too strict to construct UniformIntegerDistribution (0,0)  this should make it possible$$patch1-math-2f2a2dda_Developer_PatchNaturalnessYe$$Fix Javadoc nag. Added missing exception message. $$1
math-d4b02f6a$$Method "getResult()" in "MultiStartUnivariateRealOptimizer"$$In "MultiStartUnivariateRealOptimizer" (package "optimization"), the method "getResult" returns the result of the last run of the "underlying" optimizer; this last result might not be the best one, in which case it will not correspond to the value returned by the "optimize" method. This is confusing and does not seem very useful. I think that "getResult" should be defined as {code}  public double getResult() {     return optima[0]; } {code} and similarly {code} public double getFunctionValue() {     return optimaValues[0]; } {code}$$patch1-math-d4b02f6a_Developer_PatchNaturalnessYe$$Add one way to specify that the functions evaluations are maximal , rather than min. Add missing closing Javadoc. Fix typo in UnivariateRealOptimizer javadoc. Fix a typo in UnivariateRealOptimizer javadoc. update javadoc. update javadoc. Added missing @@. Added a couple of constructors to MultiStartUnivariateRealOptimizer . java. $$1
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch1-math-69273dca_Developer_PatchNaturalnessYe$$Fix step size in RungeKuttaIntegator. $$1
math-3a08bfa6$$inverseCumulativeProbability of BinomialDistribution returns wrong value for large trials.$$The inverseCumulativeProbability method of the BinomialDistributionImpl class returns wrong value for large trials.  Following code will be reproduce the problem.  {{System.out.println(new BinomialDistributionImpl(1000000, 0.5).inverseCumulativeProbability(0.5));}}  This returns 499525, though it should be 499999.  I'm not sure how it should be fixed, but the cause is that the cumulativeProbability method returns Infinity, not NaN.  As the result the checkedCumulativeProbability method doesn't work as expected.$$patch1-math-3a08bfa6_Developer_PatchNaturalnessYe$$added note about continued fraction implementation. Fix infinite looping in ContinuedFraction. Using the hN value of ContinuedFraction. $$1
math-76b7413d$$ResizableDoubleArray is not thread-safe yet has some synch. methods$$ResizableDoubleArray has several synchronised methods, but is not thread-safe, because class variables are not always accessed using the lock.  Is the class supposed to be thread-safe?  If so, all accesses (read and write) need to be synch.  If not, the synch. qualifiers could be dropped.  In any case, the protected fields need to be made private.$$patch1-math-76b7413d_Developer_PatchNaturalnessYe$$Removed unused import .. ResizableDoubleArray now uses the ( copied ) double array as reference. Fix double array constructor. Added default expansion mode .. Fix typo in ResizableDoubleArray .. Updating the ResizableDoubleArray class to have consistent expansion workflow. make ResizableDoubleArray constructor nullable. ResizableDoubleArray implements ResizableDoubleArray , closes # 173. Fix a merge of deprecated constructors .. added missing - arg constructor .. Add a checkNotNull check. Fix copy constructor. ResizableDoubleArray doesn ' t have a final constructor. ResizableDoubleArray doesn ' t have a synchronization point .. Synchronized addElementRolling ( ) .. Fix a typo in ResizableDoubleArray . substituteMostRecentElement ( ). removed deprecated method. Remove unused exception signature from ResizableDoubleArray . checkContractExpand ( ). Fix contract implementation. Fix a javadoc bug in ResizableDoubleArray . discardFrontElements ( ) .. Fix a warning. Fix typo in ResizableDoubleArray . expand ( ). Removed synchronized statement which was unintentionally added in ResizableDoubleArray . expand. Fix documentation for getContractionCriteria. ResizableDoubleArray . getElement ( ) is not synchronized .. ResizableDoubleArray . getElements ( ) now returns a copy of the underlying array .. Fix typo in ResizableDoubleArray . getExpansionFactor. Fix typo in ResizableDoubleArray . getExpansionMode. ResizableDoubleArray has only one method that returns a reference to the internal array .. Updating the javadoc of ResizableDoubleArray . getStartIndex ( ) to reflect the actual implementation. Fix setContractionCriteria ( ). ResizableDoubleArray doesn ' t increase the numElements value .. Fix a warning. removed synchronized statement. ResizableDoubleArray copy ( ) is deprecated .. ResizableDoubleArray copy ( ) is not synchronized. ResizableDoubleArray doesn ' t have equals method. Fix hash code for ResizableDoubleArray. $$1
math-0d057fc6$$DiscreteDistribution.sample(int) may throw an exception if first element of singletons of sub-class type$$Creating an array with {{Array.newInstance(singletons.get(0).getClass(), sampleSize)}} in DiscreteDistribution.sample(int) is risky. An exception will be thrown if: * {{singleons.get(0)}} is of type T1, an sub-class of T, and * {{DiscreteDistribution.sample()}} returns an object which is of type T, but not of type T1.  To reproduce: {code} List<Pair<Object,Double>> list = new ArrayList<Pair<Object, Double>>(); list.add(new Pair<Object, Double>(new Object() {}, new Double(0))); list.add(new Pair<Object, Double>(new Object() {}, new Double(1))); new DiscreteDistribution<Object>(list).sample(1); {code}  Attaching a patch.$$patch1-math-0d057fc6_Developer_PatchNaturalnessYe$$Removed unused import. Fix DiscreteDistribution sample ( ). $$1
math-424cbd20$$event state not updated if an unrelated event triggers a RESET_STATE during ODE integration$$When an ODE solver manages several different event types, there are some unwanted side effects.  If one event handler asks for a RESET_STATE (for integration state) when its eventOccurred method is called, the other event handlers that did not trigger an event in the same step are not updated correctly, due to an early return.  As a result, when the next step is processed with a reset integration state, the forgotten event still refer to the start date of the previous state. This implies that when these event handlers will be checked for In some cases, the function defining an event g(double t, double[] y) is called with state parameters y that are completely wrong. In one case when the y array should have contained values between -1 and +1, one function call got values up to 1.0e20.  The attached file reproduces the problem.$$patch1-math-424cbd20_Developer_PatchNaturalnessYe$$Fix double - precision thingie in AbstractIntegrator , improve stability of EventState. Fix resetOccured check. Fix bug in AbstractIntegrator , closing # 184. $$1
math-2836a6f9$$new multivariate vector optimizers cannot be used with large number of weights$$When using the Weigth class to pass a large number of weights to multivariate vector optimizers, an nxn full matrix is created (and copied) when a n elements vector is used. This exhausts memory when n is large.  This happens for example when using curve fitters (even simple curve fitters like polynomial ones for low degree) with large number of points. I encountered this with curve fitting on 41200 points, which created a matrix with 1.7 billion elements.$$patch1-math-2836a6f9_Developer_PatchNaturalnessYe$$Add missing import. Fix NPE in AbstractLeastSquaresOptimizer . squareRoot ( ). Fix warnings. Fix optimisation due to diagonal weights being too frequent. $$1
math-ce126bdb$$A random crash of MersenneTwister random generator$$There is a very small probability that MersenneTwister generator gives a following error:  java.lang.ArrayIndexOutOfBoundsException: 624 in MersenneTwister.java line 253 The error is completely random and its probability is about 1e-8.  UPD: The problem most probably arises only in multy-thread mode.$$patch1-math-ce126bdb_Developer_PatchNaturalnessYe$$fixed SynchronizedRandomGenerator use of deprecated nextInt ( ). $$1
math-63d88c74$$MultidimensionalCounter does not throw "NoSuchElementException"$$The iterator should throw when "next()" is called even though "hasNext()" would return false.$$patch1-math-63d88c74_Developer_PatchNaturalnessYe$$Add missing import. Add missing final modifier to MultidimensionalCounter .. changed hasNext ( ) to return the max count. $$1
math-621bbb8f$$Correlated random vector generator fails (silently) when faced with zero rows in covariance matrix$$The following three matrices (which are basically permutations of each other) produce different results when sampling a multi-variate Gaussian with the help of CorrelatedRandomVectorGenerator (sample covariances calculated in R, based on 10,000 samples):  Array2DRowRealMatrix{ {0.0,0.0,0.0,0.0,0.0}, {0.0,0.013445532,0.01039469,0.009881156,0.010499559}, {0.0,0.01039469,0.023006616,0.008196856,0.010732709}, {0.0,0.009881156,0.008196856,0.019023866,0.009210099}, {0.0,0.010499559,0.010732709,0.009210099,0.019107243}}  > cov(data1)    V1 V2 V3 V4 V5 V1 0 0.000000000 0.00000000 0.000000000 0.000000000 V2 0 0.013383931 0.01034401 0.009913271 0.010506733 V3 0 0.010344006 0.02309479 0.008374730 0.010759306 V4 0 0.009913271 0.00837473 0.019005488 0.009187287 V5 0 0.010506733 0.01075931 0.009187287 0.019021483  Array2DRowRealMatrix{ {0.013445532,0.01039469,0.0,0.009881156,0.010499559}, {0.01039469,0.023006616,0.0,0.008196856,0.010732709}, {0.0,0.0,0.0,0.0,0.0}, {0.009881156,0.008196856,0.0,0.019023866,0.009210099}, {0.010499559,0.010732709,0.0,0.009210099,0.019107243}}  > cov(data2)             V1 V2 V3 V4 V5 V1 0.006922905 0.010507692 0 0.005817399 0.010330529 V2 0.010507692 0.023428918 0 0.008273152 0.010735568 V3 0.000000000 0.000000000 0 0.000000000 0.000000000 V4 0.005817399 0.008273152 0 0.004929843 0.009048759 V5 0.010330529 0.010735568 0 0.009048759 0.018683544   Array2DRowRealMatrix{ {0.013445532,0.01039469,0.009881156,0.010499559}, {0.01039469,0.023006616,0.008196856,0.010732709}, {0.009881156,0.008196856,0.019023866,0.009210099}, {0.010499559,0.010732709,0.009210099,0.019107243}}  > cov(data3)             V1          V2          V3          V4 V1 0.013445047 0.010478862 0.009955904 0.010529542 V2 0.010478862 0.022910522 0.008610113 0.011046353 V3 0.009955904 0.008610113 0.019250975 0.009464442 V4 0.010529542 0.011046353 0.009464442 0.019260317   I've traced this back to the RectangularCholeskyDecomposition, which does not seem to handle the second matrix very well (decompositions in the same order as the matrices above):  CorrelatedRandomVectorGenerator.getRootMatrix() =  Array2DRowRealMatrix{{0.0,0.0,0.0,0.0,0.0},{0.0759577418122063,0.0876125188474239,0.0,0.0,0.0},{0.07764443622513505,0.05132821221460752,0.11976381821791235,0.0,0.0},{0.06662930527909404,0.05501661744114585,0.0016662506519307997,0.10749324207653632,0.0},{0.13822895138139477,0.0,0.0,0.0,0.0}} CorrelatedRandomVectorGenerator.getRank() = 5  CorrelatedRandomVectorGenerator.getRootMatrix() =  Array2DRowRealMatrix{{0.0759577418122063,0.034512751379448724,0.0},{0.07764443622513505,0.13029949164628746,0.0},{0.0,0.0,0.0},{0.06662930527909404,0.023203936694855674,0.0},{0.13822895138139477,0.0,0.0}} CorrelatedRandomVectorGenerator.getRank() = 3  CorrelatedRandomVectorGenerator.getRootMatrix() =  Array2DRowRealMatrix{{0.0759577418122063,0.034512751379448724,0.033913748226348225,0.07303890149947785},{0.07764443622513505,0.13029949164628746,0.0,0.0},{0.06662930527909404,0.023203936694855674,0.11851573313229945,0.0},{0.13822895138139477,0.0,0.0,0.0}} CorrelatedRandomVectorGenerator.getRank() = 4  Clearly, the rank of each of these matrices should be 4. The first matrix does not lead to incorrect results, but the second one does. Unfortunately, I don't know enough about the Cholesky decomposition to find the flaw in the implementation, and I could not find documentation for the "rectangular" variant (also not at the links provided in the javadoc).$$patch1-math-621bbb8f_Developer_PatchNaturalnessYe$$Fixed final declaration. Fix swapped indexing. Fix the bug in RectangularCholeskyDecomposition where the inverse is not greater than the matrix. $$1
math-133cbc2d$$SimplexSolver returns unfeasible solution$$The SimplexSolver is returning an unfeasible solution:  import java.util.ArrayList; import java.text.DecimalFormat; import org.apache.commons.math.linear.ArrayRealVector; import org.apache.commons.math.optimization.GoalType; import org.apache.commons.math.optimization.OptimizationException; import org.apache.commons.math.optimization.linear.*;  public class SimplexSolverBug {          public static void main(String[] args) throws OptimizationException {                  LinearObjectiveFunction c = new LinearObjectiveFunction(new double[]{0.0d, 1.0d, 1.0d, 0.0d, 0.0d, 0.0d, 0.0d}, 0.0d);                  ArrayList<LinearConstraint> cnsts = new ArrayList<LinearConstraint>(5);         LinearConstraint cnst;         cnst = new LinearConstraint(new double[] {1.0d, -0.1d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d}, Relationship.EQ, -0.1d);         cnsts.add(cnst);         cnst = new LinearConstraint(new double[] {1.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d}, Relationship.GEQ, -1e-18d);         cnsts.add(cnst);         cnst = new LinearConstraint(new double[] {0.0d, 1.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d}, Relationship.GEQ, 0.0d);         cnsts.add(cnst);         cnst = new LinearConstraint(new double[] {0.0d, 0.0d, 0.0d, 1.0d, 0.0d, -0.0128588d, 1e-5d}, Relationship.EQ, 0.0d);         cnsts.add(cnst);         cnst = new LinearConstraint(new double[] {0.0d, 0.0d, 0.0d, 0.0d, 1.0d, 1e-5d, -0.0128586d}, Relationship.EQ, 1e-10d);         cnsts.add(cnst);         cnst = new LinearConstraint(new double[] {0.0d, 0.0d, 1.0d, -1.0d, 0.0d, 0.0d, 0.0d}, Relationship.GEQ, 0.0d);         cnsts.add(cnst);         cnst = new LinearConstraint(new double[] {0.0d, 0.0d, 1.0d, 1.0d, 0.0d, 0.0d, 0.0d}, Relationship.GEQ, 0.0d);         cnsts.add(cnst);         cnst = new LinearConstraint(new double[] {0.0d, 0.0d, 1.0d, 0.0d, -1.0d, 0.0d, 0.0d}, Relationship.GEQ, 0.0d);         cnsts.add(cnst);         cnst = new LinearConstraint(new double[] {0.0d, 0.0d, 1.0d, 0.0d, 1.0d, 0.0d, 0.0d}, Relationship.GEQ, 0.0d);         cnsts.add(cnst);                          DecimalFormat df = new java.text.DecimalFormat("0.#####E0");                  System.out.println("Constraints:");         for(LinearConstraint con : cnsts) {             for (int i = 0; i < con.getCoefficients().getDimension(); ++i)                 System.out.print(df.format(con.getCoefficients().getData()[i]) + " ");             System.out.println(con.getRelationship() + " " + con.getValue());         }                  SimplexSolver simplex = new SimplexSolver(1e-7);         double[] sol = simplex.optimize(c, cnsts, GoalType.MINIMIZE, false).getPointRef();         System.out.println("Solution:\n" + new ArrayRealVector(sol));         System.out.println("Second constraint is violated!");     } }   It's an odd problem, but something I ran across.  I tracked the problem to the getPivotRow routine in SimplexSolver.  It was choosing a pivot that resulted in a negative right-hand-side.  I recommend a fix by replacing                 ...                 if (MathUtils.equals(ratio, minRatio, epsilon)) {                 ... with                 ...                 if (MathUtils.equals(ratio, minRatio, Math.abs(epsilon/entry))) {                 ...  I believe this would be more appropriate (and at least resolves this particular problem).  Also, you may want to consider making a change in getPivotColumn to replace             ...             if (MathUtils.compareTo(tableau.getEntry(0, i), minValue, epsilon) < 0) {             ... with             ...             if (tableau.getEntry(0, i) < minValue)              ... because I don't see the point of biasing earlier columns when multiple entries are within epsilon of each other.  Why not pick the absolute smallest.  I don't know that any problem can result from doing it the other way, but the latter may be a safer bet.  VERY IMPORTANT: I discovered another bug that occurs when not restricting to non-negatives.  In SimplexTableu::getSolution(),            ...                     if (basicRows.contains(basicRow))                // if multiple variables can take a given value               // then we choose the first and set the rest equal to 0               coefficients[i] = 0;           ... should be           ...                     if (basicRows.contains(basicRow)) {               // if multiple variables can take a given value               // then we choose the first and set the rest equal to 0               coefficients[i] = (restrictToNonNegative ? 0 : -mostNegative);           ... If necessary, I can give an example of where this bug causes a problem, but it should be fairly obvious why this was wrong.$$patch1-math-133cbc2d_Developer_PatchNaturalnessYe$$Add missing import. Add additional param for SimplexSolver constructor .. Using the Epsilon compiler for the min value of the tableau object .. Fixed a bug in the SimplexSolver .. Using the epsilon function in the SimplexSolver class .. Fixed a minor issue with the objective function due to projection outside the box .. Add maxUlps parameter to SimplexSolver. Added a method to get the epsilon from the SimplexSolver via its own repo. Add missing import. Added default ulp value to the SimplexTableau class .. Add maxUlps field to SimplexTableau .. Add additional params. Fix the inverse of the coefficient summation in SimplexTableau. Fix minor bug in SimplexTableau. revert change to getInvertedCoeffiecientSum ( ). Fixed a bug in the SimplexTableau class .. removed epsilon from positive cost tableau .. Fix the binary search .. Fix the coefficients array to be compatible with Linear Regression. Add maxUlps equals to SimplexTableau , since it is often the case of. Add maxUlps in SimplexTableau , closes # 444. Added getter for epsilon. $$1
math-bda25b40$$EigenDecomposition may not converge for certain matrices$$Jama-1.0.3 contains a bugfix for certain matrices where the original code goes into an infinite loop.  The commons-math translations would throw a MaxCountExceededException, so fails to compute the eigen decomposition.  Port the fix from jama to CM.$$patch1-math-bda25b40_Developer_PatchNaturalnessYe$$Fix the case for 1 / 2 shift .. $$1
math-ce185345$$getLInfNorm() uses wrong formula in both ArrayRealVector and OpenMapRealVector (in different ways)$$the L_infinity norm of a finite dimensional vector is just the max of the absolute value of its entries.  The current implementation in ArrayRealVector has a typo:  {code}     public double getLInfNorm() {         double max = 0;         for (double a : data) {             max += Math.max(max, Math.abs(a));         }         return max;     } {code}  the += should just be an =.  There is sadly a unit test assuring us that this is the correct behavior (effectively a regression-only test, not a test for correctness).  Worse, the implementation in OpenMapRealVector is not even positive semi-definite:  {code}        public double getLInfNorm() {         double max = 0;         Iterator iter = entries.iterator();         while (iter.hasNext()) {             iter.advance();             max += iter.value();         }         return max;     } {code}  I would suggest that this method be moved up to the AbstractRealVector superclass and implemented using the sparseIterator():  {code}   public double getLInfNorm() {     double norm = 0;     Iterator<Entry> it = sparseIterator();     Entry e;     while(it.hasNext() && (e = it.next()) != null) {       norm = Math.max(norm, Math.abs(e.getValue()));     }     return norm;   } {code}  Unit tests with negative valued vectors would be helpful to check for this kind of thing in the future.$$patch1-math-ce185345_Developer_PatchNaturalnessYe$$Remove unused method .. Fix typo in OpenMapRealVector . getNorm. Fix ArrayRealVector . getNorm. Add missing @ Override . getL1Norm ( ). Fix ArrayRealVector . getLInfNorm ( ). Fix an issue with the inverse of AbstractRealVector . getNorm ( ) .. $$1
math-c06cc933$$NaN singular value from SVD$$The following jython code Start code  from org.apache.commons.math.linear import *   Alist = [[1.0, 2.0, 3.0],[2.0,3.0,4.0],[3.0,5.0,7.0]]   A = Array2DRowRealMatrix(Alist)   decomp = SingularValueDecompositionImpl(A)   print decomp.getSingularValues()  End code  prints array('d', [11.218599757513008, 0.3781791648535976, nan]) The last singular value should be something very close to 0 since the matrix is rank deficient.  When i use the result from getSolver() to solve a system, i end  up with a bunch of NaNs in the solution.  I assumed i would get back a least squares solution.  Does this SVD implementation require that the matrix be full rank?  If so, then i would expect an exception to be thrown from the constructor or one of the methods.$$patch1-math-c06cc933_Developer_PatchNaturalnessYe$$Added details in the SingularValueDecomposition class. Add note about singular values compact SVD .. Add a constructor to SingularValueDecompositionImpl .. Remove erroneous loop. Fix the for loop. Fix singular values in Bt . E . S ^ ( - 1 ) where Bt is. Fix SingularValueDecompositionImpl . getCovariance ( ). Fix SingularValueDecompositionImpl . walkInOptimizedOrder ( ) so that the data is. Added pseudoInverse to Solver .. Add inverse of singular value decomposition. Removing intermittent erroneous throw. Use the inverse of the singular values in the decomposition .. removed erroneous import. Make getInverse ( ) return the inverse matrix. $$1
math-b285f170$$The LinearConstraintSet shall return its constraints in a deterministic way$$As previously discussed on the mailinglist, the LinearConstraintSet should return its internally stored LinearConstraints in the same iteration order as they have been provided via its constructor.  This ensures that the execution of the same linear problem results in the same results each time it is executed. This is especially important when linear problems are loaded from a file, e.g. mps format, and makes it simpler to debug problems and compare with other solvers which do the same thing.$$patch1-math-b285f170_Developer_PatchNaturalnessYe$$removed unused import. removed unnecessary initialization .. $$1
math-26a61077$$GaussianFitter Unexpectedly Throws NotStrictlyPositiveException$$Running the following:      	double[] observations =      	{      			1.1143831578403364E-29,      			 4.95281403484594E-28,      			 1.1171347211930288E-26,      			 1.7044813962636277E-25,      			 1.9784716574832164E-24,      			 1.8630236407866774E-23,      			 1.4820532905097742E-22,      			 1.0241963854632831E-21,      			 6.275077366673128E-21,      			 3.461808994532493E-20,      			 1.7407124684715706E-19,      			 8.056687953553974E-19,      			 3.460193945992071E-18,      			 1.3883326374011525E-17,      			 5.233894983671116E-17,      			 1.8630791465263745E-16,      			 6.288759227922111E-16,      			 2.0204433920597856E-15,      			 6.198768938576155E-15,      			 1.821419346860626E-14,      			 5.139176445538471E-14,      			 1.3956427429045787E-13,      			 3.655705706448139E-13,      			 9.253753324779779E-13,      			 2.267636001476696E-12,      			 5.3880460095836855E-12,      			 1.2431632654852931E-11      	};        	GaussianFitter g =      		new GaussianFitter(new LevenbergMarquardtOptimizer());     	     	for (int index = 0; index < 27; index++)     	{     		g.addObservedPoint(index, observations[index]);     	}        	g.fit();  Results in:  org.apache.commons.math.exception.NotStrictlyPositiveException: -1.277 is smaller than, or equal to, the minimum (0) 	at org.apache.commons.math.analysis.function.Gaussian Parametric.validateParameters(Gaussian.java:184) 	at org.apache.commons.math.analysis.function.Gaussian Parametric.value(Gaussian.java:129)   I'm guessing the initial guess for sigma is off.$$patch1-math-26a61077_Developer_PatchNaturalnessYe$$Add missing import. Added a method to fit Gaussian function to the observed points .. Updated comment .. Undo a typo .. Removed an unused variable .. Added missing closing Javadoc. Added missing javadoc. Added missing javadoc. Updated GaussianFitter . isBetween ( ) documentation. Fix Javadoc nag. $$1
math-4aa4c6d3$$getKernel fails for buckets with only multiple instances of the same value in random.EmpiricalDistribution$$After loading a set of values into an EmpericalDistribution, assume that there's a case where a single bin ONLY contains multiple instances of the same value.  In this case the standard deviation will equal zero.  This will fail when getKernel attempts to create a NormalDistribution.  The other case where stddev=0 is when there is only a single value in the bin, and this is handled by returning a ConstantRealDistribution rather than a NormalDistrbution.  See: https://issues.apache.org/jira/browse/MATH-984$$patch1-math-4aa4c6d3_Developer_PatchNaturalnessYe$$EmpiricalDistribution has a zero variance in size. $$1
math-09fe956a$$ResizableDoubleArray does not work with double array of size 1$$When attempting to create a ResizableDoubleArray with an array of a single value (e.g. {4.0}), the constructor creates an internal array with 16 entries that are all 0.0  Bug looks like it might be on line 414 of ResizableDoubleArray.java:          if (data != null && data.length > 1) {$$patch1-math-09fe956a_Developer_PatchNaturalnessYe$$Fixed double array resize. $$1
math-cc4ab51e$$BinomialDistribution deals with degenerate cases incorrectly$$The following calculation returns false results:  {{new BinomialDistribution(0, 0.01).logProbability(0)}}  It evaluates to Double.NaN when it should be 0 (cf., for example, "dbinom(0, 0, 0.01, log=T)" in R).  I attach a patch dealing with the problem. The patch also adds a test for this bug.$$patch1-math-cc4ab51e_Developer_PatchNaturalnessYe$$Add log probability method if ( numberOfTrials == 0 ) {. $$1
math-8f35fcb8$$UknownParameterException message prints {0} instead of parameter name$$The constructor for UnknownParameterException stores the parameter name internally but does not forward it to the base class which creates the error message.$$patch1-math-8f35fcb8_Developer_PatchNaturalnessYe$$MissingParameterException constructor should use the correct message .. $$1
math-b55e0206$$Wide bounds to CMAESOptimizer result in NaN parameters passed to fitness function$$If you give large values as lower/upper bounds (for example -Double.MAX_VALUE as a lower bound), the optimizer can call the fitness function with parameters set to NaN.  My guess is this is due to FitnessFunction.encode/decode generating NaN when normalizing/denormalizing parameters.  For example, if the difference between the lower and upper bound is greater than Double.MAX_VALUE, encode could divide infinity by infinity.$$patch1-math-b55e0206_Developer_PatchNaturalnessYe$$Removed unused imports. add comment on CMAESOptimizer. Add CMAJO 500 - Limit the maximum number of decimal places to 2 decimal .. $$1
math-e91d0f05$$Precision.round() returns different results when provided negative zero as double or float$$Precision.round(-0.0d, x) = 0.0 Precision.round(-0.0f, x) = -0.0  After discussion on the mailinglist, the result should always be -0.0.$$patch1-math-e91d0f05_Developer_PatchNaturalnessYe$$MATH - 1089 - negative values rounded to zero should result in negative zero. $$1
math-aff82362$$Stack overflow in Beta.regularizedBeta$$In org.apache.commons.math3.special.Beta.regularizedBeta(double,double,double,double,int), the case   } else if (x > (a + 1.0) / (a + b + 2.0)) {       ret = 1.0 - regularizedBeta(1.0 - x, b, a, epsilon, maxIterations); }   is prone to infinite recursion: If x is approximately the tested value, then 1-x is approximately the tested value in the recursion. Thus, due to loss of precision after the subtraction, this condition can be true for the recursive call as well.  Example: double x= Double.longBitsToDouble(4597303555101269224L); double a= Double.longBitsToDouble(4634227472812299606L); double b = Double.longBitsToDouble(4642050131540049920L); System.out.println(x > (a + 1.0) / (a + b + 2.0)); System.out.println(1-x>(b + 1.0) / (b + a + 2.0)); System.out.println(1-(1-x)>(a + 1.0) / (a + b + 2.0));  Possible solution: change the condition to x > (a + 1.0) / (a + b + 2.0) && 1-x<=(b + 1.0) / (b + a + 2.0)$$patch1-math-aff82362_Developer_PatchNaturalnessYe$$Fix erroneous beta function .. $$1
math-b07ecae3$$new multivariate vector optimizers cannot be used with large number of weights$$When using the Weigth class to pass a large number of weights to multivariate vector optimizers, an nxn full matrix is created (and copied) when a n elements vector is used. This exhausts memory when n is large.  This happens for example when using curve fitters (even simple curve fitters like polynomial ones for low degree) with large number of points. I encountered this with curve fitting on 41200 points, which created a matrix with 1.7 billion elements.$$patch1-math-b07ecae3_Developer_PatchNaturalnessYe$$Fix double import. Fix double constructor of Weight. Add missing import. Fix NPE in AbstractLeastSquaresOptimizer . squareRoot ( ). $$1
math-f83bbc1d$$LevenbergMarquardtOptimizer reports 0 iterations$$The method LevenbergMarquardtOptimizer.getIterations() does not report the correct number of iterations; It always returns 0. A quick look at the code shows that only SimplexOptimizer calls BaseOptimizer.incrementEvaluationsCount()  I've put a test case below. Notice how the evaluations count is correctly incremented, but the iterations count is not.  {noformat}     @Test     public void testGetIterations() {         // setup         LevenbergMarquardtOptimizer otim = new LevenbergMarquardtOptimizer();          // action         otim.optimize(new MaxEval(100), new Target(new double[] { 1 }),                 new Weight(new double[] { 1 }), new InitialGuess(                         new double[] { 3 }), new ModelFunction(                         new MultivariateVectorFunction() {                             @Override                             public double[] value(double[] point)                                     throws IllegalArgumentException {                                 return new double[] { FastMath.pow(point[0], 4) };                             }                         }), new ModelFunctionJacobian(                         new MultivariateMatrixFunction() {                             @Override                             public double[][] value(double[] point)                                     throws IllegalArgumentException {                                 return new double[][] { { 0.25 * FastMath.pow(                                         point[0], 3) } };                             }                         }));          // verify         assertThat(otim.getEvaluations(), greaterThan(1));         assertThat(otim.getIterations(), greaterThan(1));     }  {noformat}$$patch1-math-f83bbc1d_Developer_PatchNaturalnessYe$$Fix a bug in SimplexOptimizer with max iterations. Added incrementIterationCount ( ) to the SimplexOptimizer . java. Fix CMAESOptimizer loop .. Increment the number of iterations of the GaussNewtonOptimizer via incrementIterationCount ( ). Fix a bug in GaussNewtonOptimizer . converged ( ) .. Fix double loop. Fix a typo in the LevenbergMarquardtOptimizer class. Using the latestIterationCount. Added missing if (. Removed unused variable .. Fix PowellOptimizer ' s converged method .. Set max iterations to Integer . MAX_VALUE for BaseOptimizer. $$1
math-b01fcc31$$NullPointerException in SimplexTableau.initialize$$SimplexTableau throws a NullPointerException when no solution can be found instead of a NoFeasibleSolutionException  Here is the code that causes the NullPointerException:  LinearObjectiveFunction f = new LinearObjectiveFunction(new double[] { 1, 5 }, 0 ); Collection<LinearConstraint> constraints = new ArrayList<LinearConstraint>(); constraints.add(new LinearConstraint(new double[] { 2, 0 }, Relationship.GEQ, -1.0));  RealPointValuePair solution = new SimplexSolver().optimize(f, constraints, GoalType.MINIMIZE, true);  Note: Tested both with Apache Commons Math 2.0 release and SVN trunk$$patch1-math-b01fcc31_Developer_PatchNaturalnessYe$$Fix warning. Fix a bug in SimplexTableau where the constraints were not normalized .. Improved the size of the tableau test .. Fix the name of the method .. $$1
math-ebadb558$$ListPopulation Iterator allows you to remove chromosomes from the population.$$Calling the iterator method of ListPopulation returns an iterator of the protected modifiable list. Before returning the iterator we should wrap it in an unmodifiable list.$$patch1-math-ebadb558_Developer_PatchNaturalnessYe$$Fix typo in ListPopulation . iterator ( ). $$1
math-118e94b5$$Quaternion not normalized after construction$$The use of the Rotation(Vector3D u1,Vector3D u2,Vector3D v1,Vector3D v2) constructor with normalized angle can apparently lead to un-normalized quaternion. This case appeared to me with the following data : u1 = (0.9999988431610581, -0.0015210774290851095, 0.0) u2 = (0.0, 0.0, 1.0) and  v1 = (0.9999999999999999, 0.0, 0.0) v2 = (0.0, 0.0, -1.0)  This lead to the following quaternion : q0 = 225783.35177064248 q1 = 0.0 q2 = 0.0 q3 = -3.3684446110762543E-9  I was expecting to have a normalized quaternion, as input vector's are normalized. Does the quaternion shouldn't be normalized ? I've joined the corresponding piece of code as JUnit Test case$$patch1-math-118e94b5_Developer_PatchNaturalnessYe$$added missing import. Fix an issue with mat2quat .. Fix a warning. $$1
math-4c4b3e2e$$Overflow checks in Fraction multiply(int) / divide(int)$$The member methods multiply(int) / divide(int) in the class org.apache.commons.math3.fraction.Fraction do not have overflow checks.  {code:java} return new Fraction(numerator * i, denominator); {code}  should be  {code:java} return new Fraction(ArithmeticUtils.mulAndCheck(numerator, i), denominator); {code}  or, considering the case gcd(i, denominator) > 1,  {code:java} return multiply(new Fraction(i)); {code}$$patch1-math-4c4b3e2e_Developer_PatchNaturalnessYe$$Fix DivisionByPointException in Fraction class. Fix divide ( ). $$1
math-5b9302d5$$Dfp Dfp.multiply(int x) does not comply with the general contract FieldElement.multiply(int n)$$In class {{org.apache.commons.math3.Dfp}},  the method {{multiply(int n)}} is limited to {{0 <= n <= 9999}}. This is not consistent with the general contract of {{FieldElement.multiply(int n)}}, where there should be no limitation on the values of {{n}}.$$patch1-math-5b9302d5_Developer_PatchNaturalnessYe$$Fix speed advantages in Dfp multiply ( ). $$1
math-9c8bb934$$RandomDataImpl.nextInt does not distribute uniformly for negative lower bound$$When using the RandomDataImpl.nextInt function to get a uniform sample in a [lower, upper] interval, when the lower value is less than zero, the output is not uniformly distributed, as the lowest value is practically never returned.  See the attached NextIntUniformTest.java file. It uses a [-3, 5] interval. For several values between 0 and 1, testNextIntUniform1 prints the return value of RandomDataImpl.nextInt (as double and as int). We see that -2 through 5 are returned several times. The -3 value however, is only returned for 0.0, and is thus under-respresented in the integer samples. The output of test method testNextIntUniform2 also clearly shows that value -3 is never sampled.$$patch1-math-9c8bb934_Developer_PatchNaturalnessYe$$Add Missing Missing Missing Missing Missing Missing Missing Missing Missing Missing Missing Missing Missing Missing Missing Missing Missing Missing. Add NAN_NOT_ALLOWED back. Add missing import. FastMath . floor ( r ) ==> int. FastMath . floor ( r ) == r ; we could use some of it. Fix SecRan restrictions. FastMath . floor ( double ) doesn ' t scale well in the PR. fixed NAN / inf inf in RandomDataImpl . nextUniform ( ). fixed a bug in the RandomDataImpl class. $$1
math-471e6b07$$Digamma calculation produces SOE on NaN argument$$Digamma doesn't work particularly well with NaNs.  How to reproduce: call Gamma.digamma(Double.NaN)  Expected outcome: returns NaN or throws a meaningful exception  Real outcome: crashes with StackOverflowException, as digamma enters infinite recursion.$$patch1-math-471e6b07_Developer_PatchNaturalnessYe$$fix digamma ( x ). fix a warning. $$1
math-f656676e$$Negative value with restrictNonNegative$$Problem: commons-math-2.2 SimplexSolver.  A variable with 0 coefficient may be assigned a negative value nevertheless restrictToNonnegative flag in call: SimplexSolver.optimize(function, constraints, GoalType.MINIMIZE, true);  Function 1 * x + 1 * y + 0  Constraints: 1 * x + 0 * y = 1  Result: x = 1; y = -1;  Probably variables with 0 coefficients are omitted at some point of computation and because of that the restrictions do not affect their values.$$patch1-math-f656676e_Developer_PatchNaturalnessYe$$Fixed a bug in the objective function coefficients. $$1
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch1-math-a06a1584_Developer_PatchNaturalnessYe$$Fixed a typo in PolyhedronsSet . firstIntersection. Fixed a typo in PolyhedronsSet javadoc. reduce error in PolyhedronsSet .. Fixed a bug in PolyhedronsSet where the plane doesn ' t collide. $$1
math-3a15d8ce$$AbstractRealVector.sparseIterator fails when vector has exactly one non-zero entry$$The following program: === import java.util.Iterator; import org.apache.commons.math.linear.*;  public class SparseIteratorTester {     public static void main(String[] args) {         double vdata[] = { 0.0, 1.0, 0.0 };         RealVector v = new ArrayRealVector(vdata);         Iterator<RealVector.Entry> iter = v.sparseIterator();         while(iter.hasNext()) {             RealVector.Entry entry = iter.next();             System.out.printf("%d: %f\n", entry.getIndex(), entry.getValue());         }        }        }  === generates this output:  1: 1.000000 Exception in thread "main" java.lang.ArrayIndexOutOfBoundsException: -1 	at org.apache.commons.math.linear.ArrayRealVector.getEntry(ArrayRealVector.java:995) 	at org.apache.commons.math.linear.AbstractRealVector EntryImpl.getValue(AbstractRealVector.java:850) 	at test.SparseIteratorTester.main(SparseIteratorTester.java:13) ===  This patch fixes it, and simplifies AbstractRealVector.SparseEntryIterator  (sorry, i don't see any form entry for attaching a file) === Index: src/main/java/org/apache/commons/math/linear/AbstractRealVector.java =================================================================== --- src/main/java/org/apache/commons/math/linear/AbstractRealVector.java	(revision 936985) +++ src/main/java/org/apache/commons/math/linear/AbstractRealVector.java	(working copy) @@ -18,6 +18,7 @@  package org.apache.commons.math.linear;    import java.util.Iterator; +import java.util.NoSuchElementException;    import org.apache.commons.math.FunctionEvaluationException;  import org.apache.commons.math.MathRuntimeException; @@ -875,40 +876,25 @@          /** Dimension of the vector. */          private final int dim;   -        /** Temporary entry (reused on each call to {@link #next()}. */ -        private EntryImpl tmp = new EntryImpl(); - -        /** Current entry. */ +        /** Last entry returned by #next(). */          private EntryImpl current;   -        /** Next entry. */ +        /** Next entry for #next() to return. */          private EntryImpl next;            /** Simple constructor. */          protected SparseEntryIterator() {              dim = getDimension();              current = new EntryImpl(); -            if (current.getValue() == 0) { -                advance(current); -            } -            if(current.getIndex() >= 0){ -                // There is at least one non-zero entry -                next = new EntryImpl(); -                next.setIndex(current.getIndex()); +            next = new EntryImpl(); +            if(next.getValue() == 0)                  advance(next); -            } else { -                // The vector consists of only zero entries, so deny having a next -                current = null; -            }          }   -        /** Advance an entry up to the next non null one. +        /** Advance an entry up to the next nonzero value.           * @param e entry to advance           */          protected void advance(EntryImpl e) { -            if (e == null) { -                return; -            }              do {                  e.setIndex(e.getIndex() + 1);              } while (e.getIndex() < dim && e.getValue() == 0); @@ -919,22 +905,17 @@            /** {@inheritDoc} */          public boolean hasNext() { -            return current != null; +            return next.getIndex() >= 0;          }            /** {@inheritDoc} */          public Entry next() { -            tmp.setIndex(current.getIndex()); -            if (next != null) { -                current.setIndex(next.getIndex()); -                advance(next); -                if (next.getIndex() < 0) { -                    next = null; -                } -            } else { -                current = null; -            } -            return tmp; +            int index = next.getIndex(); +            if(index < 0) +                throw new NoSuchElementException(); +            current.setIndex(index); +            advance(next); +            return current;          }            /** {@inheritDoc} */$$patch1-math-3a15d8ce_Developer_PatchNaturalnessYe$$Add missing import. Move to the next non - zero entry. Increasing loop size for AbstractRealVector . next ( ). $$1
math-2a9cbbab$$Polygon difference produces erronious results in some cases$$The 2D polygon difference method is returning incorrect results.  Below is a test case of subtracting two polygons (Sorry, this is the simplest case that I could find that duplicates the problem).    There are three problems with the result. The first is that the first point of the first set of vertices is null (and the first point of the second set is also null).  The second is that, even if the first null points are ignored,  the returned polygon is not the correct result. The first and last points are way off, and the remaining points do not match the original polygon boundaries.  Additionally, there are two holes that are returned in the results.  This subtraction case should not have holes.  {code:title="Complex Polygon Difference Test"} public void testComplexDifference() {         Vector2D[][] vertices1 = new Vector2D[][] {             new Vector2D[] {                     new Vector2D( 90.08714908223715,  38.370299337260235),                     new Vector2D( 90.08709517675004,  38.3702895991413),                     new Vector2D( 90.08401538704919,  38.368849330127944),                     new Vector2D( 90.08258210430711,  38.367634558585564),                     new Vector2D( 90.08251455106665,  38.36763409247078),                     new Vector2D( 90.08106599752608,  38.36761621664249),                     new Vector2D( 90.08249585300035,  38.36753627557965),                     new Vector2D( 90.09075743352184,  38.35914647644972),                     new Vector2D( 90.09099945896571,  38.35896264724079),                     new Vector2D( 90.09269383800086,  38.34595756121246),                     new Vector2D( 90.09638631543191,  38.3457988093121),                     new Vector2D( 90.09666417351019,  38.34523360999418),                     new Vector2D( 90.1297082145872,  38.337670454923625),                     new Vector2D( 90.12971687748956,  38.337669827794684),                     new Vector2D( 90.1240820219179,  38.34328502001131),                     new Vector2D( 90.13084259656404,  38.34017811765017),                     new Vector2D( 90.13378567942857,  38.33860579180606),                     new Vector2D( 90.13519557833206,  38.33621054663689),                     new Vector2D( 90.13545616732307,  38.33614965452864),                     new Vector2D( 90.13553111202748,  38.33613962818305),                     new Vector2D( 90.1356903436448,  38.33610227127048),                     new Vector2D( 90.13576283227428,  38.33609255422783),                     new Vector2D( 90.13595870833188,  38.33604606376991),                     new Vector2D( 90.1361556630693,  38.3360024198866),                     new Vector2D( 90.13622408795709,  38.335987048115726),                     new Vector2D( 90.13696189099994,  38.33581914328681),                     new Vector2D( 90.13746655304897,  38.33616706665265),                     new Vector2D( 90.13845973716064,  38.33650776167099),                     new Vector2D( 90.13950901827667,  38.3368469456463),                     new Vector2D( 90.14393814424852,  38.337591835857495),                     new Vector2D( 90.14483839716831,  38.337076122362475),                     new Vector2D( 90.14565474433601,  38.33769000964429),                     new Vector2D( 90.14569421179482,  38.3377117256905),                     new Vector2D( 90.14577067124333,  38.33770883625908),                     new Vector2D( 90.14600350631684,  38.337714326520995),                     new Vector2D( 90.14600355139731,  38.33771435193319),                     new Vector2D( 90.14600369112401,  38.33771443882085),                     new Vector2D( 90.14600382486884,  38.33771453466096),                     new Vector2D( 90.14600395205912,  38.33771463904344),                     new Vector2D( 90.14600407214999,  38.337714751520764),                     new Vector2D( 90.14600418462749,  38.337714871611695),                     new Vector2D( 90.14600422249327,  38.337714915811034),                     new Vector2D( 90.14867838361471,  38.34113888210675),                     new Vector2D( 90.14923750157374,  38.341582537502575),                     new Vector2D( 90.14877083250991,  38.34160685841391),                     new Vector2D( 90.14816667319519,  38.34244232585684),                     new Vector2D( 90.14797696744586,  38.34248455284745),                     new Vector2D( 90.14484318014337,  38.34385573215269),                     new Vector2D( 90.14477919958296,  38.3453797747614),                     new Vector2D( 90.14202393306448,  38.34464324839456),                     new Vector2D( 90.14198920640195,  38.344651155237216),                     new Vector2D( 90.14155207025175,  38.34486424263724),                     new Vector2D( 90.1415196143314,  38.344871730519),                     new Vector2D( 90.14128611910814,  38.34500196593859),                     new Vector2D( 90.14047850603913,  38.34600084496253),                     new Vector2D( 90.14045907000337,  38.34601860032171),                     new Vector2D( 90.14039496493928,  38.346223030432384),                     new Vector2D( 90.14037626063737,  38.346240203360026),                     new Vector2D( 90.14030005823724,  38.34646920000705),                     new Vector2D( 90.13799164754806,  38.34903093011013),                     new Vector2D( 90.11045289492762,  38.36801537312368),                     new Vector2D( 90.10871471476526,  38.36878044144294),                     new Vector2D( 90.10424901707671,  38.374300101757),                     new Vector2D( 90.10263482039932,  38.37310041316073),                     new Vector2D( 90.09834601753448,  38.373615053823414),                     new Vector2D( 90.0979455456843,  38.373578376172475),                     new Vector2D( 90.09086514328669,  38.37527884194668),                     new Vector2D( 90.09084931407364,  38.37590801712463),                     new Vector2D( 90.09081227075944,  38.37526295920463),                     new Vector2D( 90.09081378927135,  38.375193883266434)             }         };         PolygonsSet set1 = buildSet(vertices1);          Vector2D[][] vertices2 = new Vector2D[][] {             new Vector2D[] {                     new Vector2D( 90.13067558880044,  38.36977255037573),                     new Vector2D( 90.12907570488,  38.36817308242706),                     new Vector2D( 90.1342774136516,  38.356886880294724),                     new Vector2D( 90.13090330629757,  38.34664392676211),                     new Vector2D( 90.13078571364593,  38.344904617518466),                     new Vector2D( 90.1315602208914,  38.3447185040846),                     new Vector2D( 90.1316336226821,  38.34470643148342),                     new Vector2D( 90.134020944832,  38.340936644972885),                     new Vector2D( 90.13912536387306,  38.335497255122334),                     new Vector2D( 90.1396178806582,  38.334878075552126),                     new Vector2D( 90.14083049696671,  38.33316530644106),                     new Vector2D( 90.14145252901329,  38.33152722916191),                     new Vector2D( 90.1404779335565,  38.32863516047786),                     new Vector2D( 90.14282712131586,  38.327504432532066),                     new Vector2D( 90.14616669875488,  38.3237354115015),                     new Vector2D( 90.14860976050608,  38.315714862457924),                     new Vector2D( 90.14999277782437,  38.3164932507504),                     new Vector2D( 90.15005207194997,  38.316534677663356),                     new Vector2D( 90.15508513859612,  38.31878731691609),                     new Vector2D( 90.15919938519221,  38.31852743183782),                     new Vector2D( 90.16093758658837,  38.31880662005153),                     new Vector2D( 90.16099420184912,  38.318825953291594),                     new Vector2D( 90.1665411125756,  38.31859497874757),                     new Vector2D( 90.16999653861313,  38.32505772048029),                     new Vector2D( 90.17475243391698,  38.32594398441148),                     new Vector2D( 90.17940844844992,  38.327427213761325),                     new Vector2D( 90.20951909541378,  38.330616833491774),                     new Vector2D( 90.2155400467941,  38.331746223670336),                     new Vector2D( 90.21559881391778,  38.33175551425302),                     new Vector2D( 90.21916646426041,  38.332584299620805),                     new Vector2D( 90.23863749852285,  38.34778978875795),                     new Vector2D( 90.25459855175802,  38.357790570608984),                     new Vector2D( 90.25964298227257,  38.356918010203174),                     new Vector2D( 90.26024593994703,  38.361692743151366),                     new Vector2D( 90.26146187570015,  38.36311080550837),                     new Vector2D( 90.26614159359622,  38.36510808579902),                     new Vector2D( 90.26621342936448,  38.36507942500333),                     new Vector2D( 90.26652190211962,  38.36494042196722),                     new Vector2D( 90.26621240678867,  38.365113172030874),                     new Vector2D( 90.26614057102057,  38.365141832826794),                     new Vector2D( 90.26380080055299,  38.3660381760273),                     new Vector2D( 90.26315345241,  38.36670658276421),                     new Vector2D( 90.26251574942881,  38.367490323488084),                     new Vector2D( 90.26247873448426,  38.36755266444749),                     new Vector2D( 90.26234628016698,  38.36787989125406),                     new Vector2D( 90.26214559424784,  38.36945909356126),                     new Vector2D( 90.25861728442555,  38.37200753430875),                     new Vector2D( 90.23905557537864,  38.375405314295904),                     new Vector2D( 90.22517251874075,  38.38984691662256),                     new Vector2D( 90.22549955153215,  38.3911564273979),                     new Vector2D( 90.22434386063355,  38.391476432092134),                     new Vector2D( 90.22147729457276,  38.39134652252034),                     new Vector2D( 90.22142070120117,  38.391349167741964),                     new Vector2D( 90.20665060751588,  38.39475580900313),                     new Vector2D( 90.20042268367109,  38.39842558622888),                     new Vector2D( 90.17423771242085,  38.402727751805344),                     new Vector2D( 90.16756796257476,  38.40913898597597),                     new Vector2D( 90.16728283954308,  38.411255399912875),                     new Vector2D( 90.16703538220418,  38.41136059866693),                     new Vector2D( 90.16725865657685,  38.41013618805954),                     new Vector2D( 90.16746107640665,  38.40902614307544),                     new Vector2D( 90.16122795307462,  38.39773101873203)             }         };         PolygonsSet set2 = buildSet(vertices2);         PolygonsSet set  = (PolygonsSet) new RegionFactory<Euclidean2D>().difference(set1.copySelf(),                set2.copySelf());          Vector2D[][] verticies = set.getVertices();         Assert.assertTrue(verticies[0][0] != null);         Assert.assertEquals(1, verticies.length);     } {code}$$patch1-math-2a9cbbab_Developer_PatchNaturalnessYe$$added note on PolygonsSet .. fixed edge label positionning. Fixed a bug in PolygonsSet . insertVertices ( ). Fix edge case. added missing fields. Fixed method name. $$1
math-ac597cc1$$"BrentOptimizer" not always reporting the best point$${{BrentOptimizer}} (package "o.a.c.m.optimization.univariate") does not check that the point it is going to return is indeed the best one it has encountered. Indeed, the last evaluated point might be slightly worse than the one before last.$$patch1-math-ac597cc1_Developer_PatchNaturalnessYe$$Fix a typo in the Python code. Added a note about the best point encountered so far. Fix a redundant line. Added missing closing parenthesis in BrentOptimizer. Fix error in BrentOptimizer . best ( ). Fix value compositing. $$1
math-7980a242$$Incomplete reinitialization with some events handling$$I get a bug with event handling: I track 2 events that occur in the same step, when the first one is accepted, it resets the state but the reinitialization is not complete and the second one becomes unable to find its way. I can't give my context, which is rather large, but I tried a patch that works for me, unfortunately it breaks the unit tests.$$patch1-math-7980a242_Developer_PatchNaturalnessYe$$removed unused import. removed unused variable. Fix double - packaging. Fix an issue with the intermittent double - callback. $$1
math-19c1c3bb$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch1-math-19c1c3bb_Developer_PatchNaturalnessYe$$removed unused import. Add missing factors , added Euclidean3D .. Ease in Added minor value of SphereGenerator . minor ( ) .. removed debug line. Added a TODO. Updated imports .. EnclosingBall < Euclidean2D , Vector2D > .. $$1
math-645d642b$$DormandPrince853 integrator leads to revisiting of state events$$See the attached ReappearingEventTest.java. It has two unit tests, which use either the DormandPrince853 or the GraggBulirschStoer integrator, on the same ODE problem. It is a problem starting at time 6.0, with 7 variables, and 1 state event. The state event was previously detected at time 6.0, which is why I start there now. I provide and end time of 10.0. Since I start at the state event, I expect to integrate all the way to the end (10.0). For the GraggBulirschStoer this is what happens (see attached ReappearingEventTest.out). For the DormandPrince853Integerator, it detects a state event and stops integration at 6.000000000000002.  I think the problem becomes clear by looking at the output in ReappearingEventTest.out, in particular these lines:  {noformat} computeDerivatives: t=6.0                  y=[2.0                 , 2.0                 , 2.0                 , 4.0                 , 2.0                 , 7.0                 , 15.0                ] (...) g                 : t=6.0                  y=[1.9999999999999996  , 1.9999999999999996  , 1.9999999999999996  , 4.0                 , 1.9999999999999996  , 7.0                 , 14.999999999999998  ] (...) final result      : t=6.000000000000002    y=[2.0000000000000013  , 2.0000000000000013  , 2.0000000000000013  , 4.000000000000002   , 2.0000000000000013  , 7.000000000000002   , 15.0                ] {noformat}  The initial value of the last variable in y, the one that the state event refers to, is 15.0. However, the first time it is given to the g function, the value is 14.999999999999998. This value is less than 15, and more importantly, it is a value from the past (as all functions are increasing), *before* the state event. This makes that the state event re-appears immediately, and integration stops at 6.000000000000002 because of the detected state event.  I find it puzzling that for the DormandPrince853Integerator the y array that is given to the first evaluation of the g function, has different values than the y array that is the input to the problem. For GraggBulirschStoer is can be seen that the y arrays have identical values.$$patch1-math-645d642b_Developer_PatchNaturalnessYe$$cloned runge kutta integator. Fix an issue with the coherency of runge kutta integration .. Fix compile warning. clone the array to prevent the copy of the array .. Fix minor eclipse compile warning. Remove note about changed serialVersionUID. Removed severed conflicts in DormandPrince54StepInterpolator. Reduced the number of times that a class can be loaded from the load .. Fix interpolated state changes. Fix a warning. Fix Eclipse warning. Add previousState to nonstiff step interpolator. Fix NPE in RungeKuttaStepInterpolator. Fix NPE in RungeKuttaStepInterpolator . reset ( ) .. Fix a warning. Fix writeDouble. Fix readDouble ( ) overload in RungeKuttaStepInterpolator. Fix compile warning. $$1
math-86545dab$$Fraction specified with maxDenominator and a value very close to a simple fraction should not throw an overflow exception$$An overflow exception is thrown when a Fraction is initialized with a maxDenominator from a double that is very close to a simple fraction.  For example:  double d = 0.5000000001; Fraction f = new Fraction(d, 10);  Patch with unit test on way.$$patch1-math-86545dab_Developer_PatchNaturalnessYe$$Add the DEFAULT_EPSILON constant to Fraction ( ) .. Default epsilon of Fraction constructor .. Fix typo in Fraction class. Don ' t limit overflow by the max denominator .. Don ' t limit overflow by the max denominator .. $$1
math-4080feff$$MonotoneChain handling of collinear points drops low points in a near-column$$This code {code} val points = List(   new Vector2D(     16.078200000000184,     -36.52519999989808   ),   new Vector2D(     19.164300000000186,     -36.52519999989808   ),   new Vector2D(     19.1643,     -25.28136477910407   ),   new Vector2D(     19.1643,     -17.678400000004157   ) ) new hull.MonotoneChain().generate(points.asJava) {code}  results in the exception: {code} org.apache.commons.math3.exception.ConvergenceException: illegal state: convergence failed 	at org.apache.commons.math3.geometry.euclidean.twod.hull.AbstractConvexHullGenerator2D.generate(AbstractConvexHullGenerator2D.java:106) 	at org.apache.commons.math3.geometry.euclidean.twod.hull.MonotoneChain.generate(MonotoneChain.java:50) 	at .<init>(<console>:13) 	at .<clinit>(<console>) 	at .<init>(<console>:11) 	at .<clinit>(<console>) 	at  print(<console>) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) 	at java.lang.reflect.Method.invoke(Method.java:597) 	at scala.tools.nsc.interpreter.IMain ReadEvalPrint.call(IMain.scala:704) 	at scala.tools.nsc.interpreter.IMain Request  anonfun 14.apply(IMain.scala:920) 	at scala.tools.nsc.interpreter.Line  anonfun 1.apply mcV sp(Line.scala:43) 	at scala.tools.nsc.io.package  anon 2.run(package.scala:25) 	at java.lang.Thread.run(Thread.java:662) {code}  This will be tricky to fix. Not only is the point (19.164300000000186, -36.52519999989808) is being dropped incorrectly, but any point dropped in one hull risks creating a kink when combined with the other hull.$$patch1-math-4080feff_Developer_PatchNaturalnessYe$$added missing import. fixed collinear points sort by x axis. removed unused imports. throw exception if vertices are not convex ( this is what the real euclidean type. Fixed a small bug in ConvexHull2D .. Changed sign bit for horizontal lines. $$1
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch1-math-2a6c6409_Developer_PatchNaturalnessYe$$Added a note about potential NPE in PolyhedronsSet constructor. Euclidean3D .. $$1
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch1-math-faf99727_Developer_PatchNaturalnessYe$$Fix NPE in enclose ( ) .. removed infinite loop. Fix bug in welzlEncloser. Fixed method name. removed warning. Remove unused imports. Fix encloser method to accept list as input. $$1
math-56434517$$multistep integrator start failure triggers NPE$$Multistep ODE integrators like Adams-Bashforth and Adams-Moulton require a starter procedure. If the starter integrator is not configured properly, it will not create the necessary number of initial points and the multistep integrator will not be initialized correctly. This results in NullPointErException when the scaling array is referenced later on.  The following test case (with an intentionally wrong starter configuration) shows the problem.  {code} @Test public void testStartFailure() {       TestProblem1 pb = new TestProblem1();       double minStep = 0.0001 * (pb.getFinalTime() - pb.getInitialTime());       double maxStep = pb.getFinalTime() - pb.getInitialTime();       double scalAbsoluteTolerance = 1.0e-6;       double scalRelativeTolerance = 1.0e-7;        MultistepIntegrator integ =           new AdamsBashforthIntegrator(4, minStep, maxStep,                                                             scalAbsoluteTolerance,                                                             scalRelativeTolerance);       integ.setStarterIntegrator(new DormandPrince853Integrator(0.2 * (pb.getFinalTime() - pb.getInitialTime()),                                                                 pb.getFinalTime() - pb.getInitialTime(),                                                                 0.1, 0.1));       TestProblemHandler handler = new TestProblemHandler(pb, integ);       integ.addStepHandler(handler);       integ.integrate(pb,                              pb.getInitialTime(), pb.getInitialState(),                              pb.getFinalTime(), new double[pb.getDimension()]);      } {code}  Failure to start the integrator should be detected and an appropriate exception should be triggered.$$patch1-math-56434517_Developer_PatchNaturalnessYe$$Added multistep localization message. Add missing import. Added a throw. $$1
math-9aabf587$$Use analytical function for UniformRealDistribution.inverseCumulativeProbability$$The inverse CDF is currently solved by a root finding function. It would be much simpler (and faster) to use the analytical expression. This would save the user from having to set the inverseCumAccuracy correctly.  I've attached a patch that implements this.$$patch1-math-9aabf587_Developer_PatchNaturalnessYe$$Updated imports. Deprecated default inverse cumulative probability accuracy .. Remove unused variable .. Add default constructor to UniformRealDistribution. Deprecated UniformRealDistribution constructor .. Add inverse CDF to UniformRealDistribution constructor .. remove superfluous line. Add inverseCumulativeProbability ( ) , to prevent intermittent exception. $$1
math-38983e82$$SimplexSolver not working as expected 2$$SimplexSolver didn't find the optimal solution.  Program for Lpsolve: ===================== /* Objective function */ max: 7 a 3 b;  /* Constraints */ R1: +3 a -5 c <= 0; R2: +2 a -5 d <= 0; R3: +2 b -5 c <= 0; R4: +3 b -5 d <= 0; R5: +3 a +2 b <= 5; R6: +2 a +3 b <= 5;  /* Variable bounds */ a <= 1; b <= 1; ===================== Results(correct): a = 1, b = 1, value = 10   Program for SimplexSolve: ===================== LinearObjectiveFunction kritFcia = new LinearObjectiveFunction(new double[]{7, 3, 0, 0}, 0); Collection<LinearConstraint> podmienky = new ArrayList<LinearConstraint>(); podmienky.add(new LinearConstraint(new double[]{1, 0, 0, 0}, Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[]{0, 1, 0, 0}, Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[]{3, 0, -5, 0}, Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[]{2, 0, 0, -5}, Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[]{0, 2, -5, 0}, Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[]{0, 3, 0, -5}, Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[]{3, 2, 0, 0}, Relationship.LEQ, 5)); podmienky.add(new LinearConstraint(new double[]{2, 3, 0, 0}, Relationship.LEQ, 5)); SimplexSolver solver = new SimplexSolver(); RealPointValuePair result = solver.optimize(kritFcia, podmienky, GoalType.MAXIMIZE, true); ===================== Results(incorrect): a = 1, b = 0.5, value = 8.5  P.S. I used the latest software from the repository (including MATH-286 fix).$$patch1-math-38983e82_Developer_PatchNaturalnessYe$$Fixed epsilon .. $$1
math-8e5867ed$$Incorrect rounding of float$$package org.apache.commons.math3.util  example of usage of round functions of Precision class:  Precision.round(0.0f, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float)0.0, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float) 0.0, 2) = 0.0 Precision.round(0.0, 2, BigDecimal.ROUND_UP) = 0.0  Seems the reason is usage of extending float to double inside round functions and getting influence of memory trash as value.  I think, same problem will be found at usage of other round modes.$$patch36-math-8e5867ed_GenProg_PatchNaturalnessYe$$Fix a small bug in Precision . nextAfter ( double , double ). Fix NPE. FastMath . nextAfter ( unscaled , Double . NEGATIVE_INFINITY ) doesn ' t work. FastMath . ceil ( unscaled ) doesn ' t fix the rounded angles .. $$0
math-8e5867ed$$Incorrect rounding of float$$package org.apache.commons.math3.util  example of usage of round functions of Precision class:  Precision.round(0.0f, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float)0.0, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float) 0.0, 2) = 0.0 Precision.round(0.0, 2, BigDecimal.ROUND_UP) = 0.0  Seems the reason is usage of extending float to double inside round functions and getting influence of memory trash as value.  I think, same problem will be found at usage of other round modes.$$patch31-math-8e5867ed_GenProg_PatchNaturalnessYe$$Fix Colt precision reduction. Add one line for precision patch. Fix for redundant rounding in BigDecimal precision test. Fix precision warning. $$0
math-8e5867ed$$Incorrect rounding of float$$package org.apache.commons.math3.util  example of usage of round functions of Precision class:  Precision.round(0.0f, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float)0.0, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float) 0.0, 2) = 0.0 Precision.round(0.0, 2, BigDecimal.ROUND_UP) = 0.0  Seems the reason is usage of extending float to double inside round functions and getting influence of memory trash as value.  I think, same problem will be found at usage of other round modes.$$patch54-math-8e5867ed_GenProg_PatchNaturalnessYe$$Fix a small bug in Precision . ceil ( ) .. Fix nextAfter ( ) in BigDecimal precision test. FastMath . ceil ( unscaled ) doesn ' t fix the rounded angles .. $$0
math-8e5867ed$$Incorrect rounding of float$$package org.apache.commons.math3.util  example of usage of round functions of Precision class:  Precision.round(0.0f, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float)0.0, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float) 0.0, 2) = 0.0 Precision.round(0.0, 2, BigDecimal.ROUND_UP) = 0.0  Seems the reason is usage of extending float to double inside round functions and getting influence of memory trash as value.  I think, same problem will be found at usage of other round modes.$$patch62-math-8e5867ed_GenProg_PatchNaturalnessYe$$Fix precision bug. Remove nextAfter in Precision . java. FastMath . ceil ( unscaled ) doesn ' t fix the rounded angles .. $$0
math-8e5867ed$$Incorrect rounding of float$$package org.apache.commons.math3.util  example of usage of round functions of Precision class:  Precision.round(0.0f, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float)0.0, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float) 0.0, 2) = 0.0 Precision.round(0.0, 2, BigDecimal.ROUND_UP) = 0.0  Seems the reason is usage of extending float to double inside round functions and getting influence of memory trash as value.  I think, same problem will be found at usage of other round modes.$$patch39-math-8e5867ed_GenProg_PatchNaturalnessYe$$Fix a bug in Math_8e5867ed .. Fix nextAfter ( ) in BigDecimal precision test. FastMath . ceil ( unscaled ) doesn ' t fix the rounded angles .. $$0
math-8e5867ed$$Incorrect rounding of float$$package org.apache.commons.math3.util  example of usage of round functions of Precision class:  Precision.round(0.0f, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float)0.0, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float) 0.0, 2) = 0.0 Precision.round(0.0, 2, BigDecimal.ROUND_UP) = 0.0  Seems the reason is usage of extending float to double inside round functions and getting influence of memory trash as value.  I think, same problem will be found at usage of other round modes.$$patch30-math-8e5867ed_GenProg_PatchNaturalnessYe$$Fix precision issue. Fix nextAfter ( ) in BigDecimal precision test. FastMath . ceil ( unscaled ) doesn ' t fix the rounded angles .. $$0
math-8e5867ed$$Incorrect rounding of float$$package org.apache.commons.math3.util  example of usage of round functions of Precision class:  Precision.round(0.0f, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float)0.0, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float) 0.0, 2) = 0.0 Precision.round(0.0, 2, BigDecimal.ROUND_UP) = 0.0  Seems the reason is usage of extending float to double inside round functions and getting influence of memory trash as value.  I think, same problem will be found at usage of other round modes.$$patch37-math-8e5867ed_GenProg_PatchNaturalnessYe$$Fix one more test. FastMath . ceil ( unscaled ) doesn ' t fix the rounded angles .. $$0
math-8e5867ed$$Incorrect rounding of float$$package org.apache.commons.math3.util  example of usage of round functions of Precision class:  Precision.round(0.0f, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float)0.0, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float) 0.0, 2) = 0.0 Precision.round(0.0, 2, BigDecimal.ROUND_UP) = 0.0  Seems the reason is usage of extending float to double inside round functions and getting influence of memory trash as value.  I think, same problem will be found at usage of other round modes.$$patch63-math-8e5867ed_GenProg_PatchNaturalnessYe$$Fix a bug in Math_8e5867ed .. Add one line , as per EvilSeph ' s request. Fix 64 - bit precision mistake. FixBigDecimal roundoff. FastMath . ceil ( unscaled ) doesn ' t fix the rounded angles .. $$0
math-8e5867ed$$Incorrect rounding of float$$package org.apache.commons.math3.util  example of usage of round functions of Precision class:  Precision.round(0.0f, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float)0.0, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float) 0.0, 2) = 0.0 Precision.round(0.0, 2, BigDecimal.ROUND_UP) = 0.0  Seems the reason is usage of extending float to double inside round functions and getting influence of memory trash as value.  I think, same problem will be found at usage of other round modes.$$patch64-math-8e5867ed_GenProg_PatchNaturalnessYe$$FastMath . ceil ( unscaled ) doesn ' t fix the rounded angles .. $$0
math-8e5867ed$$Incorrect rounding of float$$package org.apache.commons.math3.util  example of usage of round functions of Precision class:  Precision.round(0.0f, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float)0.0, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float) 0.0, 2) = 0.0 Precision.round(0.0, 2, BigDecimal.ROUND_UP) = 0.0  Seems the reason is usage of extending float to double inside round functions and getting influence of memory trash as value.  I think, same problem will be found at usage of other round modes.$$patch52-math-8e5867ed_GenProg_PatchNaturalnessYe$$Add a facta field to precision . abs. FastMath . ceil ( unscaled ) doesn ' t fix the rounded angles .. $$0
math-8e5867ed$$Incorrect rounding of float$$package org.apache.commons.math3.util  example of usage of round functions of Precision class:  Precision.round(0.0f, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float)0.0, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float) 0.0, 2) = 0.0 Precision.round(0.0, 2, BigDecimal.ROUND_UP) = 0.0  Seems the reason is usage of extending float to double inside round functions and getting influence of memory trash as value.  I think, same problem will be found at usage of other round modes.$$patch55-math-8e5867ed_GenProg_PatchNaturalnessYe$$Fix 64 bit fallthrough in BigDecimal precision test. FastMath . ceil ( unscaled ) doesn ' t fix the rounded angles .. $$0
math-8e5867ed$$Incorrect rounding of float$$package org.apache.commons.math3.util  example of usage of round functions of Precision class:  Precision.round(0.0f, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float)0.0, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float) 0.0, 2) = 0.0 Precision.round(0.0, 2, BigDecimal.ROUND_UP) = 0.0  Seems the reason is usage of extending float to double inside round functions and getting influence of memory trash as value.  I think, same problem will be found at usage of other round modes.$$patch48-math-8e5867ed_GenProg_PatchNaturalnessYe$$Fix a bug in Math_8e5867ed .. Fix a bug in BigDecimal precision. Fix precision issue. FixBigDecimal roundoff. FastMath . ceil ( unscaled ) doesn ' t fix the rounded angles .. $$0
math-8e5867ed$$Incorrect rounding of float$$package org.apache.commons.math3.util  example of usage of round functions of Precision class:  Precision.round(0.0f, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float)0.0, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float) 0.0, 2) = 0.0 Precision.round(0.0, 2, BigDecimal.ROUND_UP) = 0.0  Seems the reason is usage of extending float to double inside round functions and getting influence of memory trash as value.  I think, same problem will be found at usage of other round modes.$$patch46-math-8e5867ed_GenProg_PatchNaturalnessYe$$Fix reduceResults. Add one line for precision patch. Fix a bug in BigDecimal precision checking. FastMath . ceil ( unscaled ) doesn ' t fix the rounded angles .. $$0
math-8e5867ed$$Incorrect rounding of float$$package org.apache.commons.math3.util  example of usage of round functions of Precision class:  Precision.round(0.0f, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float)0.0, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float) 0.0, 2) = 0.0 Precision.round(0.0, 2, BigDecimal.ROUND_UP) = 0.0  Seems the reason is usage of extending float to double inside round functions and getting influence of memory trash as value.  I think, same problem will be found at usage of other round modes.$$patch41-math-8e5867ed_GenProg_PatchNaturalnessYe$$Fix a bug in BigDecimal precision. Fix a minor issue with floor ( ) / 2 . 0 being too far away. FastMath . ceil ( unscaled ) doesn ' t fix the rounded angles .. $$0
math-8e5867ed$$Incorrect rounding of float$$package org.apache.commons.math3.util  example of usage of round functions of Precision class:  Precision.round(0.0f, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float)0.0, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float) 0.0, 2) = 0.0 Precision.round(0.0, 2, BigDecimal.ROUND_UP) = 0.0  Seems the reason is usage of extending float to double inside round functions and getting influence of memory trash as value.  I think, same problem will be found at usage of other round modes.$$patch15-math-8e5867ed_GenProg_PatchNaturalnessYe$$Fix a bug in BigDecimal precision. Fix nextAfter ( ) in BigDecimal precision test. FastMath . ceil ( unscaled ) doesn ' t fix the rounded angles .. $$0
math-8e5867ed$$Incorrect rounding of float$$package org.apache.commons.math3.util  example of usage of round functions of Precision class:  Precision.round(0.0f, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float)0.0, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float) 0.0, 2) = 0.0 Precision.round(0.0, 2, BigDecimal.ROUND_UP) = 0.0  Seems the reason is usage of extending float to double inside round functions and getting influence of memory trash as value.  I think, same problem will be found at usage of other round modes.$$patch12-math-8e5867ed_GenProg_PatchNaturalnessYe$$Fix a bug in BigDecimal precision. FastMath . ceil ( unscaled ) doesn ' t fix the rounded angles .. $$0
math-8e5867ed$$Incorrect rounding of float$$package org.apache.commons.math3.util  example of usage of round functions of Precision class:  Precision.round(0.0f, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float)0.0, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float) 0.0, 2) = 0.0 Precision.round(0.0, 2, BigDecimal.ROUND_UP) = 0.0  Seems the reason is usage of extending float to double inside round functions and getting influence of memory trash as value.  I think, same problem will be found at usage of other round modes.$$patch24-math-8e5867ed_GenProg_PatchNaturalnessYe$$Fix a bug in the precision of Math_8e5867ed. Fix nextAfter ( ) in BigDecimal precision test. FastMath . ceil ( unscaled ) doesn ' t fix the rounded angles .. $$0
math-8e5867ed$$Incorrect rounding of float$$package org.apache.commons.math3.util  example of usage of round functions of Precision class:  Precision.round(0.0f, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float)0.0, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float) 0.0, 2) = 0.0 Precision.round(0.0, 2, BigDecimal.ROUND_UP) = 0.0  Seems the reason is usage of extending float to double inside round functions and getting influence of memory trash as value.  I think, same problem will be found at usage of other round modes.$$patch23-math-8e5867ed_GenProg_PatchNaturalnessYe$$Fix a bug in BigDecimal precision. Fix nextAfter in Math_8e5867ed. FastMath . ceil ( unscaled ) doesn ' t fix the rounded angles .. $$0
math-8e5867ed$$Incorrect rounding of float$$package org.apache.commons.math3.util  example of usage of round functions of Precision class:  Precision.round(0.0f, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float)0.0, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float) 0.0, 2) = 0.0 Precision.round(0.0, 2, BigDecimal.ROUND_UP) = 0.0  Seems the reason is usage of extending float to double inside round functions and getting influence of memory trash as value.  I think, same problem will be found at usage of other round modes.$$patch40-math-8e5867ed_GenProg_PatchNaturalnessYe$$Fix Colt rounding bug. Add one line for precision patch. Fix for previous round. FastMath . ceil ( unscaled ) doesn ' t fix the rounded angles .. $$0
math-8e5867ed$$Incorrect rounding of float$$package org.apache.commons.math3.util  example of usage of round functions of Precision class:  Precision.round(0.0f, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float)0.0, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float) 0.0, 2) = 0.0 Precision.round(0.0, 2, BigDecimal.ROUND_UP) = 0.0  Seems the reason is usage of extending float to double inside round functions and getting influence of memory trash as value.  I think, same problem will be found at usage of other round modes.$$patch47-math-8e5867ed_GenProg_PatchNaturalnessYe$$Add pi2b precision. Fix the rounded corner case for BigDecimal precision .. Fix nextAfter in Math_8e5867ed. FastMath . ceil ( unscaled ) doesn ' t fix the rounded angles .. $$0
math-8e5867ed$$Incorrect rounding of float$$package org.apache.commons.math3.util  example of usage of round functions of Precision class:  Precision.round(0.0f, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float)0.0, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float) 0.0, 2) = 0.0 Precision.round(0.0, 2, BigDecimal.ROUND_UP) = 0.0  Seems the reason is usage of extending float to double inside round functions and getting influence of memory trash as value.  I think, same problem will be found at usage of other round modes.$$patch49-math-8e5867ed_GenProg_PatchNaturalnessYe$$Fix a bug in Math_8e5867ed. FastMath . ceil ( unscaled ) doesn ' t fix the rounded angles .. $$0
math-8e5867ed$$Incorrect rounding of float$$package org.apache.commons.math3.util  example of usage of round functions of Precision class:  Precision.round(0.0f, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float)0.0, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float) 0.0, 2) = 0.0 Precision.round(0.0, 2, BigDecimal.ROUND_UP) = 0.0  Seems the reason is usage of extending float to double inside round functions and getting influence of memory trash as value.  I think, same problem will be found at usage of other round modes.$$patch22-math-8e5867ed_GenProg_PatchNaturalnessYe$$Fix a bug in Math_8e5867ed .. Fix floor. FastMath . nextAfter ( unscaled , Double . NEGATIVE_INFINITY ) added missing patch. FastMath . ceil ( unscaled ) doesn ' t fix the rounded angles .. $$0
math-8e5867ed$$Incorrect rounding of float$$package org.apache.commons.math3.util  example of usage of round functions of Precision class:  Precision.round(0.0f, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float)0.0, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float) 0.0, 2) = 0.0 Precision.round(0.0, 2, BigDecimal.ROUND_UP) = 0.0  Seems the reason is usage of extending float to double inside round functions and getting influence of memory trash as value.  I think, same problem will be found at usage of other round modes.$$patch25-math-8e5867ed_GenProg_PatchNaturalnessYe$$Fix a bug in Math_8e5867ed .. Fix the rounded angles of the precision column .. Fix a bug in the floor of Math . floor ( unscaled ) .. FastMath . nextAfter ( unscaled , Double . NEGATIVE_INFINITY ) fix. FastMath . ceil ( unscaled ) doesn ' t fix the rounded angles .. $$0
math-8e5867ed$$Incorrect rounding of float$$package org.apache.commons.math3.util  example of usage of round functions of Precision class:  Precision.round(0.0f, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float)0.0, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float) 0.0, 2) = 0.0 Precision.round(0.0, 2, BigDecimal.ROUND_UP) = 0.0  Seems the reason is usage of extending float to double inside round functions and getting influence of memory trash as value.  I think, same problem will be found at usage of other round modes.$$patch13-math-8e5867ed_GenProg_PatchNaturalnessYe$$Fix MCP patch. Fix a bug in the rounded part of the Math class .. FastMath . ceil ( unscaled ) doesn ' t fix the rounded angles .. $$0
math-8e5867ed$$Incorrect rounding of float$$package org.apache.commons.math3.util  example of usage of round functions of Precision class:  Precision.round(0.0f, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float)0.0, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float) 0.0, 2) = 0.0 Precision.round(0.0, 2, BigDecimal.ROUND_UP) = 0.0  Seems the reason is usage of extending float to double inside round functions and getting influence of memory trash as value.  I think, same problem will be found at usage of other round modes.$$patch14-math-8e5867ed_GenProg_PatchNaturalnessYe$$Fix MCP patch. Remove nextAfter in BigDecimal precision. FastMath . ceil ( unscaled ) doesn ' t fix the rounded angles .. $$0
math-8e5867ed$$Incorrect rounding of float$$package org.apache.commons.math3.util  example of usage of round functions of Precision class:  Precision.round(0.0f, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float)0.0, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float) 0.0, 2) = 0.0 Precision.round(0.0, 2, BigDecimal.ROUND_UP) = 0.0  Seems the reason is usage of extending float to double inside round functions and getting influence of memory trash as value.  I think, same problem will be found at usage of other round modes.$$patch57-math-8e5867ed_GenProg_PatchNaturalnessYe$$Add zb to precision fix .. Fix a potential NPE in BigDecimal precision test. Fix the rounded angles of the precision column .. $$0
math-8e5867ed$$Incorrect rounding of float$$package org.apache.commons.math3.util  example of usage of round functions of Precision class:  Precision.round(0.0f, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float)0.0, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float) 0.0, 2) = 0.0 Precision.round(0.0, 2, BigDecimal.ROUND_UP) = 0.0  Seems the reason is usage of extending float to double inside round functions and getting influence of memory trash as value.  I think, same problem will be found at usage of other round modes.$$patch61-math-8e5867ed_GenProg_PatchNaturalnessYe$$Remove patch from unscaled precision test. FastMath . ceil ( unscaled ) doesn ' t fix the rounded angles .. $$0
math-8e5867ed$$Incorrect rounding of float$$package org.apache.commons.math3.util  example of usage of round functions of Precision class:  Precision.round(0.0f, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float)0.0, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float) 0.0, 2) = 0.0 Precision.round(0.0, 2, BigDecimal.ROUND_UP) = 0.0  Seems the reason is usage of extending float to double inside round functions and getting influence of memory trash as value.  I think, same problem will be found at usage of other round modes.$$patch59-math-8e5867ed_GenProg_PatchNaturalnessYe$$Fix a potential NPE in BigDecimal precision test. FastMath . ceil ( unscaled ) doesn ' t fix the rounded angles .. $$0
math-8e5867ed$$Incorrect rounding of float$$package org.apache.commons.math3.util  example of usage of round functions of Precision class:  Precision.round(0.0f, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float)0.0, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float) 0.0, 2) = 0.0 Precision.round(0.0, 2, BigDecimal.ROUND_UP) = 0.0  Seems the reason is usage of extending float to double inside round functions and getting influence of memory trash as value.  I think, same problem will be found at usage of other round modes.$$patch11-math-8e5867ed_GenProg_PatchNaturalnessYe$$Fix Colt precision reduction. Fix precision warning. Fix a bug in BigDecimal precision checking. FastMath . ceil ( unscaled ) doesn ' t fix the rounded angles .. $$0
math-8e5867ed$$Incorrect rounding of float$$package org.apache.commons.math3.util  example of usage of round functions of Precision class:  Precision.round(0.0f, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float)0.0, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float) 0.0, 2) = 0.0 Precision.round(0.0, 2, BigDecimal.ROUND_UP) = 0.0  Seems the reason is usage of extending float to double inside round functions and getting influence of memory trash as value.  I think, same problem will be found at usage of other round modes.$$patch29-math-8e5867ed_GenProg_PatchNaturalnessYe$$raw precision fix. Fix a small bug in Precision . nextAfter ( double , double ). Fix the rounded corner bug in Math_8e5867ed .. Fix PMD precision test. FastMath . ceil ( unscaled ) doesn ' t fix the rounded angles .. $$0
math-8e5867ed$$Incorrect rounding of float$$package org.apache.commons.math3.util  example of usage of round functions of Precision class:  Precision.round(0.0f, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float)0.0, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float) 0.0, 2) = 0.0 Precision.round(0.0, 2, BigDecimal.ROUND_UP) = 0.0  Seems the reason is usage of extending float to double inside round functions and getting influence of memory trash as value.  I think, same problem will be found at usage of other round modes.$$patch16-math-8e5867ed_GenProg_PatchNaturalnessYe$$Add pi2b precision. Fix the rounded corner case for BigDecimal precision .. Fix nextAfter ( ) in BigDecimal precision test. FastMath . ceil ( unscaled ) doesn ' t fix the rounded angles .. $$0
math-8e5867ed$$Incorrect rounding of float$$package org.apache.commons.math3.util  example of usage of round functions of Precision class:  Precision.round(0.0f, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float)0.0, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float) 0.0, 2) = 0.0 Precision.round(0.0, 2, BigDecimal.ROUND_UP) = 0.0  Seems the reason is usage of extending float to double inside round functions and getting influence of memory trash as value.  I think, same problem will be found at usage of other round modes.$$patch20-math-8e5867ed_GenProg_PatchNaturalnessYe$$Fix a small bug in Precision . nextAfter ( double , double ). FastMath . ceil ( unscaled ) doesn ' t fix the rounded angles .. $$0
math-8e5867ed$$Incorrect rounding of float$$package org.apache.commons.math3.util  example of usage of round functions of Precision class:  Precision.round(0.0f, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float)0.0, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float) 0.0, 2) = 0.0 Precision.round(0.0, 2, BigDecimal.ROUND_UP) = 0.0  Seems the reason is usage of extending float to double inside round functions and getting influence of memory trash as value.  I think, same problem will be found at usage of other round modes.$$patch27-math-8e5867ed_GenProg_PatchNaturalnessYe$$Fix Colt precision reduction. Fix the bug for Java 8 . 5 .. Fix compile error. $$0
math-8e5867ed$$Incorrect rounding of float$$package org.apache.commons.math3.util  example of usage of round functions of Precision class:  Precision.round(0.0f, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float)0.0, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float) 0.0, 2) = 0.0 Precision.round(0.0, 2, BigDecimal.ROUND_UP) = 0.0  Seems the reason is usage of extending float to double inside round functions and getting influence of memory trash as value.  I think, same problem will be found at usage of other round modes.$$patch42-math-8e5867ed_GenProg_PatchNaturalnessYe$$Add a facta field to precision . abs. Remove patch from unscaled to ceil. FastMath . ceil ( unscaled ) doesn ' t fix the rounded angles .. $$0
math-8e5867ed$$Incorrect rounding of float$$package org.apache.commons.math3.util  example of usage of round functions of Precision class:  Precision.round(0.0f, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float)0.0, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float) 0.0, 2) = 0.0 Precision.round(0.0, 2, BigDecimal.ROUND_UP) = 0.0  Seems the reason is usage of extending float to double inside round functions and getting influence of memory trash as value.  I think, same problem will be found at usage of other round modes.$$patch2-math-8e5867ed_GenProg_PatchNaturalnessYe$$Fix the rounded corner case for BigDecimal precision .. Fix nextAfter in Math_8e5867ed. FastMath . ceil ( unscaled ) doesn ' t fix the rounded angles .. $$0
math-8e5867ed$$Incorrect rounding of float$$package org.apache.commons.math3.util  example of usage of round functions of Precision class:  Precision.round(0.0f, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float)0.0, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float) 0.0, 2) = 0.0 Precision.round(0.0, 2, BigDecimal.ROUND_UP) = 0.0  Seems the reason is usage of extending float to double inside round functions and getting influence of memory trash as value.  I think, same problem will be found at usage of other round modes.$$patch45-math-8e5867ed_GenProg_PatchNaturalnessYe$$Fix a bug in BigDecimal precision. Fix precision thingie in java 8e5867ed. FastMath . ceil ( unscaled ) doesn ' t fix the rounded angles .. $$0
math-8e5867ed$$Incorrect rounding of float$$package org.apache.commons.math3.util  example of usage of round functions of Precision class:  Precision.round(0.0f, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float)0.0, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float) 0.0, 2) = 0.0 Precision.round(0.0, 2, BigDecimal.ROUND_UP) = 0.0  Seems the reason is usage of extending float to double inside round functions and getting influence of memory trash as value.  I think, same problem will be found at usage of other round modes.$$patch19-math-8e5867ed_GenProg_PatchNaturalnessYe$$Fix a bug in BigDecimal precision. Fix nextAfter ( ) in BigDecimal precision test. FastMath . ceil ( unscaled ) doesn ' t fix the rounded angles .. $$0
math-8e5867ed$$Incorrect rounding of float$$package org.apache.commons.math3.util  example of usage of round functions of Precision class:  Precision.round(0.0f, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float)0.0, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float) 0.0, 2) = 0.0 Precision.round(0.0, 2, BigDecimal.ROUND_UP) = 0.0  Seems the reason is usage of extending float to double inside round functions and getting influence of memory trash as value.  I think, same problem will be found at usage of other round modes.$$patch26-math-8e5867ed_GenProg_PatchNaturalnessYe$$Fix Colt precision reduction. Add one line for precision patch. Fix for previous round. FastMath . ceil ( unscaled ) doesn ' t fix the rounded angles .. $$0
math-8e5867ed$$Incorrect rounding of float$$package org.apache.commons.math3.util  example of usage of round functions of Precision class:  Precision.round(0.0f, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float)0.0, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float) 0.0, 2) = 0.0 Precision.round(0.0, 2, BigDecimal.ROUND_UP) = 0.0  Seems the reason is usage of extending float to double inside round functions and getting influence of memory trash as value.  I think, same problem will be found at usage of other round modes.$$patch21-math-8e5867ed_GenProg_PatchNaturalnessYe$$Fix a bug in the rounded part of the Math class .. FastMath . ceil ( unscaled ) doesn ' t fix the rounded angles .. $$0
math-8e5867ed$$Incorrect rounding of float$$package org.apache.commons.math3.util  example of usage of round functions of Precision class:  Precision.round(0.0f, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float)0.0, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float) 0.0, 2) = 0.0 Precision.round(0.0, 2, BigDecimal.ROUND_UP) = 0.0  Seems the reason is usage of extending float to double inside round functions and getting influence of memory trash as value.  I think, same problem will be found at usage of other round modes.$$patch28-math-8e5867ed_GenProg_PatchNaturalnessYe$$Fix a bug in Math_8e5867ed. Fix the rounded angles of the precision column .. $$0
math-8e5867ed$$Incorrect rounding of float$$package org.apache.commons.math3.util  example of usage of round functions of Precision class:  Precision.round(0.0f, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float)0.0, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float) 0.0, 2) = 0.0 Precision.round(0.0, 2, BigDecimal.ROUND_UP) = 0.0  Seems the reason is usage of extending float to double inside round functions and getting influence of memory trash as value.  I think, same problem will be found at usage of other round modes.$$patch17-math-8e5867ed_GenProg_PatchNaturalnessYe$$Fix a bug in BigDecimal precision. Add Rln10a patch .. FastMath . ceil ( unscaled ) doesn ' t fix the rounded angles .. $$0
math-8e5867ed$$Incorrect rounding of float$$package org.apache.commons.math3.util  example of usage of round functions of Precision class:  Precision.round(0.0f, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float)0.0, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float) 0.0, 2) = 0.0 Precision.round(0.0, 2, BigDecimal.ROUND_UP) = 0.0  Seems the reason is usage of extending float to double inside round functions and getting influence of memory trash as value.  I think, same problem will be found at usage of other round modes.$$patch10-math-8e5867ed_GenProg_PatchNaturalnessYe$$Fix Colt precision reduction. Fix precision warning. Fix for previous round. FastMath . ceil ( unscaled ) doesn ' t fix the rounded angles .. $$0
math-8e5867ed$$Incorrect rounding of float$$package org.apache.commons.math3.util  example of usage of round functions of Precision class:  Precision.round(0.0f, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float)0.0, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float) 0.0, 2) = 0.0 Precision.round(0.0, 2, BigDecimal.ROUND_UP) = 0.0  Seems the reason is usage of extending float to double inside round functions and getting influence of memory trash as value.  I think, same problem will be found at usage of other round modes.$$patch44-math-8e5867ed_GenProg_PatchNaturalnessYe$$Fix a bug in Math_8e5867ed. FastMath . ceil ( unscaled ) doesn ' t fix the rounded angles .. $$0
math-8e5867ed$$Incorrect rounding of float$$package org.apache.commons.math3.util  example of usage of round functions of Precision class:  Precision.round(0.0f, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float)0.0, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float) 0.0, 2) = 0.0 Precision.round(0.0, 2, BigDecimal.ROUND_UP) = 0.0  Seems the reason is usage of extending float to double inside round functions and getting influence of memory trash as value.  I think, same problem will be found at usage of other round modes.$$patch3-math-8e5867ed_GenProg_PatchNaturalnessYe$$Fix precision bug. FastMath . ceil ( unscaled ) doesn ' t fix the rounded angles .. $$0
math-8e5867ed$$Incorrect rounding of float$$package org.apache.commons.math3.util  example of usage of round functions of Precision class:  Precision.round(0.0f, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float)0.0, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float) 0.0, 2) = 0.0 Precision.round(0.0, 2, BigDecimal.ROUND_UP) = 0.0  Seems the reason is usage of extending float to double inside round functions and getting influence of memory trash as value.  I think, same problem will be found at usage of other round modes.$$patch4-math-8e5867ed_GenProg_PatchNaturalnessYe$$Fix precision bug. Fix a bug in BigDecimal precision test. FastMath . ceil ( unscaled ) doesn ' t fix the rounded angles .. $$0
math-8e5867ed$$Incorrect rounding of float$$package org.apache.commons.math3.util  example of usage of round functions of Precision class:  Precision.round(0.0f, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float)0.0, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float) 0.0, 2) = 0.0 Precision.round(0.0, 2, BigDecimal.ROUND_UP) = 0.0  Seems the reason is usage of extending float to double inside round functions and getting influence of memory trash as value.  I think, same problem will be found at usage of other round modes.$$patch43-math-8e5867ed_GenProg_PatchNaturalnessYe$$Fix a bug in BigDecimal precision. FastMath . ceil ( unscaled ) doesn ' t fix the rounded angles .. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch120-math-f6dd42b4_GenProg_PatchNaturalnessYe$$BrentSolver now clears the result of the last update. I had left it as is. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch323-math-f6dd42b4_GenProg_PatchNaturalnessYe$$BrentSolver patch was failing because I added another GLIBC - 2 closing loop. Clarify variable name to match new BrentSolver. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch111-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Added patch for variable max value function. Added verifyBracketing ( min , max , f ). $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch38-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Reset the iterator count as well. Added verifyBracketing to the BrentSolver. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch324-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Fixing the BrentSolver bug .. Add patch for bisection. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch129-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Fix the bug in BrentSolver from patch 0 .. Added verifyBracketing to the BrentSolver. Add back missing patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch189-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Remove a couple incorrect lines. I was confused , sorry. Add back missing patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch348-math-f6dd42b4_GenProg_PatchNaturalnessYe$$BrentSolver now uses the patched value , but is not good enough. changed default function value accuracy in BrentSolver. Added patch for function value endpoints. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch98-math-f6dd42b4_GenProg_PatchNaturalnessYe$$remove clearResult ( ) from verifySequence ( ). Remove erroneous test. Set variable name as used by the BrentSolver class .. Added patch for function value min and max. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch145-math-f6dd42b4_GenProg_PatchNaturalnessYe$$BrentSolver patch was not re - used .. Clarify BrentSolver with better error handling. Set x1 = x2 before setting result .. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch53-math-f6dd42b4_GenProg_PatchNaturalnessYe$$I had left it as is. Add back missing patch. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch187-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Added patch for variable max value checking. Added verifyBracketing ( min , max , f ). $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch325-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Set iterationCount as it is only used when checking the sequence. Set iterationCount as well as verifyBracketing ( min , max , f ). revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch110-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Added patch for 1 . 8 . 3 .. Reset the iterator count as well. Added verifyBracketing to the BrentSolver. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch39-math-f6dd42b4_GenProg_PatchNaturalnessYe$$BrentSolver now uses the fixed value function , so it is normally possible to reuse it .. changed variable name. Clarify BrentSolver with better error handling. Added patch to prove that the function value is close to 0 . 0 .. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch314-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Added verifyBracketing ( min , max , f ) to the BrentSolver bug. Fix the bug in the elm function to be compatible with the current AEJC. I had left it as is done in the last commit .. Add a missing return value in inverse quadratic interpolation. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch119-math-f6dd42b4_GenProg_PatchNaturalnessYe$$changed variable name for BrentSolver. Added verifyBracketing method to the BrentSolver. Fix erroneous test .. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch121-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Remove a couple incorrect code lines. changed max value of BrentSolver to be 0 if max is good enough. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch37-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Fix erroneous test. Clarify variable name to match new BrentSolver. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch63-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Clarify BrentSolver with better error handling. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch175-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Updated BrentSolver . setResult ( initial , 0 ) ;. Clarify BrentSolver . resultComputed with better accuracy .. Added patch for function value coherency. Add patch for easing of the bug. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch347-math-f6dd42b4_GenProg_PatchNaturalnessYe$$BrentSolver patch was not re - used .. Clarify solution with better error handling. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch97-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Set this . iterationCount = iterationCount + 1 ; rather than the default max .. Set iterationCount as it is normally possible to reuse the same x , y , yInitial .. Added patch for function value min and max. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch181-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Set function value accuracy to default , as it is done every time. Added verifyBracketing ( ) to the function value accuracy patch. Fix erroneous test case. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch90-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Set x2 and y2 as arguments to the BrentSolver class .. Clarify solution with better error handling. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch186-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Set x2 = max value of BrentSolver as well. Clarify variable name to match provided function value accuracy. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch340-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Added verifyBracketing ( ) to the function value accuracy patch. I had left it as was done before the move . . .. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch64-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Set x2 = max as the max is not good enough .. Added verifyBracketing ( ) to the function value accuracy patch. Remove a redundant check. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch172-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Remove redundant clearResult ( ) patch from BrentSolver. Added erroneous inverse of verifyBracketing ( min , max , f ). Set p1 = 1 . 0 - r3 instead of p0 = 0 . 5 *. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch144-math-f6dd42b4_GenProg_PatchNaturalnessYe$$BrentSolver now uses the patched value ( already provided by valgrind ). Set this . iterationCount to zero so that the function value is always good enough. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch52-math-f6dd42b4_GenProg_PatchNaturalnessYe$$changed variable name for BrentSolver. Added more iterations to the stub. Fix erroneous test .. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch143-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Clarify variable name to match new BrentSolver. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch232-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Added verifyBracketing ( ) to the function value accuracy patch. Added patch for 1 . 8 . 0 .. I had left it harwired to verifyBracketing .. Added patch for variable x0 in BrentSolver. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch261-math-f6dd42b4_GenProg_PatchNaturalnessYe$$BrentSolver now uses the patched value .. Fix swapped line. I had left it harwired to verifyBracketing .. Add a missing return value in inverse quadratic interpolation. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch259-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Remove redundant clearResult ( ) call. Added verifyBracketing ( ) to the function value otherwise it will break the build .. Clarify variable name to match provided function value accuracy .. Add back missing patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch266-math-f6dd42b4_GenProg_PatchNaturalnessYe$$remove clearResult ( ) from verifySequence ( ). Added verifyBracketing ( min , max , f ). Set p1 = 1 . 0 - r3 instead of p1 = 0 . 5 *. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch292-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Added verifyBracketing ( ) to the function value accuracy patch. Clarify a use case .. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch250-math-f6dd42b4_GenProg_PatchNaturalnessYe$$BrentSolver now clears the result of the last fix. Set iterationCount as it was done before the closure was opened .. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch234-math-f6dd42b4_GenProg_PatchNaturalnessYe$$BrentSolver now uses the patched value , but not the initial guess. changed max value of BrentSolver to be 0 if ( yMax < min ). $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch233-math-f6dd42b4_GenProg_PatchNaturalnessYe$$BrentSolver now clears the result of the last fix. I had made a mistake in the previous patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch205-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Set iterationCount as well as verifyBracketing ( min , max , f ). $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch256-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Remove a couple incorrect code lines. Set maxIterCount and verifyBracketing ( ) as well as maxIterCount. Added patch for function value min and max. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch258-math-f6dd42b4_GenProg_PatchNaturalnessYe$$BrentSolver now uses the patched value ( already provided by valgrind ). Set function value accuracy to default , this way we can use the expanded function .. Fix erroneous test .. Fix erroneous variable. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch289-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Added verifyBracketing ( min , max , f ). Added patch for BrentSolver. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch273-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Added verifyBracketing ( ) to the function value accuracy patch. Remove a redundant patch. Added patch to erroneous integration test .. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch280-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Reset function value accuracy to default function value accuracy. Added verifyBracketing ( ) to the function value accuracy patch. Added verifyBracketing to the BrentSolver. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch218-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Added patch for variable max value function. Added verifyBracketing ( min , max , f ). revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch281-math-f6dd42b4_GenProg_PatchNaturalnessYe$$BrentSolver now uses the patched value .. Clarify BrentSolver . resultComputed with updated max value. Added patch for function value endpoints. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch275-math-f6dd42b4_GenProg_PatchNaturalnessYe$$BrentSolver now uses Object . POSITIVE_INFINITY for parallel execution. Added verifyBracketing ( ) to the function value accuracy patch. Set iterationCount as it was done before the closure closure closure closure closure closure .. revert accidently change. BrentSolver now uses the old delta value .. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch272-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Set function value accuracy to default , as it is done every time. Added verifyBracketing ( ) to the function value accuracy patch. Fixing a minor typo in the BrentSolver test. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch219-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Set x1 = 0 after verifying the pair. Added a throw to the right part of BrentSolver. Fix erroneous test .. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch77-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Fix erroneous regression in BrentSolver. I had left it harwired to verifyBracketing .. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch195-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Fix erroneous integration with BrentSolver. I had left it as is done in the last commit .. Set iterationCount as - 1 . 5 for inverse quadratic interpolation .. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch83-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Set this . iterationCount to zero so that the function value is always good enough. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch166-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Added verifyBracketing ( min , max , f ). $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch70-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Remove redundant clearResult ( ) call. changed vars as well. Set p1 = 1 . 0 - r3 instead of p1 = 0 . 5 *. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch8-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Remove redundant clearResult ( ) call. Added verifyBracketing ( min , max , f ). Add the missing patch .. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch159-math-f6dd42b4_GenProg_PatchNaturalnessYe$$BrentSolver now clears the result of the last fix. Added patch for bracketing in BrentSolver. I had left it as is. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch46-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Set updated iteration count. Added patch for function value endpoints. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch168-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Remove redundant clearResult ( ) call. changed maxIterCount as well. I had left it harwired to verifyBracketing .. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch103-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Added patch for variable max value function. Added verifyBracketing ( ) to the function value accuracy patch. Clarify solution with the min value in the function valueAccuracy. Added missing variable .. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch309-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Added verifyBracketing ( ) to the function value accuracy patch. Set iterationCount as well. Added patch to erroneous integration test .. Add a missing line. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch336-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Added x2 = max ; to the main objective function. Set iterationCount as it is normally possible to reuse the same x , y , yInitial .. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch104-math-f6dd42b4_GenProg_PatchNaturalnessYe$$BrentSolver now returns 0 iff y = = min ;. Reset the iterator count as well. I had left it as is. Fix erroneous test .. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch300-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Added patch for 1 . 6 . 2. changed itercount. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch132-math-f6dd42b4_GenProg_PatchNaturalnessYe$$revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch40-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Set x2 = max oversampling in BrentSolver. Added verifyBracketing ( ) to the function value accuracy patch. Remove a redundant patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch156-math-f6dd42b4_GenProg_PatchNaturalnessYe$$changed erroneously set maxIterCount as well. Add back missing patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch169-math-f6dd42b4_GenProg_PatchNaturalnessYe$$remove clearResult ( ) from verifySequence ( ). Fix erroneous variable. Add back missing patch .. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch78-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Fix erroneous test. Clarify BrentSolver . resultComputed with better accuracy .. Added patch for function value endpoints. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch71-math-f6dd42b4_GenProg_PatchNaturalnessYe$$I had left it as a last resort .. I had left it as is done in the last commit .. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch85-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Added patch for 1 . 8 . 3. Set default function value accuracy using provided bracketing. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch49-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Remove a couple incorrect lines. Set iterationCount as full Brent algorithm. Added patch to erroneous integration test .. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch339-math-f6dd42b4_GenProg_PatchNaturalnessYe$$BrentSolver now clears the result of the last fix. Added verifyBracketing ( ) to the function value accuracy patch. changed max iter count. Add patch for wrong step .. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch330-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Set iterationCount = this . iterationCount + 1 ;. Added patch to prevent adding value to the same endpoint .. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch14-math-f6dd42b4_GenProg_PatchNaturalnessYe$$BrentSolver now uses the patched value ( already provided by valgrind ). Set function value accuracy to default , this way we can use the expanded function .. Added patch for BrentSolver .. Fix erroneous variable. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch50-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Set function value accuracy to default function value accuracy in BrentSolver. Clarify the Brent solver with better error handling. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch61-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Added default function value accuracy checking for BrentSolver. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch59-math-f6dd42b4_GenProg_PatchNaturalnessYe$$BrentSolver now uses the patched value ( already provided by valgrind ). Clarify BrentSolver with better error handling. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch32-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Added patch to verifySequence ( ). Added verifyBracketing ( ) to the function value accuracy patch. Remove a redundant patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch58-math-f6dd42b4_GenProg_PatchNaturalnessYe$$BrentSolver now uses the patched value ( already provided by valgrind ). Clarify BrentSolver with better error handling. Fix erroneous test .. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch93-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Added patch to verifySequence ( ). Added verifyBracketing ( ) to the function value accuracy patch. Clarify a use case for BrentSolver as well. Added missing variable .. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch185-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Remove redundant clearResult ( ) call. Added verifyBracketing ( ) to the function value otherwise it will break the build .. I was confused , sorry. Add back missing patch .. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch343-math-f6dd42b4_GenProg_PatchNaturalnessYe$$changed variable name for BrentSolver. I had left it as is. Fix erroneous test .. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch149-math-f6dd42b4_GenProg_PatchNaturalnessYe$$BrentSolver now uses the fixed value function , so it is normally possible to reuse it .. I had left it as is. Fix erroneous test .. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch344-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Fix erroneous regression in BrentSolver. Added verifyBracketing method to the BrentSolver class .. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch60-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Set x2 = max oversampling in BrentSolver. Added patch to verifyBracketing ( min , max , f ). Clarify a use case for BrentSolver as well. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch69-math-f6dd42b4_GenProg_PatchNaturalnessYe$$changed variable names. Set p1 = 1 . 0 - r3 instead of p1 = 0 . 5 *. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch140-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Set default function value accuracy across the board. Remove erroneous test. I had left it harwired to verifyBracketing .. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch51-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Remove redundant clearResult ( ) line. changed max value of BrentSolver to 0 , this way we avoid an NPE. I had left it harwired to verifyBracketing , was causing some problem in the code. revert wrong patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch114-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Fix variable name error in BrentSolver. changed max value of BrentSolver to verify bracketing. Add back missing patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch326-math-f6dd42b4_GenProg_PatchNaturalnessYe$$BrentSolver now returns 0 if the initial guess is good enough. Added verifyBracketing ( ) to the function value accuracy patch. Clarify solution with better error handling. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch33-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Add i = 0 ;. Clarify BrentSolver with better error handling. Fix erroneous test .. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch265-math-f6dd42b4_GenProg_PatchNaturalnessYe$$changed variable name for BrentSolver. Set function value accuracy to 0 if max is not good enough. Fix erroneous test .. Set x1 = x2 before setting result. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch291-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Set this . iterationCount as this . x1 = x1 + 1 ;. Clarify variable name to match provided function value accuracy. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch296-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Fix erroneous variable name. Added patch to erroneous integration test .. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch253-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Set iterationCount as it is normally possible to reuse the same thread. Set iterationCount as it is normally possible to reuse the same x0 value as the min value. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch238-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Reset function value accuracy to default function value accuracy. Added verifyBracketing ( ) to the function value accuracy patch. Clarify solution with the fix from Wright. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch200-math-f6dd42b4_GenProg_PatchNaturalnessYe$$BrentSolver patch was failing because I added another GLIBC - 2 closing loop. Added verifyBracketing ( ) to the function value accuracy patch. Set default function value accuracy to 1 . 0e - 15 ( minor ). Fix a minor bug in the same way as in the old german. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch236-math-f6dd42b4_GenProg_PatchNaturalnessYe$$remove clearResult ( ) from BrentSolver. Remove erroneous test. Set iterationCount and default function value accuracy. Added patch for function value coherency. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch209-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Added patch for 1 . 8 . 3 .. Clarify solution with better error handling. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch299-math-f6dd42b4_GenProg_PatchNaturalnessYe$$BrentSolver now uses the patched value .. Fixing erroneous variable. Added patch for function value endpoints. Add 284 patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch297-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Remove a couple incorrect lines. Set iterationCount as well as verifyBracketing ( min , max , f ). Added patch to erroneous integration test .. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch230-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Set iterationCount and default function value accuracy. Added patch to prevent app from being loaded into workflow. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch237-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Added verifyBracketing to class path. Fix a minor bug in the same way as in the old german. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch208-math-f6dd42b4_GenProg_PatchNaturalnessYe$$I had a bit more debug .. Fix erroneous variable. Add patch for easing of logistics .. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch201-math-f6dd42b4_GenProg_PatchNaturalnessYe$$BrentSolver now clears the result of the last fix. Added patch for bracketing. Clarify variable name as match provided by the old pull request. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch212-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Set iterationCount = this . iterationCount + 1 ; as it was the case before the reverse. Add back missing patch. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch241-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Fix variable local variable warning. Added support for bracketing in BrentSolver. Fix a minor bug in the same way as in the old german. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch277-math-f6dd42b4_GenProg_PatchNaturalnessYe$$BrentSolver patch was failing because I added another GLIBC - 2 closing loop. Added patch for bracketing in BrentSolver. Set default function value accuracy to 1 . 0e - 15 ( minor ). Fix a minor warning. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch270-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Add a fix for changed variable names in BrentSolver . java. Clarify BrentSolver with better error handling. Added patch to verifyInterval. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch213-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Added verifyBracketing ( min , max , f ). Add a missing return value in inverse quadratic interpolation. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch222-math-f6dd42b4_GenProg_PatchNaturalnessYe$$BrentSolver now clears the result of the last fix. Added verifyBracketing ( ) to the function value accuracy patch. Set iterationCount as it was done before the closure closure closure closure closure closure .. revert accidently change. BrentSolver now uses the old delta value .. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch271-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Fix erroneous variable. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch247-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Added verifyInterval ( min , max , f ) to BrentSolver. I had left it harwired to verifyBracketing .. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch138-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Set iterationCount as it is only used when checking the sequence. Clarify variable name to match provided function value accuracy. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch107-math-f6dd42b4_GenProg_PatchNaturalnessYe$$BrentSolver doesn ' t clear the result before attempting to use it. Fixing the BrentSolver bug .. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch335-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Clarify a use case for bracketing the root .. I had left it harwired to verifyBracketing , was causing some problem in the code. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch136-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Remove redundant clearResult ( ) call. Fix erroneous variable. Add back missing patch .. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch303-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Added verifyBracketing ( ) to the function value accuracy patch. Clarify variable name to match provided input .. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch191-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Set iterationCount as it is normally possible to reuse old BrentSolver. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch357-math-f6dd42b4_GenProg_PatchNaturalnessYe$$BrentSolver now uses the patched value , but is not good enough. Clarify BrentSolver . resultComputed with better accuracy .. Added patch for function value coherency. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch165-math-f6dd42b4_GenProg_PatchNaturalnessYe$$BrentSolver now uses the patched value ( already provided by valgrind ). Clarify solution with better error handling. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch359-math-f6dd42b4_GenProg_PatchNaturalnessYe$$changed variable name for BrentSolver. Clarify BrentSolver with better error handling. Fix erroneous test .. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch302-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Added verifyBracketing ( min , max , f ). $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch130-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Remove redundant clearResult ( ) call. Fix NPE in BrentSolver. I was confused , sorry. Add back missing patch. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch137-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Added patch to verifySequence. Set maxIterCount to zero if min is good enough .. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch101-math-f6dd42b4_GenProg_PatchNaturalnessYe$$remove clearResult ( ) from verifySequence ( ). Set iterationCount and default function value accuracy. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch139-math-f6dd42b4_GenProg_PatchNaturalnessYe$$BrentSolver now uses the result of running f ( x , y ) instead of the min /. I had made a mistake in the previous patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch44-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Clarify solution with the right function value. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch3-math-f6dd42b4_GenProg_PatchNaturalnessYe$$remove clearResult ( ) from verifySequence ( ). Remove unused variable. Set iterationCount as well as the last set of functions. I had left it harwired to verifyBracketing , was causing some problem in the code. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch199-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Remove erroneous test. Clarify solution with better error handling. Added patch to erroneous integration test .. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch43-math-f6dd42b4_GenProg_PatchNaturalnessYe$$remove clearResult ( ) from BrentSolver. Added verifyBracketing ( min , max , f ). Set p1 = 1 . 0 - r3 instead of p1 = 0 . 5 *. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch351-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Fix erroneous variable. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch163-math-f6dd42b4_GenProg_PatchNaturalnessYe$$Remove redundant clearResult ( ) line. changed max value of BrentSolver to 0 , this way we avoid an NPE. I had left it harwired to verifyBracketing , was causing some problem in the code. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch75-math-f6dd42b4_GenProg_PatchNaturalnessYe$$BrentSolver now uses the patched value ( already provided by valgrind ). Set default function value accuracy to 0 if max is good enough. Fix erroneous test .. Fix erroneous variable. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch190-math-f6dd42b4_GenProg_PatchNaturalnessYe$$I had left it as a last resort .. I had left it harwired to the IGN patch .. revert patch. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch120-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not used. Tweak optimisation for 64 bit KMeans clustering. Fix the build .. Fix NPE in FuzzyKMeansClusterer patch .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch36-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix the build .. added new centroid clustering method. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch127-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not used .. Fix the for loop. added new centroid clustering method. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch31-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not accessible from the. Fix one more test. Throw a NumberIsTooSmallException if fuzziness <= 1 . 0d. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch118-math-596ccd59_GenProg_PatchNaturalnessYe$$added k = number of nodes to the new collection. Fix one more test. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch111-math-596ccd59_GenProg_PatchNaturalnessYe$$fixed 10 / 11 error. Fix the bug in FuzzyKMeansClusterer .. Fix NPE in FuzzyKMeansClusterer patch .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch38-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where the size of the points are zero. Fix the case of new centroid clustering. added new centroid clustering patch. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch116-math-596ccd59_GenProg_PatchNaturalnessYe$$throw exception if size < k. added points = Collections . unmodifiable ( dataPoints ) ;. Remove unused code. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch129-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not accessible from the. Fix membership matrix for newly added k = 1 . 5 new dataset. added missing int .. Remove oversampling .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch189-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix NPE in FuzzyKMeansClusterer , closes # 773. Fix minCluster. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch142-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not accessible from the. Remove oversampling .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch54-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not used. Fix the build .. Fix NPE in FuzzyKMeansClusterer patch .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch98-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix # 1796. Removed patched code. Fix the for loop. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch145-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not used. Fix the build .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch53-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix # 7759. Removed patch for max membership in FuzzyKMeansClusterer. Fix membership matrix .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch65-math-596ccd59_GenProg_PatchNaturalnessYe$$Readded clear ( ) patched by accident. Fix membership matrix .. Fix a bug in FuzzyKMeansClusterer. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch173-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where no points are provided. Remove unused local variable .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch91-math-596ccd59_GenProg_PatchNaturalnessYe$$Don ' t update cluster leaders if size = 0. Fix NPE in FuzzyKMeansClusterer .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch187-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix NPE in FuzzyKMeansClusterer. Fix the for loop. added new centroid clustering bug. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch96-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are too small to split. Set membershipMatrix [ i ] [ j ] = random . nextDouble ( ). $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch180-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix the for loop. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch62-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not used. Fix a bug in FuzzyKMeansClusterer .. Reverted the change in clustering code. Fix the build .. Fix NPE in FuzzyKMeansClusterer patch .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch174-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are too small to split. Fix the bug. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch117-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix 1082 issue. Remove a redundant check. Fix the indentation of FuzzyKMeansClusterer .. Fix NPE in FuzzyKMeansClusterer patch .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch128-math-596ccd59_GenProg_PatchNaturalnessYe$$Remove oversampling .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch110-math-596ccd59_GenProg_PatchNaturalnessYe$$Remove unused line. added new centroid clustering method. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch39-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not used. Fix a bug in FuzzyKMeansClusterer .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch126-math-596ccd59_GenProg_PatchNaturalnessYe$$Add minDistance patch forFUzzyKMeansClusterer .. Fix a bug in FuzzyKMeansClusterer .. Fix minCluster in FuzzyKMeansClusterer .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch119-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer .. Fix minCluster in FuzzyKMeansClusterer .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch121-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not used. Remove unused local variable .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch37-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix NPE in FuzzyKMeansClusterer , closes # 774. Fix the bug in FuzzyKMeansClusterer .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch63-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix NPE in FuzzyKMeansClusterer , closes # 771. Fix the bug in FuzzyKMeansClusterer .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch97-math-596ccd59_GenProg_PatchNaturalnessYe$$Removed patched code. Fixing the build .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch181-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix # 7759. Remove a couple of unused patches. Removed patch from fuzzy kmeans clusterer. Removed unused variable .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch90-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not used. Fix the build .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch186-math-596ccd59_GenProg_PatchNaturalnessYe$$Set minDistance and maxDistance as constants. Remove unused code. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch64-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix swapped line. Remove unused patch. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch172-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not used. Fix a bug in FuzzyKMeansClusterer where neighbors are not used. Fix the for loop. Fixing the build .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch99-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer .. Fix the build .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch144-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not used. Removed patch from fuzzykmeans clusterer. Fix minCluster in FuzzyKMeansClusterer .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch52-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix swapped line. Remove unused code. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch188-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not used. added missing patch. Remove unused local variable .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch143-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not used. Fix the bug. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch55-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not used .. Fix membership matrix for newly added k = 1 . 5 new dataset. Remove unused line. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch203-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not used. Remove unused local variable .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch204-math-596ccd59_GenProg_PatchNaturalnessYe$$Don ' t update cluster leaders if size = 0. Fix the for loop. Fix a bug in FuzzyKMeansClusterer. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch232-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not used .. Fix one more test. Fix the issue of best variance summation in FuzzyKMeansClusterer .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch235-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not used. Set membershipMatrix [ i ] [ j ] = random . nextDouble ( ). $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch295-math-596ccd59_GenProg_PatchNaturalnessYe$$added k = number of nodes to the new collection. Readded missing call to initializeMembershipMatrix ( ) .. Fix the build .. Fix a bug in FuzzyKMeansClusterer. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch261-math-596ccd59_GenProg_PatchNaturalnessYe$$added k = number of nodes to the new collection. Remove unused code. Fix a bug in FuzzyKMeansClusterer. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch259-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix 64 - bit GROOVY - 7759 .. Fix minCluster and maxCluster for FuzzyKMeansClusterer. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch266-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer .. Fix a bug in FuzzyKMeansClusterer where neighbors are not used. Remove unused line. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch292-math-596ccd59_GenProg_PatchNaturalnessYe$$Remove too small exception. Fix the bug. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch250-math-596ccd59_GenProg_PatchNaturalnessYe$$Remove unused code. Fix the merge issue of FuzzyKMeansClusterer .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch268-math-596ccd59_GenProg_PatchNaturalnessYe$$Remove unneeded patch. removed unused variable. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch257-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not accessible from the. Fix the for loop. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch234-math-596ccd59_GenProg_PatchNaturalnessYe$$Add minDistance patch forFUzzyKMeansClusterer .. Remove unused code. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch205-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix the for loop. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch202-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix the for loop. Fix a bug in FuzzyKMeansClusterer. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch256-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not used. Remove unused local variable .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch251-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix # 1796. Fix membership matrix from patched file. Remove oversampling .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch258-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer .. Fix membership matrix .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch293-math-596ccd59_GenProg_PatchNaturalnessYe$$Remove unused code. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch267-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix # 7759. Remove unused local variable .. added missing int. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch294-math-596ccd59_GenProg_PatchNaturalnessYe$$added k = number of nodes to the new collection. Fix minCluster in FuzzyKMeansClusterer .. added new centroid clustering method. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch242-math-596ccd59_GenProg_PatchNaturalnessYe$$added k = number of nodes to the new collection. Fix minCluster used in clustering test. Remove oversampling .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch245-math-596ccd59_GenProg_PatchNaturalnessYe$$throw exception if size < k. Fix the for loop. Fix a bug in FuzzyKMeansClusterer. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch273-math-596ccd59_GenProg_PatchNaturalnessYe$$Removed patched code. Remove redundant code. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch274-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not used. Fix a bug in FuzzyKMeansClusterer where neighbors are not used. Fix the for loop. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch280-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer .. Fix minCluster in FuzzyKMeansClusterer .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch220-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix minCluster in FuzzyKMeansClusterer .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch218-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not accessible from the. Fix int index = 0 ;. Fix minCluster in FuzzyKMeansClusterer .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch227-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not used. Remove patch from fuzzy kmeans clusterer. Removed patch from fuzzykmeans clusterer. Fix membership matrix .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch211-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where the size of the points are zero. Remove unused code. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch229-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not used. Removed unused code. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch216-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where no points are provided. Fix minCluster. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch281-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where no points are provided. Fix the bug in FuzzyKMeansClusterer .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch275-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer .. Fix the bug. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch272-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points were not being taken into. Fix membership matrix for newly added k = 1 . 5 new dataset. Fix membership matrix .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch286-math-596ccd59_GenProg_PatchNaturalnessYe$$Set minDistance so that it is more reliably located .. Removed patch from fuzzykmeans clusterer. Fixing the bug in FuzzyKMeansClusterer .. Fix NPE in FuzzyKMeansClusterer patch .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch244-math-596ccd59_GenProg_PatchNaturalnessYe$$added k = number of nodes to the new collection. Remove unused code. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch288-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix # 7759. Fix the for loop. added new centroid clustering bug. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch228-math-596ccd59_GenProg_PatchNaturalnessYe$$Removed unused variable .. Fix NPE in FuzzyKMeansClusterer . updateMembershipMatrix ( ). $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch217-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where no points are provided. Remove unused code. Fix the merge issue of FuzzyKMeansClusterer .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch210-math-596ccd59_GenProg_PatchNaturalnessYe$$throw exception if size < k. Fixing the build .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch219-math-596ccd59_GenProg_PatchNaturalnessYe$$Remove oversampling .. added a little bit more tidying of the code. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch226-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix NPE in FuzzyKMeansClusterer , closes # 774. Remove unused local variable .. Fix NPE in FuzzyKMeansClusterer .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch221-math-596ccd59_GenProg_PatchNaturalnessYe$$Remove unused code. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch161-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not used. Removed patch from fuzzykmeans clusterer. Fix membership matrix .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch77-math-596ccd59_GenProg_PatchNaturalnessYe$$Set points to null in FuzzyKMeansClusterer , this way we can access the. Fix the build .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch195-math-596ccd59_GenProg_PatchNaturalnessYe$$added missing patch. Fix the for loop. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch83-math-596ccd59_GenProg_PatchNaturalnessYe$$Don ' t reset the points array if no points are provided , as we don ' t have. Remove unused code. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch192-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix # 7759. Don ' t throw NPE if fuzziness < 1 . 0d. Removed patch from fuzzykmeans clusterer. Fixing a bug in FuzzyKMeansClusterer .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch84-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix indentation and code style. Fix the case of one - to - one calls to scaleInPlace ( ) in Fuzzy. added new centroid clustering method. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch8-math-596ccd59_GenProg_PatchNaturalnessYe$$added k = number of nodes to the new collection. Don ' t update cluster leaders if size = 0. Remove unused code. Fix a bug in FuzzyKMeansClusterer. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch159-math-596ccd59_GenProg_PatchNaturalnessYe$$throw exception if size < k. Fix the for loop. Fix a bug in FuzzyKMeansClusterer. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch46-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix # 278. Fix membership matrix for newly added k = 1 . 5 new dataset. Fix membership matrix .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch1-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix the bug in FuzzyKMeansClusterer .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch79-math-596ccd59_GenProg_PatchNaturalnessYe$$Fixing the build .. Fix a bug in FuzzyKMeansClusterer. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch6-math-596ccd59_GenProg_PatchNaturalnessYe$$Remove too small exception. Fix the issue of new centroid clustering. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch41-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where no points are provided , this way. Fix the for loop. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch157-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix maxIterations. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch168-math-596ccd59_GenProg_PatchNaturalnessYe$$Fixing the build .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch103-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer. Remove unused code. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch15-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not listed in the. Fix a bug in FuzzyKMeansClusterer where neighbors are not used. Fix the bug in FuzzyKMeansClusterer .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch104-math-596ccd59_GenProg_PatchNaturalnessYe$$added k = number of nodes to the new collection. Don ' t update cluster leaders if size = 0. Remove unused code. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch12-math-596ccd59_GenProg_PatchNaturalnessYe$$Remove too small exception. Fix a bug in FuzzyKMeansClusterer where the size of the points are zero. Remove unused code. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch300-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not used .. Remove unused code. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch24-math-596ccd59_GenProg_PatchNaturalnessYe$$Remove too small exception. throw exception if size < k. Fix the for loop. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch132-math-596ccd59_GenProg_PatchNaturalnessYe$$Remove a redundant empty line. fixed accidently removed int j = 0 ;. Remove oversampling .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch23-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where no points are provided. Remove oversampling .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch135-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not used .. Removed unused variable .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch40-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not used. Remove unused local variable .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch156-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not used. Fix FUNY_KMeansClusterer .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch169-math-596ccd59_GenProg_PatchNaturalnessYe$$added k = number of nodes to the new collection. Don ' t update cluster leaders if size = 0. Fixing the build .. Fix a bug in FuzzyKMeansClusterer. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch47-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix # 7759. Fix minCluster in FuzzyKMeansClusterer .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch151-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not accessible from the. Fix minCluster in FuzzyKMeansClusterer . java. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch71-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix # 278. Fix the for loop. Fix a bug in FuzzyKMeansClusterer. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch193-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not used .. Fix minCluster in FuzzyKMeansClusterer .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch85-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not used. added missing patch. Remove unused code. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch158-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer .. Set membershipMatrix [ i ] [ j ] = random . nextDouble ( ). $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch9-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not used. Fix minCluster in FuzzyKMeansClusterer .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch194-math-596ccd59_GenProg_PatchNaturalnessYe$$added k = number of nodes to the new collection. Removed patch from fuzzykmeans clusterer. Fixing the bug in FuzzyKMeansClusterer .. Fix a bug in FuzzyKMeansClusterer . updateMembershipMatrix ( ) .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch82-math-596ccd59_GenProg_PatchNaturalnessYe$$Remove too small exception. Remove a couple of unused patches. Remove unused local variable .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch160-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix NPE in FuzzyKMeansClusterer , closes # 773. Fix the for loop. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch76-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer .. Fix membership matrix for newly added k = 1 . 5 new dataset. Fix the for loop. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch22-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix # 7759. Removed patch from fuzzy kmeans clusterer. Remove unused variable .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch134-math-596ccd59_GenProg_PatchNaturalnessYe$$added k = number of nodes to the new collection. Removed patched file. Remove oversampling .. Fix NPE in FuzzyKMeansClusterer. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch25-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not accessible from the. Fix minCluster. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch133-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not accessible from the. Fix minCluster in FuzzyKMeansClusterer .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch105-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not used. Removed patch from fuzzykmeans clusterer. Fix minCluster in FuzzyKMeansClusterer .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch13-math-596ccd59_GenProg_PatchNaturalnessYe$$Remove unneeded patch. Remove oversampling .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch102-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix the for loop. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch14-math-596ccd59_GenProg_PatchNaturalnessYe$$fixed 1014 bug. Remove unused code. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch179-math-596ccd59_GenProg_PatchNaturalnessYe$$Readded missing assignment. Fix minCluster in FuzzyKMeansClusterer .. added new centroid clustering bug. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch146-math-596ccd59_GenProg_PatchNaturalnessYe$$added missing int. Remove unused code. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch68-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix NPE in FuzzyKMeansClusterer , closes # 773. Fix the case of new membership information propagation. Fix membership matrix .. Fix NPE in FuzzyKMeansClusterer patch .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch141-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where the size of the points are zero. Remove unused line. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch57-math-596ccd59_GenProg_PatchNaturalnessYe$$fixed 10 / 10 error. Remove unused line. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch148-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are too small to split. Set membershipMatrix [ i ] [ j ] = random . nextDouble ( ). $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch95-math-596ccd59_GenProg_PatchNaturalnessYe$$Remove unused line. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch183-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix NPE in FuzzyKMeansClusterer. Fix the for loop. added new centroid clustering bug. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch61-math-596ccd59_GenProg_PatchNaturalnessYe$$Don ' t update cluster leaders if size = 0. Fix membership matrix .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch177-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not used .. Removed patch of FuzzyKMeansClusterer , closes # 774. Remove redundant code. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch59-math-596ccd59_GenProg_PatchNaturalnessYe$$added k = number of nodes to the new collection. Don ' t update cluster leaders if size = 0. Fixing the build .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch66-math-596ccd59_GenProg_PatchNaturalnessYe$$Removed unused variable .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch170-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix # 7759. Remove unused code. Fix the issue of best variance summation in FuzzyKMeansClusterer .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch92-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix # 7759. Fix UnboundLocalException in FuzzyKMeansClusterer. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch184-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer. Remove unused code. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch124-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix NPE in FuzzyKMeansClusterer , closes # 773. Remove unused code. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch32-math-596ccd59_GenProg_PatchNaturalnessYe$$Add minDistance patch for FuzzyKMeansClusterer .. Fixing the build .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch123-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not used. Removed patched code. Fix one more test. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch35-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix the for loop. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch115-math-596ccd59_GenProg_PatchNaturalnessYe$$added k = number of nodes to the new collection. Remove unused code. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch112-math-596ccd59_GenProg_PatchNaturalnessYe$$added missing patch. Fix the for loop. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch58-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix NPE in FuzzyKMeansClusterer constructor. Fix the bug in FuzzyKMeansClusterer .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch93-math-596ccd59_GenProg_PatchNaturalnessYe$$Readded missing assignment. Fixing the build .. added new centroid clustering patch. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch185-math-596ccd59_GenProg_PatchNaturalnessYe$$added k = n pairs. Remove unused code. Fix a bug in FuzzyKMeansClusterer. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch67-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not used. Set membershipMatrix [ i ] [ j ] = random . nextDouble ( ). $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch171-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix NPE in FuzzyKMeansClusterer. Fix minCluster. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch149-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix the for loop. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch176-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix # 7759. Remove oversampling .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch94-math-596ccd59_GenProg_PatchNaturalnessYe$$added neighbors list. Fix minCluster. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch182-math-596ccd59_GenProg_PatchNaturalnessYe$$Readded missing patch. Fix membership matrix .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch69-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not used. Set membershipMatrix [ i ] [ j ] = random . nextDouble ( ). Fix NPE in FuzzyKMeansClusterer patch .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch56-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not used. Remove unused code. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch178-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not used. Set membershipMatrix [ i ] [ j ] = random . nextDouble ( ). $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch51-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not accessible from outside. Fix membership matrix for newly added k = 1 .. Fixing the membership matrix used for the first time. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch113-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix the bug. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch122-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where the clustering table was not updated. Remove unused code. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch34-math-596ccd59_GenProg_PatchNaturalnessYe$$throw exception if size < k. Fix the for loop. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch125-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not accessible from the. Fix the bug. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch296-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer .. Fix the bug in FuzzyKMeansClusterer .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch262-math-596ccd59_GenProg_PatchNaturalnessYe$$Add minDistance patch toFUzzyKMeansClusterer , too .. Remove unneeded patch. Fixing the build .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch254-math-596ccd59_GenProg_PatchNaturalnessYe$$Remove too small exception. added missing int. Fix the bug in FuzzyKMeansClusterer .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch253-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where no points are provided. Remove unused local variable .. Fix the merge issue of FuzzyKMeansClusterer .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch207-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not accessible from the. Fix minCluster in FuzzyKMeansClusterer .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch238-math-596ccd59_GenProg_PatchNaturalnessYe$$Don ' t update cluster leaders if size = 0. Fix the for loop. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch200-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not used .. Fix membership matrix for newly added k = 1 . 5 new dataset. added missing int .. Remove oversampling .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch236-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not accessible from the. Don ' t update cluster leaders if size = 0. Fix bug in FuzzyKMeansClusterer .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch209-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix NPE in FuzzyKMeansClusterer , closes # 773. Fix the case of new membership information propagation in FuzzyKMeansClusterer .. Fix the build .. Fix the merge issue of FuzzyKMeansClusterer .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch231-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not used. Updated distSqSum in FuzzyKMeansClusterer. Don ' t throw NPE if fuzziness < 1 . 0d. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch263-math-596ccd59_GenProg_PatchNaturalnessYe$$Updated the iterative implementation of FuzzyKMeansClusterer . addPoint ( ). Remove unused code. Normalize membership matrix .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch290-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix # 7759. Remove unused code. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch264-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are too small to split. Set membershipMatrix [ i ] [ j ] = random . nextDouble ( ). Remove a couple of unused patches. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch230-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not used .. Fix membership matrix from patch. Remove oversampling .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch208-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not used. Removed unused variable .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch201-math-596ccd59_GenProg_PatchNaturalnessYe$$added k = number of nodes to the new collection. Fix minCluster in FuzzyKMeansClusterer .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch206-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where the size of the points are zero. Remove unused line. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch239-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix the for loop. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch223-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not used. Fix NPE in FuzzyKMeansClusterer .. Removed unused code. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch215-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not used .. Fix the bug. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch212-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix NPE in FuzzyKMeansClusterer , closes # 773. Fix the build .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch246-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix NPE in FuzzyKMeansClusterer. Fix the for loop. added new centroid clustering bug. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch241-math-596ccd59_GenProg_PatchNaturalnessYe$$Set minDistance so that it is more reliably located .. Fixed a bug where clustering is not enabled .. Fixing the build .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch277-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not accessible from the. Fix membership matrix for newly added k = 1 . 5 new dataset. Fix minCluster in FuzzyKMeansClusterer .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch283-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix NPE in FuzzyKMeansClusterer , closes # 773. added missing patch. Remove unused code. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch248-math-596ccd59_GenProg_PatchNaturalnessYe$$Remove oversampling. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch284-math-596ccd59_GenProg_PatchNaturalnessYe$$Remove a redundant empty line. Remove oversampling .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch270-math-596ccd59_GenProg_PatchNaturalnessYe$$Remove too small exception. Remove oversampling .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch213-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not used .. Fix the bug. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch214-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not used. removed minCluster from FuzzyKMeansClusterer . java. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch222-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix NPE in FuzzyKMeansClusterer , closes # 773. Removed patch from fuzzy kmeans clusterer. Fix the bug in FuzzyKMeansClusterer .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch225-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer. Using a patched file from Gremlin .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch271-math-596ccd59_GenProg_PatchNaturalnessYe$$added resultSet = new ArrayList < CentroidCluster < T > > ( ) ;. Remove oversampling .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch285-math-596ccd59_GenProg_PatchNaturalnessYe$$Remove too small exception. added missing int. Fix the issue of new centroid clustering. Fix NPE in FuzzyKMeansClusterer. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch249-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not used .. Removed unused variable .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch240-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix minCluster. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch247-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not used .. Fix a bug in FuzzyKMeansClusterer where neighbors are not used. Fix a bug in the membership matrix .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch278-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix indentation and code style. Fix the for loop. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch11-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not used .. Fix the for loop. added new centroid clustering method. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch29-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix # 1796. Remove unused code. Fix the issue of best variance summation in FuzzyKMeansClusterer .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch16-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not used. Removed patch from fuzzykmeans clusterer. Remove too - old merge of one another line. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch20-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where no points are provided. Fix minCluster in FuzzyKMeansClusterer . java. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch18-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not used .. Fix the for loop. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch87-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix # 273. Fix the for loop. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch73-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix NPE in FuzzyKMeansClusterer , closes # 773. Removed patch from fuzzykmeans clusterer. Fix the bug in FuzzyKMeansClusterer .. Fix a bug in FuzzyKMeansClusterer . updateMembershipMatrix ( ) .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch74-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not accessible from the. Remove scaleInPlace ( ) from FuzzyKMeansClusterer . java. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch80-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix # 7759. Remove unused code. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch42-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix # 7759. Fix the for loop. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch5-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix NPE in FuzzyKMeansClusterer. Remove unused code. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch89-math-596ccd59_GenProg_PatchNaturalnessYe$$Remove oversampling .. added new centroid clustering method. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch2-math-596ccd59_GenProg_PatchNaturalnessYe$$fix a crash. Fix the for loop. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch45-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not used. Fix the bug in FuzzyKMeansClusterer .. Fix NPE in FuzzyKMeansClusterer patch .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch19-math-596ccd59_GenProg_PatchNaturalnessYe$$Removed patch from fuzzy kmeans clusterer. Fix a bug in FuzzyKMeansClusterer .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch26-math-596ccd59_GenProg_PatchNaturalnessYe$$added k = number of nodes to the new collection. Fix minCluster and maxCluster for FuzzyKMeansClusterer. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch108-math-596ccd59_GenProg_PatchNaturalnessYe$$Removed patch from fuzzy kmeans clusterer. Fix the bug in FuzzyKMeansClusterer .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch21-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not used .. Fix a bug in the membership matrix .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch28-math-596ccd59_GenProg_PatchNaturalnessYe$$Remove unused code. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch17-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix # 7759. Removed patch from fuzzykmeans clusterer. Fix membership matrix .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch10-math-596ccd59_GenProg_PatchNaturalnessYe$$throw exception if size < k. Fix the for loop. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch44-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix # 7759. don ' t update cluster centers on the fly. Fix membership matrix .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch152-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not used. Fix the build .. Fix NPE in FuzzyKMeansClusterer patch .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch199-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not used .. Remove unused for loop. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch4-math-596ccd59_GenProg_PatchNaturalnessYe$$Updated the code for the clustering test .. Removing minCluster from newly added code. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch43-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not accessible from the. fixed a small bug. Remove oversampling .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch155-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not used. Fix the for loop. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch88-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not used .. Don ' t scale the array .. added new centroid clustering bug. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch197-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix swapped line. Remove unused line. added minCluster to newly added clusters .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch81-math-596ccd59_GenProg_PatchNaturalnessYe$$Set minDistance to Double . MAX_VALUE for FuzzyKMeansClusterer. Remove unused code. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch75-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are not accessible from outside. Fix membership matrix for newly added k = 1 . 5 new dataset. Fixing the bug in FuzzyKMeansClusterer .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch164-math-596ccd59_GenProg_PatchNaturalnessYe$$Remove a redundant empty line. Remove unused code. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch72-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix new feature. Fix a bug in FuzzyKMeansClusterer . updateMembershipMatrix ( ) .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch190-math-596ccd59_GenProg_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer where data points are too small to split. Fix a bug in FuzzyKMeansClusterer .. Using a patched file from Gremlin .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch86-math-596ccd59_GenProg_PatchNaturalnessYe$$added k = number of nodes to the new collection. Fix the for loop. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch120-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix the bug in gamma ( x ). Fix the missing patch. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch127-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix unused local variable. Fix the bug in Gamma where the inverse of the reflection relation is not found .. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch118-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix the warning. Fix the recurrence relation of Gamma ( x ) .. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch111-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix the bug in Gamma where the inverse of 1 . 0 is not greater than - 1. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch116-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix unused local variable. Fix Gamma inverse inverse function. Fix x3max = 0 ;. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch145-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix the bug in Gamma where the inverse of the reflection relation is not found .. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch117-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix unused var. Fix Gamma inverse inverse function. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch128-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix the bug in Gamma where the inverse of 1 . 0 is not greater than 1 .. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch30-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix the recurrence relation of Gamma ( x ) .. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch119-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix the bug in gamma ( x ). Fix the bug in Gamma where the inverse of 1 . 0 is not greater than 1 .. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch121-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix the warning. Fix the case of Gamma ( x ) being > 1 . 0. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch90-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix the bug in gamma ( x ). Fix the bug in Gamma where the inverse of 1 . 0 is not found .. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch144-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix the recurrence relation of Gamma ( x ) .. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch143-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix the warning. Fix the missing patch. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch55-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix the bug in gamma ( x ). Fix Gamma function .. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch48-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix the bug in gamma ( x ). Fix the bug in Gamma where the inverse of 1 . 0 is not greater than - 1. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch1-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix the warning. Fix Gamma inverse inverse function. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch41-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix unused local variable. Fix the bug in Gamma where the inverse of 1 . 0 is not greater than - 1. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch103-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix unused local variable. Fix the recurrence relation in Gamma ( x ) .. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch15-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix unused variable warning. Fix the recurrence relation of Gamma ( x ) .. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch12-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix unused local variable. Fix the bug in Gamma where the inverse of the reflection relation is not found .. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch132-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix the recurrence relation in Gamma ( x ). $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch135-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix the bug in gamma ( x ). Fix the recurrence relation of Gamma ( x ) .. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch40-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix unused var. Fix the bug in Gamma where the inverse of the reflection relation is not found .. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch78-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix the bug in gamma ( x ). Fix the recurrence relation in Gamma ( x ) .. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch71-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix unused local variable. Fix Gamma function. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch85-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix unused local variable. Fix the recurrence relation of Gamma ( x ) .. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch82-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix unused local variable. Fix the case for 1 . 0 / ( 1 . 0 + invGamma1pm1 (. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch76-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix unused variable warning. Fix the bug in Gamma where the inverse of the reflection relation is not found .. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch49-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix Gamma function path. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch134-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix the warning. Fix the recurrence relation of Gamma ( x ) .. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch133-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix the bug in Gamma where the inverse of the reflection relation is not found .. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch105-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix unused local variable. Fix the bug in Gamma where the inverse of the reflection relation is not found .. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch13-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$fix a warning. Fix the recurrence relation of Gamma ( x ) .. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch102-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix unused local variable. Fix the recurrence relation of Gamma ( x ) .. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch14-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix the warning. Fix the recurrence relation in Gamma ( x ) .. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch68-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix unused local variable. Fix the missing patch. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch141-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix unused local variable. Fix the case for Gamma ( x ) where the inverse of the reflection relation is not found. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch124-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix the bug in Gamma .. Fix the bug in Gamma where the inverse of 1 . 0 is not greater than - 1. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch32-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix unused local variable. Fix the bug in Gamma where the inverse of 1 . 0 is not found .. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch35-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix Gamma function path. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch93-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix memory leak. Fix the case for Gamma ( x ) where the inverse of the reflection relation is not found. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch149-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix the bug in gamma ( x ). Fix the bug in Gamma ( x ) .. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch94-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix the bug in Gamma where the inverse of 1 . 0 is not found .. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch140-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix the bug in gamma ( x ). Fix the missing patch. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch56-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix Gamma inverse inverse function. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch113-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix the warning. Fix the gamma function .. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch122-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix unused local variable. Fix the bug in Gamma ( x ) .. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch33-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix unused local variable. Fix the bug in Gamma ( x ) .. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch138-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix the bug in gamma ( x ). Add the nend variable to the gamma function .. Fix the bug in Gamma where the inverse of 1 . 0 is not greater than - 1. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch107-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix unused local variable. Fix the case for Gamma ( x ) where the inverse of the reflection relation is not found. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch100-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix unused var. added fix in Gamma. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch16-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix the warning. Fix the bug in Gamma ( x ) where the inverse of the reflection relation is not found. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch109-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix the bug in gamma ( x ). Fix the bug in Gamma where the inverse of 1 . 0 is not greater than 1 .. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch131-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix the lanczos function. Fix the bug in Gamma where the inverse of the reflection relation is not found .. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch87-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix unused local variable. Fix the recurrence relation found by JDK .. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch73-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix the warning. Fix the recurrence relation of Gamma ( x ) .. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch74-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix the bug in gamma ( x ). Fix the recurrence relation of Gamma ( x ) .. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch2-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix the warning. Fix the bug in Gamma ( x ) where the inverse of the reflection relation is not found. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch45-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix the bug in gamma ( x ). Allow 1 . 5 to be passed to Gamma .. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch153-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix the warning. Fix the recurrence relation in Gamma ( x ) .. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch26-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix unused local variable. Fix the bug in Gamma where the inverse of the reflection relation is not found .. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch108-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix the bug in gamma ( x ). Fix the recurrence relation of Gamma ( x ) .. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch21-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix unused var. Fix the recurrence relation of Gamma ( x ) .. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch28-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$added fix in Gamma function. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch101-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix the missing lanczos fix .. Fix the bug in Gamma where the inverse of 1 . 0 is not greater than - 1. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch106-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix unused local variable. Fix the bug in Gamma where the inverse of the reflection relation is not found .. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch10-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix unused local variable. Fix the bug in Gamma where the inverse of the reflection relation is not found .. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch152-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$Fix the bug in gamma ( x ). Fix the recurrence relation in Gamma ( x ). $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch88-math-9e0c5ad4_GenProg_PatchNaturalnessYe$$fix a warning. Fix the recurrence relation of Gamma ( x ) .. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch1-math-69273dca_GenProg_PatchNaturalnessYe$$Fix for OE. Fix compile error. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch2-math-69273dca_GenProg_PatchNaturalnessYe$$Fix NPE in runge kutta integration loop. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch127-math-a06a1584_GenProg_PatchNaturalnessYe$$removed intermittent crossed point search. fixed bug. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch31-math-a06a1584_GenProg_PatchNaturalnessYe$$Remove a bit of code that was accidentally left in the last commit .. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch118-math-a06a1584_GenProg_PatchNaturalnessYe$$fixed a small bug. Removed debug code. fixed bug. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch38-math-a06a1584_GenProg_PatchNaturalnessYe$$fixed a small bug. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch116-math-a06a1584_GenProg_PatchNaturalnessYe$$fixed bug. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch53-math-a06a1584_GenProg_PatchNaturalnessYe$$fixed a small bug. removed a bit of a hack to fix the merge issue. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch62-math-a06a1584_GenProg_PatchNaturalnessYe$$Fix a small bug in the code. Remove a bit of a debug code. fixed merge issue with prev PR. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch117-math-a06a1584_GenProg_PatchNaturalnessYe$$Fixed a bug in PolyhedronsSet .. fixed bug. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch128-math-a06a1584_GenProg_PatchNaturalnessYe$$updated quat array for 1 . 5. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch110-math-a06a1584_GenProg_PatchNaturalnessYe$$Set the size of the PolyhedronsSet in the temporary file for the compatibility fix. fixed bug. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch30-math-a06a1584_GenProg_PatchNaturalnessYe$$fixed a small bug. Removed a redundant merge of PolyhedronsSet . boundaryFacet. Remove inside attribute from merge of PolyhedronsSet. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch119-math-a06a1584_GenProg_PatchNaturalnessYe$$Removed fixed bug. fixed bug. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch97-math-a06a1584_GenProg_PatchNaturalnessYe$$fixed a small bug. fix merge issue. removed a bit from a jar. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch52-math-a06a1584_GenProg_PatchNaturalnessYe$$Fixed a bug in the PolyhedronsSet .. Removed debug code. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch83-math-a06a1584_GenProg_PatchNaturalnessYe$$fixed a small bug. fixed a small bug. fixed bug. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch48-math-a06a1584_GenProg_PatchNaturalnessYe$$Remove a bit of code that was accidentally left in the last commit .. fixed bug. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch84-math-a06a1584_GenProg_PatchNaturalnessYe$$fixed a small bug. Removed a bit of a rebase. updated far branch quat array. fixed a small bug. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch8-math-a06a1584_GenProg_PatchNaturalnessYe$$Add missing hj array .. fixed a bug in the - - plane constructor. removed a bit of a hack to fix the merge issue. fixed bug. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch1-math-a06a1584_GenProg_PatchNaturalnessYe$$fixed a small bug. Removed a redundant check. Fix Euclidean3D bug. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch6-math-a06a1584_GenProg_PatchNaturalnessYe$$fixed a small bug. removed a redundant empty line. Remove inside attribute from PolyhedronsSet. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch103-math-a06a1584_GenProg_PatchNaturalnessYe$$Remove boundaryFacet from PolyhedronsSet. fixed bug. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch15-math-a06a1584_GenProg_PatchNaturalnessYe$$fixed a minor issue with first intersection. fixed a small bug. Removed a redundant merge of PolyhedronsSet . boundaryFacet. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch104-math-a06a1584_GenProg_PatchNaturalnessYe$$fixed a small bug. Removed a redundant merge of PolyhedronsSet . boundaryFacet. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch12-math-a06a1584_GenProg_PatchNaturalnessYe$$fixed a small bug. Removed a redundant merge of PolyhedronsSet . boundaryFacet. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch132-math-a06a1584_GenProg_PatchNaturalnessYe$$fixed a small bug. updated bug. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch23-math-a06a1584_GenProg_PatchNaturalnessYe$$fixed a small bug. fixed a small bug. removed a bit of a hack to fix the merge issue. Remove inside attribute from merge of PolyhedronsSet. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch40-math-a06a1584_GenProg_PatchNaturalnessYe$$Fixed a bug in the PolyhedronsSet .. fixed a bug in BSP tree edge attribute .. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch76-math-a06a1584_GenProg_PatchNaturalnessYe$$fixed a bug in the same line as the other two points .. Fixed a bit of code with the crossed line being too far from the near branch .. fixed bug. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch22-math-a06a1584_GenProg_PatchNaturalnessYe$$Add missing patch .. fixed a small bug. Removed debug code. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch25-math-a06a1584_GenProg_PatchNaturalnessYe$$Fixed a bug in the PolyhedronsSet .. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch105-math-a06a1584_GenProg_PatchNaturalnessYe$$Set the size of the PolyhedronsSet in the patched code. Removed a bit of a rebase. Remove patched file. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch13-math-a06a1584_GenProg_PatchNaturalnessYe$$Removed debug code. Readded visit to fix merge issue. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch102-math-a06a1584_GenProg_PatchNaturalnessYe$$removed a bit of a hack to fix the merge issue. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch50-math-a06a1584_GenProg_PatchNaturalnessYe$$fixed a small bug. don ' t use crossed branch for now. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch68-math-a06a1584_GenProg_PatchNaturalnessYe$$Remove boundaryFacet from PolyhedronsSet. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch57-math-a06a1584_GenProg_PatchNaturalnessYe$$fixed a small bug. Removed a redundant merge of PolyhedronsSet . boundaryFacet. removed unused code. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch66-math-a06a1584_GenProg_PatchNaturalnessYe$$removed debug code. Remove stray patch code. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch124-math-a06a1584_GenProg_PatchNaturalnessYe$$removed debug code. Remove stray check for inside attribute. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch123-math-a06a1584_GenProg_PatchNaturalnessYe$$Fixed a bug in PolyhedronsSet .. Fix Euclidean geometry test. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch35-math-a06a1584_GenProg_PatchNaturalnessYe$$removed a bit of a hack to fix the merge issue. Remove inside attribute from merge of PolyhedronsSet. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch115-math-a06a1584_GenProg_PatchNaturalnessYe$$removed a bit of a check. fixed bug. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch112-math-a06a1584_GenProg_PatchNaturalnessYe$$removed a bit from a jar. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch113-math-a06a1584_GenProg_PatchNaturalnessYe$$fixed a small bug. Removed debug code. Remove stray check for inside attribute. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch114-math-a06a1584_GenProg_PatchNaturalnessYe$$Removed a bit of a rebase. fixed merge issue with prev PR. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch125-math-a06a1584_GenProg_PatchNaturalnessYe$$Fix a small bug. Remove a bit of code that was accidentally left in the last commit .. fixed merge issue with closed pull request. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch107-math-a06a1584_GenProg_PatchNaturalnessYe$$Removed debug code. fixed bug. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch11-math-a06a1584_GenProg_PatchNaturalnessYe$$fixed a small bug. removed a bit of a hack to fix the merge issue. Remove inside attribute from merge of PolyhedronsSet. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch100-math-a06a1584_GenProg_PatchNaturalnessYe$$Remove a bit of a test code. fixed Euclidean3D bug. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch27-math-a06a1584_GenProg_PatchNaturalnessYe$$removed a bit of a cruft. Remove stray check for inside attribute. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch87-math-a06a1584_GenProg_PatchNaturalnessYe$$fixed a small bug. fixed bug. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch74-math-a06a1584_GenProg_PatchNaturalnessYe$$fixed a bug in the - - plane constructor. Fixed a bug in the near branch of PolyhedronsSet. fixed NPE in PolyhedronsSet. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch80-math-a06a1584_GenProg_PatchNaturalnessYe$$fixed a small bug. updated quat array. fixed a small bug. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch42-math-a06a1584_GenProg_PatchNaturalnessYe$$fixed a bug in the - - plane constructor. Removed a redundant check. fixed bug. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch89-math-a06a1584_GenProg_PatchNaturalnessYe$$fixed a small bug. Fixed a bug in the near branch of PolyhedronsSet. fixed NPE in PolyhedronsSet fixed. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch2-math-a06a1584_GenProg_PatchNaturalnessYe$$fixed a small bug. removed a bit from a jar. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch45-math-a06a1584_GenProg_PatchNaturalnessYe$$fixed a small bug. Removed debug code. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch19-math-a06a1584_GenProg_PatchNaturalnessYe$$fixed a small bug. Remove a couple of debug code. Revert previous patch. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch21-math-a06a1584_GenProg_PatchNaturalnessYe$$fixed a small bug. Removed a redundant merge of PolyhedronsSet . boundaryFacet. fixed bug. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch28-math-a06a1584_GenProg_PatchNaturalnessYe$$updated crossed crossed line. fixed bug. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch106-math-a06a1584_GenProg_PatchNaturalnessYe$$Fixed a bug in PolyhedronsSet .. fixed bug. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch10-math-a06a1584_GenProg_PatchNaturalnessYe$$Fixed a bug in the near branch of PolyhedronsSet. Remove stray check for inside attribute. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch44-math-a06a1584_GenProg_PatchNaturalnessYe$$Fixed a bug in PolyhedronsSet .. Remove stray check for inside attribute. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch4-math-a06a1584_GenProg_PatchNaturalnessYe$$Removed debug code. Remove stray check for inside attribute. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch88-math-a06a1584_GenProg_PatchNaturalnessYe$$fixed a bug in the same line as the other two points on the plane. Fixed a bug in PolyhedronsSet .. fixed bug. $$0
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch31-math-2a6c6409_GenProg_PatchNaturalnessYe$$removed patch from BSPTree. fixed a merge bug in PolygonsSet caused by unecessary close. $$0
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch62-math-2a6c6409_GenProg_PatchNaturalnessYe$$fixed erroneous warning. fixed a bug in the ot package. removed patch from BSPTree. $$0
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch37-math-2a6c6409_GenProg_PatchNaturalnessYe$$fixed a bug in Groovy. removed patch from BSPTree. fixed erroneous warning. $$0
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch64-math-2a6c6409_GenProg_PatchNaturalnessYe$$fixed erroneous warning. fixed a small bug. removed unused patch. fixed issue with inf / negInf in GenProg_Bug_dot_jar_Common. $$0
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch99-math-2a6c6409_GenProg_PatchNaturalnessYe$$fixed a merge bug in PolygonsSet caused by unecessary close. fixed a bug in ot . encode. BSPTree now uses BSPTree . chopOffMinus ( ) , not BSPTree. $$0
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch55-math-2a6c6409_GenProg_PatchNaturalnessYe$$fixed a bit of code. fixed erroneous warning. $$0
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch40-math-2a6c6409_GenProg_PatchNaturalnessYe$$fixed erroneous warning. fixed a bug in OrderedTuple. $$0
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch13-math-2a6c6409_GenProg_PatchNaturalnessYe$$fixed erroneous warning. removed patch from BSPTree. fixed erroneous import. $$0
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch14-math-2a6c6409_GenProg_PatchNaturalnessYe$$fixed erroneous warning. fixed ot . offset = ot . encode ( offset ) ;. removed patch from BSPTree. fixed a small bug. $$0
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch50-math-2a6c6409_GenProg_PatchNaturalnessYe$$removed patch. fixed ot . offset = offset ;. fixed erroneous warning. $$0
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch57-math-2a6c6409_GenProg_PatchNaturalnessYe$$fixed erroneous warning. removed patch. fixed a bug in ot . encode ( ). $$0
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch35-math-2a6c6409_GenProg_PatchNaturalnessYe$$fixed erroneous warning. fixed OE. removed patch from BSPTree. $$0
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch67-math-2a6c6409_GenProg_PatchNaturalnessYe$$fixed erroneous warning. $$0
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch56-math-2a6c6409_GenProg_PatchNaturalnessYe$$fixed ot . offset = offset ;. removed patch from BSPTree. fixed erroneous warning. $$0
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch34-math-2a6c6409_GenProg_PatchNaturalnessYe$$fixed a bug in ot . encode ( ) .. fixed erroneous warning. $$0
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch29-math-2a6c6409_GenProg_PatchNaturalnessYe$$fixed erroneous warning. fixed ot . offset = offset ;. $$0
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch20-math-2a6c6409_GenProg_PatchNaturalnessYe$$fixed scale of OrderedTuple. removed patch from BSPTree. fixed erroneous warning. $$0
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch27-math-2a6c6409_GenProg_PatchNaturalnessYe$$fixed erroneous warning. fixed old bug. removed unused vars. $$0
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch80-math-2a6c6409_GenProg_PatchNaturalnessYe$$fixed erroneous import. removed unused patch. fixed old bug. fixed erroneous warning. fixed a small bug. Change the ExceptionContext . getMessage ( ) to use the default locale .. Missing final modifier .. $$0
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch42-math-2a6c6409_GenProg_PatchNaturalnessYe$$removed unused patch. fixed old bug. fixed erroneous warning. $$0
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch75-math-2a6c6409_GenProg_PatchNaturalnessYe$$fixed erroneous warning. fixed a bug in ot . encode. removed patch from BSPTree. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch145-math-faf99727_GenProg_PatchNaturalnessYe$$Fix a bug in the lineToFrontBall method .. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch91-math-faf99727_GenProg_PatchNaturalnessYe$$Delete some unused code. fix merge issue. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch62-math-faf99727_GenProg_PatchNaturalnessYe$$Delete unsed check for inner ball. fix merge issue. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch128-math-faf99727_GenProg_PatchNaturalnessYe$$Fix a bug in the lineToFrontBall method , too .. removed extraneous whitespace. Save EnclosingBall in case of max distance from the point .. Ignore patterns added by accident. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch119-math-faf99727_GenProg_PatchNaturalnessYe$$Fix NPE in JDK7. removed extraneous whitespace. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch37-math-faf99727_GenProg_PatchNaturalnessYe$$Delete some unused code. removed NPE from extremes. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch64-math-faf99727_GenProg_PatchNaturalnessYe$$Added extreme . add ( points . get ( 0 ) ) to ball .. Delete some unused code. moving EnclosingBall to the front of the list. added support list. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch144-math-faf99727_GenProg_PatchNaturalnessYe$$Delete some debug code. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch52-math-faf99727_GenProg_PatchNaturalnessYe$$Add the missing localization message. Fix a bug in the lineToFrontBall method where the ball was not centered. removed patch from extreme . set ( 0 , pi ). Save saved ball in case of closer bug. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch77-math-faf99727_GenProg_PatchNaturalnessYe$$Delete unsed check. fixed E3E. fixed ExceptionContext . getLocalizedMessage ( ). $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch48-math-faf99727_GenProg_PatchNaturalnessYe$$Fix merge conflict in MathIllegalStateException. Delete some unused code. Fix Rearranger - fix. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch157-math-faf99727_GenProg_PatchNaturalnessYe$$Fix a bug in the lineToFrontBall method where the ball is not restricted to the very. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch132-math-faf99727_GenProg_PatchNaturalnessYe$$Fix a bug in the lineToFrontBall method where the ball is not restricted to the far. removed patch from extremes. Save saved ball in g7c9a9. Add the missing localization message. Ignore patterns added by accident. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch135-math-faf99727_GenProg_PatchNaturalnessYe$$removed patch. Fix EvilSeph ' s patch. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch40-math-faf99727_GenProg_PatchNaturalnessYe$$Fix a bug in the support ArrayList. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch156-math-faf99727_GenProg_PatchNaturalnessYe$$removed patch from code. Fix EvilSeph ' s patch. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch82-math-faf99727_GenProg_PatchNaturalnessYe$$Fix a bug in the lineToFrontBall method .. Improve cleanup of ExceptionContext . format ( Object , String , boolean , boolean ). $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch160-math-faf99727_GenProg_PatchNaturalnessYe$$Fix a bug in the lineToFrontBall method where the ball is not restricted to the very. removed patch from extremes. Save saved ball in g7c9a9. Add the missing localization message. Ignore patterns added by accident. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch102-math-faf99727_GenProg_PatchNaturalnessYe$$Added extreme . add ( points . get ( 0 ) ) to ball .. Delete some unused code. moving extreme ball to the front. added support list. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch14-math-faf99727_GenProg_PatchNaturalnessYe$$Add the missing localization message. Fix a bug in the lineToFrontBall method where the ball is not restricted to the far. removed patch for GROOVY - 816. Save EnclosingBall in case of max distance from the point .. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch57-math-faf99727_GenProg_PatchNaturalnessYe$$Fix NPE in JDK9. Fix Rearranger - fix. removed erroneous patch. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch61-math-faf99727_GenProg_PatchNaturalnessYe$$Fix a bug in the lineToFrontBall method , too .. fix merge issue. Fix a bug in the middle of the beam. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch66-math-faf99727_GenProg_PatchNaturalnessYe$$Delete some unused code. removed NPE in extreme patch. Ignore patterns added by accident. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch124-math-faf99727_GenProg_PatchNaturalnessYe$$Fix a bug in the lineToFrontBall method that was not reliably located .. Fix EvilSeph ' s patch. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch56-math-faf99727_GenProg_PatchNaturalnessYe$$Fix a bug in the lineToFrontBall method , too .. fix merge issue. Updated the fix of the rebase of welzlEncloser .. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch113-math-faf99727_GenProg_PatchNaturalnessYe$$Delete unsed check for inner ball. fix a bug in the extremes of WelzlEncloser .. removed extraneous line. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch122-math-faf99727_GenProg_PatchNaturalnessYe$$Delete some unused code. Removed fixed NPE in WelzlEncloser .. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch109-math-faf99727_GenProg_PatchNaturalnessYe$$Fix NPE in pivotingBall. Fix moveToFrontBall ( ). Fix EvilSeph ' s patch. removed patch from selectFarthest ( ). $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch45-math-faf99727_GenProg_PatchNaturalnessYe$$Improve cleanup of ExceptionContext . format ( Object , String , boolean , boolean ). Delete unsed check. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch19-math-faf99727_GenProg_PatchNaturalnessYe$$Ignore patterns added by accident. Delete some unused code. removed NPE from extremes. Save saved ball in case of closer bug. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch137-math-faf99727_GenProg_PatchNaturalnessYe$$Delete unneeded patch. Fix EvilSeph ' s patch. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch88-math-faf99727_GenProg_PatchNaturalnessYe$$Fix NPE in JDK7. Fix EvilSeph ' s patch .. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch72-math-faf99727_GenProg_PatchNaturalnessYe$$Fix NPE in JSF. $$0
oak-7c320b1e$$issues with JsopBuilder.encode and .escape$$1) escape() escapes many characters that do not need to be escaped (>127)  2) encode() does not encode many control characters that would need to be escaped when read through a JSON parser.$$patch1-oak-7c320b1e_Kali_PatchNaturalnessYe$$"Revert "" fix buffer overflow on Travis """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1-oak-3ce758b7_Kali_PatchNaturalnessYe$$Fix avoid warning. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1-accumulo-a64151e6_Kali_PatchNaturalnessYe$$fix # 268. $$0
accumulo-cd7feb4d$$RegExFilter deepCopy NullPointerException$$If any of the regex matcher objects are null (i.e. for example, if you only specify a regex for the column family), the deepCopy call will throw a NullPointerException.$$patch1-accumulo-cd7feb4d_Kali_PatchNaturalnessYe$$add return. $$0
accumulo-b007b22e$$Combiner default behavior is dangerous$$Currently if the users does not give the combiner any columns to work against, it will work against all columns.  This is dangerous, if a user accidentally forgets to specify columns then their data could be unintentionally corrupted.  Something different needs to be done.    Also classes that extend combiner should call super.validateOptions().$$patch1-accumulo-b007b22e_Kali_PatchNaturalnessYe$$Fix top level delete / merge bug. $$0
accumulo-3d55560a$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch1-accumulo-3d55560a_Kali_PatchNaturalnessYe$$"Revert "" fix merge conflict """. $$0
accumulo-6dbbdc21$$RegExFilter deepCopy NullPointerException$$If any of the regex matcher objects are null (i.e. for example, if you only specify a regex for the column family), the deepCopy call will throw a NullPointerException.$$patch1-accumulo-6dbbdc21_Kali_PatchNaturalnessYe$$add return. $$0
accumulo-94c2a31f$$calling MiniAccumuloCluster.stop multiple times fails with NPE$$On the mailing list [~ctubbsii] mentioned seeing some NPEs in the stderr for {{mvn verify}}.  I see one here when running mvn verify with either hadoop profile:  {quote} Exception in thread "Thread-0" java.lang.NullPointerException 	at org.apache.accumulo.minicluster.MiniAccumuloCluster.stopProcessWithTimeout(MiniAccumuloCluster.java:449) 	at org.apache.accumulo.minicluster.MiniAccumuloCluster.stop(MiniAccumuloCluster.java:376) 	at org.apache.accumulo.minicluster.MiniAccumuloCluster 1.run(MiniAccumuloCluster.java:318) {quote}  The relevant piece of code (in 1.5.2-SNAP) is the {{executor.execute}} below  {code}   private int stopProcessWithTimeout(final Process proc, long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException {     FutureTask<Integer> future = new FutureTask<Integer>(new Callable<Integer>() {         @Override         public Integer call() throws InterruptedException {           proc.destroy();           return proc.waitFor();         }     });      executor.execute(future);      return future.get(timeout, unit);   } {code}  Reading through the code for stop, it nulls out executor when it's done. So the easy way to get an NPE is calling stop() multiple times on a MAC instance. Since we have a shutdown hook that calls stop, that means that a single user invocation of stop should result in a NPE later.  Since start() doesn't allow multiple starts, we probably shouldn't allow multiple stops. That would mean adding logic to the shutdown hook to check if we're already stopped or making a private unguarded version of stop that allows multiple calls and using that from the hook.  criteria for closing this issue:  * MAC should document wether calling stop() multiple times is allowed * fix MAC.stop to either guard against multiple calls or handle them gracefully * find out why this only gets an NPE in one place. Do we rely on the shutdown hook everywhere?$$patch1-accumulo-94c2a31f_Kali_PatchNaturalnessYe$$Fix secondary scan JAR after 90 min. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch1-math-596ccd59_Kali_PatchNaturalnessYe$$Revert this patch if it is still failing .. $$0
math-a7363a2a$$Bug in MonotoneChain: a collinear point landing on the existing boundary should be dropped (patch)$$The is a bug on the code in MonotoneChain.java that attempts to handle the case of a point on the line formed by the previous last points and the last point of the chain being constructed. When `includeCollinearPoints` is false, the point should be dropped entirely. In common-math 3,3, the point is added, which in some cases can cause a `ConvergenceException` to be thrown.  In the patch below, the data points are from a case that showed up in testing before we went to production.  {code:java} Index: src/main/java/org/apache/commons/math3/geometry/euclidean/twod/hull/MonotoneChain.java =================================================================== --- src/main/java/org/apache/commons/math3/geometry/euclidean/twod/hull/MonotoneChain.java	(revision 1609491) +++ src/main/java/org/apache/commons/math3/geometry/euclidean/twod/hull/MonotoneChain.java	(working copy) @@ -160,8 +160,8 @@                  } else {                      if (distanceToCurrent > distanceToLast) {                          hull.remove(size - 1); +                        hull.add(point);                      } -                    hull.add(point);                  }                  return;              } else if (offset > 0) { Index: src/test/java/org/apache/commons/math3/geometry/euclidean/twod/hull/ConvexHullGenerator2DAbstractTest.java =================================================================== --- src/test/java/org/apache/commons/math3/geometry/euclidean/twod/hull/ConvexHullGenerator2DAbstractTest.java	(revision 1609491) +++ src/test/java/org/apache/commons/math3/geometry/euclidean/twod/hull/ConvexHullGenerator2DAbstractTest.java	(working copy) @@ -204,6 +204,24 @@      }        @Test +    public void testCollinnearPointOnExistingBoundary() { +        final Collection<Vector2D> points = new ArrayList<Vector2D>(); +        points.add(new Vector2D(7.3152, 34.7472)); +        points.add(new Vector2D(6.400799999999997, 34.747199999999985)); +        points.add(new Vector2D(5.486399999999997, 34.7472)); +        points.add(new Vector2D(4.876799999999999, 34.7472)); +        points.add(new Vector2D(4.876799999999999, 34.1376)); +        points.add(new Vector2D(4.876799999999999, 30.48)); +        points.add(new Vector2D(6.0959999999999965, 30.48)); +        points.add(new Vector2D(6.0959999999999965, 34.1376)); +        points.add(new Vector2D(7.315199999999996, 34.1376)); +        points.add(new Vector2D(7.3152, 30.48)); + +        final ConvexHull2D hull = generator.generate(points); +        checkConvexHull(points, hull); +    } + +    @Test      public void testIssue1123() {            List<Vector2D> points = new ArrayList<Vector2D>(); {code}$$patch1-math-a7363a2a_Kali_PatchNaturalnessYe$$removed patched code. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch1-math-9e0c5ad4_Kali_PatchNaturalnessYe$$Bump the gamma patch to see if it is still failing after rebase. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch1-math-a06a1584_Kali_PatchNaturalnessYe$$Fixed a bug in PolyhedronsSet .. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch1-math-faf99727_Kali_PatchNaturalnessYe$$Fix a bug in the enclosing Balls. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch36-oak-5286861d_Arja_PatchNaturalnessYe$$remove patch for same parent branch. get back automatic branch root id. "Revert "" update to latest AJAX - level upgrade """. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch118-oak-5286861d_Arja_PatchNaturalnessYe$$remove old map entry. "Revert "" update to latest AJAX - level upgrade """. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch740-oak-5286861d_Arja_PatchNaturalnessYe$$"Revert "" update to latest AJAX - level upgrade """. getHeadRevisionId ( ) didn ' t match any file ( s ) known to git (. remove unused commitTS. remove patched put commit. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch714-oak-5286861d_Arja_PatchNaturalnessYe$$Ooops . . . forgot a conflict in commit builder. remove patched putTokens . remove ( token ). "Revert "" update to latest AJAX - level upgrade """. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch370-oak-5286861d_Arja_PatchNaturalnessYe$$"Revert "" update to latest AJAX - level upgrade """. Remove patched commit .. I was confused , sorry. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch142-oak-5286861d_Arja_PatchNaturalnessYe$$remove wrong branch root id. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch1134-oak-5286861d_Arja_PatchNaturalnessYe$$remove branch root id from commit object. remove old branch head after merge. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch949-oak-5286861d_Arja_PatchNaturalnessYe$$"Revert "" update to latest AJAX - level upgrade """. getHeadRevisionId ( ) didn ' t match any file ( s ) known to git (. Remove patch_149. "Revert "" update patch for 1 . 4 . 0 """. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch1104-oak-5286861d_Arja_PatchNaturalnessYe$$"Revert "" update to latest AJAX - level upgrade """. Remove unused patch. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch203-oak-5286861d_Arja_PatchNaturalnessYe$$Oops , now have a 179 commit code. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch204-oak-5286861d_Arja_PatchNaturalnessYe$$remove branch root id from commit object. Remove unused method. remove old branch head after merge. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch803-oak-5286861d_Arja_PatchNaturalnessYe$$Remove unused method. "Revert "" update to latest AJAX - level upgrade """. getHeadRevisionId ( ) didn ' t match any file ( s ) known to git (. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch1047-oak-5286861d_Arja_PatchNaturalnessYe$$use the same map. "Revert "" update to latest AJAX - level upgrade """. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch651-oak-5286861d_Arja_PatchNaturalnessYe$$only replace one head with the same parent branch. remove wrong branch root id. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch1078-oak-5286861d_Arja_PatchNaturalnessYe$$Improved javadoc comment. remove msg override for merges. "Revert "" update to latest AJAX update """. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch1025-oak-5286861d_Arja_PatchNaturalnessYe$$"Revert "" update to latest AJAX - level upgrade """. getHeadRevisionId ( ) didn ' t match any file ( s ) known to git (. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch401-oak-5286861d_Arja_PatchNaturalnessYe$$remove old branch head after merge. remove wrong branch root id. getHeadRevisionId ( ) didn ' t match any file ( s ) known to git (. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch894-oak-5286861d_Arja_PatchNaturalnessYe$$"Revert "" update to latest AJAX - level upgrade """. remove unused commitTS. remove unused variable. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch289-oak-5286861d_Arja_PatchNaturalnessYe$$remove branch root id from commit object. getHeadRevisionId ( ) didn ' t match any file ( s ) known to git (. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch413-oak-5286861d_Arja_PatchNaturalnessYe$$remove patched putTokens . remove ( token ). remove patched commit. "Revert "" update to latest AJAX - level upgrade """. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch227-oak-5286861d_Arja_PatchNaturalnessYe$$Ooops , really update commit builder. Remove patch from putCommit ( ) method. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch478-oak-5286861d_Arja_PatchNaturalnessYe$$Remove unused method. remove wrong branch root id. "Revert "" update to latest AJAX - level upgrade """. remove old map entry. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch675-oak-5286861d_Arja_PatchNaturalnessYe$$remove branch root id from commit object. remove old map entry. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch1007-oak-5286861d_Arja_PatchNaturalnessYe$$remove extraneous line. "Revert "" update to latest AJAX - level upgrade """. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch412-oak-5286861d_Arja_PatchNaturalnessYe$$remove unused commitTS. remove branch root id from commit object. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch1036-oak-5286861d_Arja_PatchNaturalnessYe$$remove old if / else. Ooops , really set the message. "Revert "" update to latest AJAX - level upgrade """. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch288-oak-5286861d_Arja_PatchNaturalnessYe$$Remove unused method. remove old branch head after merge. "Revert "" update to latest AJAX - level upgrade """. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch618-oak-5286861d_Arja_PatchNaturalnessYe$$"Revert "" update to latest AJAX - level upgrade """. remove old map entry. Oops , now have a 179 commit code. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch821-oak-5286861d_Arja_PatchNaturalnessYe$$"Revert "" update to latest AJAX - level upgrade """. Improve readme. remove patched commit property. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch817-oak-5286861d_Arja_PatchNaturalnessYe$$Oops , now have a 179 commit code. getHeadRevision from patched file. "Revert "" update to latest AJAX - level upgrade """. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch810-oak-5286861d_Arja_PatchNaturalnessYe$$getHeadRevision from patch. "Revert "" update to latest AJAX - level upgrade """. getHeadRevisionId ( ) didn ' t match any file ( s ) known to git (. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch161-oak-5286861d_Arja_PatchNaturalnessYe$$"Revert "" update to latest AJAX - level upgrade """. remove extra line break. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch1126-oak-5286861d_Arja_PatchNaturalnessYe$$remove branch root id from commit object. getHeadRevision from patch_971. "Revert "" update to latest AJAX - level upgrade """. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch1-oak-5286861d_Arja_PatchNaturalnessYe$$"Revert "" update to latest AJAX - level upgrade """. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch157-oak-5286861d_Arja_PatchNaturalnessYe$$"Revert "" update to latest AJAX - level upgrade """. getMD. remove patched putTokens . remove ( token ). $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch103-oak-5286861d_Arja_PatchNaturalnessYe$$I was confused , sorry. "Revert "" update to latest AJAX - level upgrade """. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch594-oak-5286861d_Arja_PatchNaturalnessYe$$"Revert "" update to latest AJAX - level upgrade """. only replace one head with the same parent branch. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch338-oak-5286861d_Arja_PatchNaturalnessYe$$"Revert "" update to latest AJAX - level upgrade """. Remove extra line break. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch1116-oak-5286861d_Arja_PatchNaturalnessYe$$remove old branch head after merging. getMD. "Revert "" update to latest AJAX - level upgrade """. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch363-oak-5286861d_Arja_PatchNaturalnessYe$$Ooops , really set the message. remove unused variable. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch397-oak-5286861d_Arja_PatchNaturalnessYe$$"Revert "" update to latest AJAX - level upgrade """. remove extra line. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch1118-oak-5286861d_Arja_PatchNaturalnessYe$$"Revert "" update to latest AJAX - level upgrade """. removed change . asDiff ( ). $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch50-oak-5286861d_Arja_PatchNaturalnessYe$$remove patched putTokens . remove ( token ). getHeadRevision from patch. "Revert "" update to latest AJAX - level upgrade """. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch380-oak-5286861d_Arja_PatchNaturalnessYe$$remove old map entry. Remove unused method. "Revert "" update to latest AJAX - level upgrade """. getHeadRevisionId ( ) didn ' t match any file ( s ) known to git (. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch1108-oak-5286861d_Arja_PatchNaturalnessYe$$don ' t replace branch root ID with another one. getMD. "Revert "" update to latest AJAX - level upgrade """. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch61-oak-5286861d_Arja_PatchNaturalnessYe$$use the same map. I was confused , sorry. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch513-oak-5286861d_Arja_PatchNaturalnessYe$$I was confused , sorry. remove old head check. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch59-oak-5286861d_Arja_PatchNaturalnessYe$$get back automatic branch root id. remove old branch head after merge. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch578-oak-5286861d_Arja_PatchNaturalnessYe$$"Revert "" update to latest AJAX - level upgrade """. getHeadRevisionId ( ) didn ' t match any file ( s ) known to git (. remove patched commit property. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch112-oak-5286861d_Arja_PatchNaturalnessYe$$Ooops , really set the message. Ooops . . . forgot a conflict in commit builder. "Revert "" update to latest AJAX - level upgrade """. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch576-oak-5286861d_Arja_PatchNaturalnessYe$$getHeadRevision from patch. getHeadRevisionId ( ) didn ' t return an empty revision ID. use the same map. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch727-oak-5286861d_Arja_PatchNaturalnessYe$$Remove patch from putCommit ( ) method. remove old map entry. remove parent reference. remove wrong branch root id. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch343-oak-5286861d_Arja_PatchNaturalnessYe$$remove patched commit property. use the same map. "Revert "" update to latest AJAX - level upgrade """. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch1109-oak-5286861d_Arja_PatchNaturalnessYe$$"Revert "" update to latest AJAX - level upgrade """. remove old branch head after merge. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch94-oak-5286861d_Arja_PatchNaturalnessYe$$use the same map. remove unused commitTS. getHeadRevision from patch_265. "Revert "" update to latest AJAX - level upgrade """. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch114-oak-5286861d_Arja_PatchNaturalnessYe$$"Revert "" update to latest AJAX - level upgrade """. remove branch root id from commit object. remove old head check. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch317-oak-5286861d_Arja_PatchNaturalnessYe$$remove old map entry. getHeadRevision from patched file. "Revert "" update to latest AJAX - level upgrade """. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch1017-oak-5286861d_Arja_PatchNaturalnessYe$$remove patched commits that were not necessary after last commit. "Revert "" update to latest AJAX - level upgrade """. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch1075-oak-5286861d_Arja_PatchNaturalnessYe$$remove patched putTokens . remove ( token ). removed change . asDiff ( ). "Revert "" update to latest AJAX - level upgrade """. getHeadRevisionId ( ) didn ' t match any file ( s ) known to git (. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch469-oak-5286861d_Arja_PatchNaturalnessYe$$Remove patched put commit .. remove old head branch if it is the first branch. "Revert "" update to latest AJAX - level upgrade """. getHeadRevisionId ( ) didn ' t match any file ( s ) known to git (. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch1044-oak-5286861d_Arja_PatchNaturalnessYe$$remove extraneous line. Remove patch from putCommit ( ) method. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch800-oak-5286861d_Arja_PatchNaturalnessYe$$remove extra line. remove patched putTokens . remove ( token ). "Revert "" update to latest AJAX - level upgrade """. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch1027-oak-5286861d_Arja_PatchNaturalnessYe$$"Revert "" update to latest AJAX - level upgrade """. remove old branch head after merging. remove branch root id from commit object. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch1016-oak-5286861d_Arja_PatchNaturalnessYe$$remove msg override for merges. "Revert "" update to latest AJAX - level upgrade """. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch1074-oak-5286861d_Arja_PatchNaturalnessYe$$getHeadRevision from patched file. "Revert "" update to latest AJAX - 1006 """. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch812-oak-5286861d_Arja_PatchNaturalnessYe$$Remove unused method. use the same map. "Revert "" update to latest AJAX - level upgrade """. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch823-oak-5286861d_Arja_PatchNaturalnessYe$$remove temp branch mapping. Remove unused method. "Revert "" update to latest AJAX - level upgrade """. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch1060-oak-5286861d_Arja_PatchNaturalnessYe$$"Revert "" update to latest AJAX - level upgrade """. use the same map. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch682-oak-5286861d_Arja_PatchNaturalnessYe$$remove extra line break. use the same map. "Revert "" update to latest AJAX - level upgrade """. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch212-oak-5286861d_Arja_PatchNaturalnessYe$$remove wrong branch root id. Remove unused method. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch846-oak-5286861d_Arja_PatchNaturalnessYe$$"Revert "" update to latest AJAX - level upgrade """. remove unused commitTS. use the same map. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch1061-oak-5286861d_Arja_PatchNaturalnessYe$$use the same map. remove extraneous line. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch1059-oak-5286861d_Arja_PatchNaturalnessYe$$Remove patched put commit .. "Revert "" update patch for 1 . 4 . 0 """. "Revert "" update to latest AJAX - level upgrade """. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch878-oak-5286861d_Arja_PatchNaturalnessYe$$don ' t replace branch root ID with another one. remove branch root id from commit object. "Revert "" update to latest AJAX - level upgrade """. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch1035-oak-5286861d_Arja_PatchNaturalnessYe$$getHeadRevision from patched file. "Revert "" update to latest AJAX - level upgrade """. Remove unused patch. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch876-oak-5286861d_Arja_PatchNaturalnessYe$$"Revert "" update to latest AJAX - level upgrade """. getHeadRevisionId ( ) didn ' t match any file ( s ) known to git (. remove temp branch mapping. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch138-oak-5286861d_Arja_PatchNaturalnessYe$$remove old branch head after merging. remove unused commitTS. "Revert "" update to latest AJAX - level upgrade """. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch11-oak-5286861d_Arja_PatchNaturalnessYe$$remove branch root id from commit object. Remove patched put commit .. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch100-oak-5286861d_Arja_PatchNaturalnessYe$$remove branch root id from commit object. getHeadRevision from patch. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch758-oak-5286861d_Arja_PatchNaturalnessYe$$remove old branch head after merge. "Revert "" update to latest AJAX - level upgrade """. getHeadRevisionId ( ) didn ' t match any file ( s ) known to git (. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch162-oak-5286861d_Arja_PatchNaturalnessYe$$"Revert "" update to latest AJAX - level upgrade """. getHeadRevisionId ( ) didn ' t match any file ( s ) known to git (. remove unused commitTS. Ooops , really correct the commit object. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch506-oak-5286861d_Arja_PatchNaturalnessYe$$"Revert "" update patch for 1 . 4 . 0 """. "Revert "" update to latest AJAX - level upgrade """. getHeadRevisionId ( ) didn ' t match any file ( s ) known to git (. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch42-oak-5286861d_Arja_PatchNaturalnessYe$$remove branch root id from commit object. use the same map. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch101-oak-5286861d_Arja_PatchNaturalnessYe$$Remove patched commit .. "Revert "" update to latest AJAX - level upgrade """. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch509-oak-5286861d_Arja_PatchNaturalnessYe$$Remove unused method. set gcStart. "Revert "" update to latest AJAX - level upgrade """. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch1115-oak-5286861d_Arja_PatchNaturalnessYe$$"Revert "" update to latest AJAX - level upgrade """. remove changeLog closing. Remove unused method. $$0
oak-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.$$patch369-oak-5286861d_Arja_PatchNaturalnessYe$$don ' t replace branch root ID with another one. remove branch root id from commit object. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1160-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest patch """. Remove patched put token equals method. Remove patch for rebase. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch312-oak-3ce758b7_Arja_PatchNaturalnessYe$$remove patched commitTS. "Revert "" update to latest put token "" after patch """. remove blobStoreNeedsClose. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch315-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest cache size """. "Revert "" update to latest put token "" after patch """. remove blobStoreNeedsClose check. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch924-oak-3ce758b7_Arja_PatchNaturalnessYe$$update checkstyle. don ' t write initial commit. "Revert "" update to latest put token "" after patch """. Remove rethrowing of closed stream objects in patch. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch588-oak-3ce758b7_Arja_PatchNaturalnessYe$$Remove patched files. "Revert "" update to latest head """. Remove patched put token equals method. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch111-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to 2005 - 12 - 29 """. "Revert "" update to latest patch """. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch778-oak-3ce758b7_Arja_PatchNaturalnessYe$$update tmp. "Revert "" update to latest cache with updated memory """. "Revert "" update patch to match class name """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch324-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest cache size """. "Revert "" update to latest patch """. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch129-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest patch """. "Revert "" update to latest oak - mk implements new constructor """. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch526-oak-3ce758b7_Arja_PatchNaturalnessYe$$remove blobStoreNeedsClose. remove patched code. Remove patched put token equals method. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch370-oak-3ce758b7_Arja_PatchNaturalnessYe$$remove fail silently on stop ( ). Remove rethrowing of stderr in case of error. "Revert "" update to latest put token "" after patch """. remove blobStoreNeedsClose. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch713-oak-3ce758b7_Arja_PatchNaturalnessYe$$remove blobStoreNeedsClose. "Revert "" update to latest put token "" after patch """. update tmp. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch983-oak-3ce758b7_Arja_PatchNaturalnessYe$$remove blobStoreNeedsClose. "Revert "" update to latest put token "" after patch """. remove patched file. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1134-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest cache with updated memory """. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch948-oak-3ce758b7_Arja_PatchNaturalnessYe$$update tmp. remove patched commitTS. "Revert "" update to latest patch """. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch574-oak-3ce758b7_Arja_PatchNaturalnessYe$$update tmp. update checkstyle. "Revert "" update to latest put token "" after patch """. remove checkstyle blobstore close. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch770-oak-3ce758b7_Arja_PatchNaturalnessYe$$add patch. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch126-oak-3ce758b7_Arja_PatchNaturalnessYe$$remove blobStoreNeedsClose. Remove rethrowing of deprecated API in DefaultRevisionStore. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1159-oak-3ce758b7_Arja_PatchNaturalnessYe$$remove patched commit ts. "Revert "" update to latest patch """. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch922-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest put token "" after patch """. remove patched file. remove blobStoreNeedsClose. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch347-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest head """. "Revert "" update to latest put token """. Remove patch from tmp. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch97-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest patch """. Remove redundant shutdown method. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch90-oak-3ce758b7_Arja_PatchNaturalnessYe$$remove patched commitTS. Remove redundant shutdown / restore lines. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch186-oak-3ce758b7_Arja_PatchNaturalnessYe$$update tmp. remove patched commit ts. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch947-oak-3ce758b7_Arja_PatchNaturalnessYe$$update cache to not use full disk. Remove patch from tmp. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch978-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest put token "" after patch """. remove patched file. Remove IOUtils . closeQuietly for repo replication. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch52-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest put token "" after patch """. remove patched file. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch520-oak-3ce758b7_Arja_PatchNaturalnessYe$$remove patched file. remove patched commitTS. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch518-oak-3ce758b7_Arja_PatchNaturalnessYe$$don ' t write head twice. Remove patched put token equals method. update tmp. remove blobStoreNeedsClose. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch188-oak-3ce758b7_Arja_PatchNaturalnessYe$$remove patch for hadoop / fs / LengthFileChecksum . java. "Revert "" update to latest cache with updated memory """. "Revert "" update to latest put token """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch715-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest head """. "Revert "" check initialized """. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch55-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" check initialized before attempting to patch """. "Revert "" update to latest patch """. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch385-oak-3ce758b7_Arja_PatchNaturalnessYe$$remove closeQuietly. Remove patched put token equals method. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1085-oak-3ce758b7_Arja_PatchNaturalnessYe$$remove patched file. Remove patched put token equals method. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1082-oak-3ce758b7_Arja_PatchNaturalnessYe$$remove blobStoreNeedsClose. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1076-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" remove marked "" after readIds ( ) """. "Revert "" update to latest put token "" after patching". $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch499-oak-3ce758b7_Arja_PatchNaturalnessYe$$update checkstyle. don ' t write head twice. "Revert "" update to latest put token "" after patch """. remove fail silently on stop ( ). $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1040-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest cache size """. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch497-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" check initialized """. "Revert "" update to latest put token "" after patch """. remove patched gate commit. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1281-oak-3ce758b7_Arja_PatchNaturalnessYe$$update checkstyle. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1078-oak-3ce758b7_Arja_PatchNaturalnessYe$$don ' t write initial commit. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1013-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" check initialized """. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1219-oak-3ce758b7_Arja_PatchNaturalnessYe$$Remove patch from tmp. "Revert "" update to latest put token "" after patch """. remove fail silently on stop ( ). $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1226-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest head """. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1014-oak-3ce758b7_Arja_PatchNaturalnessYe$$remove patch from repo. "Revert "" update to latest put token "" after patching """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch895-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest """. Remove patch_10w1 try {. "Revert "" update to latest head of cache in case of need """. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1022-oak-3ce758b7_Arja_PatchNaturalnessYe$$Remove IOUtils . closeQuietly for patch_908. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch892-oak-3ce758b7_Arja_PatchNaturalnessYe$$Remove patched file. don ' t write head twice. Remove patched put token equals method. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch866-oak-3ce758b7_Arja_PatchNaturalnessYe$$Remove rethrowing of closed streams in ` init ` file. "Revert "" update to latest head """. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1025-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" check initialized before attempting to patch """. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1217-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" check initialized """. "Revert "" update to latest put token "" after patch """. Remove IOUtils . closeQuietly for patch_1268. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch401-oak-3ce758b7_Arja_PatchNaturalnessYe$$remove patch. Remove redundant shutdown / restore of cache in case of error. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch633-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to 2005 - 12 - 29 """. "Revert "" update to latest put token "" after patch """. remove patch. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch650-oak-3ce758b7_Arja_PatchNaturalnessYe$$Remove patch for now. "Revert "" check initialized before attempting to patch """. Remove rethrowing of stderr in case of error. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1280-oak-3ce758b7_Arja_PatchNaturalnessYe$$don ' t write head twice. "Revert "" update to latest cache size """. Remove patch from put token equals method. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1273-oak-3ce758b7_Arja_PatchNaturalnessYe$$remove possible NPE in dependency. "Revert "" update to latest patch """. "Revert "" update to latest put token """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1041-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest patch """. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch465-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest patch """. Remove patched put token equals method. remove blobStoreNeedsClose. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch453-oak-3ce758b7_Arja_PatchNaturalnessYe$$remove blobStoreNeedsClose. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1083-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" check initialized before attempting to patch """. remove patch_139. "Revert "" update to latest cache size """. "Revert "" update patch to match class name """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1084-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest cache """. Remove patch from tmp. "Revert "" update patch to match class name """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch692-oak-3ce758b7_Arja_PatchNaturalnessYe$$don ' t write head twice. "Revert "" update to latest put token "" after patch """. remove blobStoreNeedsClose. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch269-oak-3ce758b7_Arja_PatchNaturalnessYe$$don ' t write initial commit. "Revert "" update to latest patch """. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1229-oak-3ce758b7_Arja_PatchNaturalnessYe$$remove blobStoreNeedsClose. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch858-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest patch """. "Revert "" update patch to match class name """. remove fail silently on stop ( ). $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch632-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" check initialized before attempting to patch """. don ' t write initial commit. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1023-oak-3ce758b7_Arja_PatchNaturalnessYe$$remove blobStoreNeedsClose check. Remove patch for failing test. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch267-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest head """. don ' t write initial commit. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch603-oak-3ce758b7_Arja_PatchNaturalnessYe$$don ' t write head twice. Remove patch from tmp. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1220-oak-3ce758b7_Arja_PatchNaturalnessYe$$shut down the hell for the duration of the upgrade. "Revert "" update to latest cache with updated memory """. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1012-oak-3ce758b7_Arja_PatchNaturalnessYe$$Remove patch from tmp. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1030-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest patch """. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1001-oak-3ce758b7_Arja_PatchNaturalnessYe$$update checkstyle. remove patch_178. "Revert "" update to latest cache size """. "Revert "" update to latest put token """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch617-oak-3ce758b7_Arja_PatchNaturalnessYe$$remove fail silently on stop ( ). "Revert "" update to latest cache size """. "Revert "" update to latest put token """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch280-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" check initialized """. "Revert "" update to latest put token "" after patch """. remove blobStoreNeedsClose check. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch449-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" remove objects . clear ( ) """. "Revert "" update patch to match class name """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch476-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest head """. "Revert "" update to latest put token "" after patch """. remove blobStoreNeedsClose. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch688-oak-3ce758b7_Arja_PatchNaturalnessYe$$update checkstyle. Remove rethrowing of deprecated API in DefaultRevisionStore. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch227-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest put token "" after patch """. remove patch for hadoop / fs / LengthFileChecksum . java. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch829-oak-3ce758b7_Arja_PatchNaturalnessYe$$update tmp. "Revert "" update to latest cache """. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch229-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest cache size """. "Revert "" update to latest """. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch424-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest patch """. "Revert "" update to latest put token """. remove blobStoreNeedsClose. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch620-oak-3ce758b7_Arja_PatchNaturalnessYe$$remove patched commit ts. "Revert "" update to latest oak - mk """. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch618-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest head """. "Revert "" update to latest oak - mk """. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch872-oak-3ce758b7_Arja_PatchNaturalnessYe$$update checkstyle. don ' t write head twice. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch477-oak-3ce758b7_Arja_PatchNaturalnessYe$$Remove patch from tmp. "Revert "" update to latest put token """. "Revert "" update to latest """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch221-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest oak - mk implements new constructor """. "Revert "" update to latest put token """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch353-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest oak - mk implements new constructor """. "Revert "" update to latest put token "" after patch """. Remove IOUtils . closeQuietly for patch_3185 .. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch708-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest patch """. "Revert "" update to latest put token "" after patch """. remove patch for hadoop / fs / LengthFileChecksum . java. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch84-oak-3ce758b7_Arja_PatchNaturalnessYe$$remove patched file. "Revert "" update to latest put token "" after patch """. remove blobStoreNeedsClose. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch354-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest head of cache in case of need """. "Revert "" update to latest patch """. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1119-oak-3ce758b7_Arja_PatchNaturalnessYe$$update checkstyle. "Revert "" update to latest patch """. Remove patched put token equals method. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch365-oak-3ce758b7_Arja_PatchNaturalnessYe$$update tmp. "Revert "" update to 2005 - 12 - 29 """. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch755-oak-3ce758b7_Arja_PatchNaturalnessYe$$update checkstyle. "Revert "" update to latest put token """. update tmp. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch331-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" set rep to null """. "Revert "" check initialized before attempting to patch """. remove patched commitTS. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch938-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest cache with updated memory """. Remove patch from put token equals method. remove blobStoreNeedsClose. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch556-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" check initialized before attempting to patch """. "Revert "" check initialized """. "Revert "" update patch to match class name """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch764-oak-3ce758b7_Arja_PatchNaturalnessYe$$remove fail silently on stop ( ). update checkstyle. "Revert "" check initialized """. "Revert "" update to latest put token """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch24-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest patch """. Remove patched files. Remove patched put token equals method. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch135-oak-3ce758b7_Arja_PatchNaturalnessYe$$don ' t write head twice. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch307-oak-3ce758b7_Arja_PatchNaturalnessYe$$Remove patch from tmp. "Revert "" update to latest put token "" after patch """. remove blobStoreNeedsClose. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch551-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest cache with updated memory """. don ' t write head twice. Remove patched put token equals method. "Revert "" update to latest patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch797-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest head """. "Revert "" update to latest patch """. "Revert "" update to latest put token """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1175-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest cache and shutdown ( ) """. "Revert "" update to latest put token """. remove patched file. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch390-oak-3ce758b7_Arja_PatchNaturalnessYe$$remove fail silently on stop ( ). Remove redundant shutdown method. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch700-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest cache with updated data """. "Revert "" update to latest put token """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch7-oak-3ce758b7_Arja_PatchNaturalnessYe$$don ' t write head twice. "Revert "" update patch to match class name """. remove blobStoreNeedsClose. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch955-oak-3ce758b7_Arja_PatchNaturalnessYe$$Remove patch for now. update cache size to fix # 1227. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch363-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest patch """. "Revert "" check initialized """. Remove patched put token equals method. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch151-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest patch """. "Revert "" update to latest put token "" after patch """. remove closeQuietly. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch71-oak-3ce758b7_Arja_PatchNaturalnessYe$$remove blobStoreNeedsClose. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1118-oak-3ce758b7_Arja_PatchNaturalnessYe$$remove patched file. "Revert "" update to latest patch """. "Revert "" update to latest put token """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1120-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest put token "" after patch """. remove blobStoreNeedsClose. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch306-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" check initialized before attempting to patch """. remove patched dirs. "Revert "" update to latest put token """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch134-oak-3ce758b7_Arja_PatchNaturalnessYe$$Remove patch from tmp. remove patched commit ts. "Revert "" update to latest oak - mk implements new constructor """. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch550-oak-3ce758b7_Arja_PatchNaturalnessYe$$update checkstyle. Remove patched put token equals method. remove fail silently on stop ( ). $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch133-oak-3ce758b7_Arja_PatchNaturalnessYe$$remove blobStoreNeedsClose. "Revert "" check initialized before attempting to patch """. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch308-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest cache size """. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1189-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest patch """. "Revert "" update to latest put token "" after patch """. Remove IOUtils . closeQuietly for repo replication. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch559-oak-3ce758b7_Arja_PatchNaturalnessYe$$don ' t write head twice. Remove redundant shutdown / restore of cache .. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch754-oak-3ce758b7_Arja_PatchNaturalnessYe$$update checkstyle. "Revert "" update to latest oak - mk implements new constructor """. "Revert "" update to latest put token "" after patch """. remove patch for hadoop / fs / LengthFileChecksum . java. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch374-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest patch """. don ' t write initial commit. "Revert "" update to latest patch """. "Revert "" update to latest put token "" after patching """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch989-oak-3ce758b7_Arja_PatchNaturalnessYe$$don ' t write head twice. "Revert "" update to latest oak - mk implements new constructor """. "Revert "" update to latest put token """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1333-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest head """. "Revert "" remove patched opengl opengl opengl opengl opengl opengl opengl opengl opengl opengl opengl opengl. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch717-oak-3ce758b7_Arja_PatchNaturalnessYe$$don ' t write initial commit. "Revert "" check initialized """. "Revert "" update to latest put token """. Remove patched file. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch95-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest """. "Revert "" check initialized before attempting to start". "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch183-oak-3ce758b7_Arja_PatchNaturalnessYe$$Remove patched files. don ' t write head twice. Remove patched put token equals method. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1305-oak-3ce758b7_Arja_PatchNaturalnessYe$$update tmp. update checkstyle. Remove patched put token equals method. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1137-oak-3ce758b7_Arja_PatchNaturalnessYe$$remove patch for same node. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch177-oak-3ce758b7_Arja_PatchNaturalnessYe$$remove blobStoreNeedsClose. "Revert "" update to latest cache with updated memory """. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch721-oak-3ce758b7_Arja_PatchNaturalnessYe$$update checkstyle. update default revision store. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch719-oak-3ce758b7_Arja_PatchNaturalnessYe$$update cache size to not increase in case of an update. "Revert "" update to latest put token "" after patch """. "Revert "" update to latest patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch316-oak-3ce758b7_Arja_PatchNaturalnessYe$$Remove redundant shutdown / restore lines. "Revert "" update to latest put token "" after patch """. remove blobStoreNeedsClose. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1164-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest cache size """. "Revert "" update to latest put token "" after patch """. remove blobStoreNeedsClose. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch786-oak-3ce758b7_Arja_PatchNaturalnessYe$$set gcStart. "Revert "" update to latest put token "" after patching """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch123-oak-3ce758b7_Arja_PatchNaturalnessYe$$remove patch from tmp. remove patch_156. "Revert "" check initialized """. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch578-oak-3ce758b7_Arja_PatchNaturalnessYe$$remove blobStoreNeedsClose check. "Revert "" update to 2005 - 12 - 29 """. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch327-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest cache """. "Revert "" update to latest patch """. "Revert "" update to latest put token """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch929-oak-3ce758b7_Arja_PatchNaturalnessYe$$Remove patched directory. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch788-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest source of authority """. "Revert "" update to latest patch """. "Revert "" update to latest put token """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch916-oak-3ce758b7_Arja_PatchNaturalnessYe$$remove fail silently on stop ( ). "Revert "" update to latest patch """. "Revert "" update to latest put token """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch972-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" check initialized """. "Revert "" update to latest put token "" after patch """. remove patch for hadoop / fs / LengthFileChecksum . java. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch185-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest source / rdoc """. "Revert "" update to 2005 - 12 - 29 """. "Revert "" update to latest master ; """. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch515-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest ( = = snapshot ) """. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch176-oak-3ce758b7_Arja_PatchNaturalnessYe$$update checkstyle. "Revert "" update to latest patch """. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1136-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest patch """. "Revert "" update to latest patch """. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch182-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest patch """. "Revert "" update to latest patch """. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch386-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest patch """. don ' t write head twice. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1138-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest cache """. "Revert "" update to latest patch """. Remove patched put token equals method. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch711-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest head """. Remove patched put token equals method. remove blobStoreNeedsClose check. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch745-oak-3ce758b7_Arja_PatchNaturalnessYe$$don ' t write initial commit. "Revert "" update to latest put token "" after patch """. "Revert "" update to latest patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch548-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest patch """. "Revert "" update to latest put token "" after patch """. remove shut down try catch block. "Revert "" update to latest patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch34-oak-3ce758b7_Arja_PatchNaturalnessYe$$remove blobStoreNeedsClose. "Revert "" update to latest cache with updated memory """. "Revert "" update to latest cache """. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch310-oak-3ce758b7_Arja_PatchNaturalnessYe$$remove patched commit ts. "Revert "" update to latest put token """. remove blobStoreNeedsClose. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch579-oak-3ce758b7_Arja_PatchNaturalnessYe$$Remove patch .. "Revert "" update to latest archive , started with """. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch921-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest patch """. "Revert "" update to latest put token "" after patch """. remove blobStoreNeedsClose. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1165-oak-3ce758b7_Arja_PatchNaturalnessYe$$update checkstyle. "Revert "" update to latest put token "" after patching """. remove patched file. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch787-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest patch """. "Revert "" update to latest head """. remove patch_187. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1028-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to 2005 - 12 - 29 """. "Revert "" update to latest patch """. Remove patched put token equals method. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1225-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest patch """. "Revert "" update to latest put token """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1222-oak-3ce758b7_Arja_PatchNaturalnessYe$$update tmp. "Revert "" update to latest head """. remove patched commit ts. "Revert "" check initialized """. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1010-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest archive , started with """. "Revert "" update to latest put token "" after patching". update tmp. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1019-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest put token "" after patch """. "Revert "" set rep to null """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1021-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" set rep to null """. "Revert "" update to latest put token """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1081-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" set rep to null """. "Revert "" update to latest cache size """. "Revert "" update to latest put token """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1278-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest patch """. "Revert "" update to latest put token """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1088-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest patch """. "Revert "" update to latest patch """. "Revert "" update to latest put token """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch299-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest oak - mk implements new constructor """. "Revert "" update to latest put token "" after patch """. remove blobStoreNeedsClose. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1020-oak-3ce758b7_Arja_PatchNaturalnessYe$$update tmp. "Revert "" update patch to match class name """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch252-oak-3ce758b7_Arja_PatchNaturalnessYe$$remove blobStoreNeedsClose. don ' t write initial commit. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch435-oak-3ce758b7_Arja_PatchNaturalnessYe$$don ' t write head twice. "Revert "" update to latest cache size """. "Revert "" update to latest put token "" after patch """. Remove patch from tmp. remove blobStoreNeedsClose. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1223-oak-3ce758b7_Arja_PatchNaturalnessYe$$remove patched commitTS. "Revert "" update to latest cache size """. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1029-oak-3ce758b7_Arja_PatchNaturalnessYe$$remove fail silently on stop ( ). "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1016-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest head """. Remove patched put token equals method. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1042-oak-3ce758b7_Arja_PatchNaturalnessYe$$Remove redundant shutdown / restore of cache .. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1277-oak-3ce758b7_Arja_PatchNaturalnessYe$$remove blobStoreNeedsClose check. don ' t write initial commit. "Revert "" remove patched opengl opengl opengl opengl opengl opengl opengl opengl opengl opengl opengl opengl. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1248-oak-3ce758b7_Arja_PatchNaturalnessYe$$don ' t write initial commit. Remove patch from tmp. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch486-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest cache with updated memory """. remove patch_120. "Revert "" update to latest put token """. remove patch from tmp. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1056-oak-3ce758b7_Arja_PatchNaturalnessYe$$Remove patched put token equals method. remove blobStoreNeedsClose. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch812-oak-3ce758b7_Arja_PatchNaturalnessYe$$remove patched commitTS. "Revert "" update to latest patch """. "Revert "" update to latest put token "" after patch """. remove blobStoreNeedsClose. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch671-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to 2005 - 06 07 """. remove closeQuietly. "Revert "" update patch to match class name """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1255-oak-3ce758b7_Arja_PatchNaturalnessYe$$don ' t write head twice. "Revert "" update to latest put token "" after patch """. remove fail silently on stop ( ). $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch444-oak-3ce758b7_Arja_PatchNaturalnessYe$$remove closeQuietly. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch848-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest put token "" after patch """. shut down the hell for the duration of the upgrade. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch279-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest cache """. "Revert "" update to latest put token "" after patch """. remove patched file. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch884-oak-3ce758b7_Arja_PatchNaturalnessYe$$Remove redundant shutdown / restore of cache .. "Revert "" check initialized """. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch613-oak-3ce758b7_Arja_PatchNaturalnessYe$$remove unused opengl. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch248-oak-3ce758b7_Arja_PatchNaturalnessYe$$remove blobStoreNeedsClose. "Revert "" update to latest cache size """. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch841-oak-3ce758b7_Arja_PatchNaturalnessYe$$Remove patch from tmp. Remove patch from put token equals method. remove patch. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1095-oak-3ce758b7_Arja_PatchNaturalnessYe$$added patch for readIds. "Revert "" update to 2005 - 06 07 """. don ' t write head twice. "Revert "" update to latest put token """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1061-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest cache """. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch473-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest archive , started with """. "Revert "" update to latest put token """. "Revert "" update to latest patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch878-oak-3ce758b7_Arja_PatchNaturalnessYe$$Remove patch from gate. "Revert "" update to latest cache size """. "Revert "" update patch to match class name """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch885-oak-3ce758b7_Arja_PatchNaturalnessYe$$remove fail silently on stop ( ). "Revert "" update to latest put token """. remove blobStoreNeedsClose. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch429-oak-3ce758b7_Arja_PatchNaturalnessYe$$don ' t write head twice. "Revert "" update to latest patch """. "Revert "" update to latest put token "" after patch """. remove blobStoreNeedsClose. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch623-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to 2005 - 06 07 """. "Revert "" update to latest cache size """. "Revert "" update patch to match class name """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1238-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest patch """. Remove patch from put token equals method. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch564-oak-3ce758b7_Arja_PatchNaturalnessYe$$remove blobStoreNeedsClose check. "Revert "" remove closeQuietly ( pm ) "" for readme". "Revert "" update to latest put token "" after patching """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch100-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch16-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest put token "" after patch """. remove patched gate commit. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1149-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to 2005 - 06 07 """. "Revert "" update to latest patch """. "Revert "" update to latest put token """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1317-oak-3ce758b7_Arja_PatchNaturalnessYe$$remove closeQuietly. Remove patched files. "Revert "" update to latest put token """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch992-oak-3ce758b7_Arja_PatchNaturalnessYe$$remove patched commit ts. "Revert "" check initialized """. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch19-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" check initialized before attempting to patch """. "Revert "" update to latest put token "" after patch """. remove blobStoreNeedsClose. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1170-oak-3ce758b7_Arja_PatchNaturalnessYe$$update gcpm. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch565-oak-3ce758b7_Arja_PatchNaturalnessYe$$don ' t write initial commit. "Revert "" update to latest patch """. Remove patched put token equals method. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch101-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest put token "" after patch """. "Revert "" update to latest patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch17-oak-3ce758b7_Arja_PatchNaturalnessYe$$Remove redundant shutdown / restore lines. "Revert "" update to latest put token "" after patch """. Fix commit of gate. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch333-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to 2005 - 06 07 """. "Revert "" update to latest head """. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1179-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest patch """. "Revert "" update to latest put token "" after patch """. remove blobStoreNeedsClose check. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch334-oak-3ce758b7_Arja_PatchNaturalnessYe$$remove blobStoreNeedsClose. "Revert "" update to latest patch """. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch106-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest put token "" after patch """. remove blobStoreNeedsClose. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch394-oak-3ce758b7_Arja_PatchNaturalnessYe$$remove blobStoreNeedsClose. "Revert "" update to latest patch """. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch509-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" check initialized """. "Revert "" update to latest put token "" after patch """. "Revert "" update to latest patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch199-oak-3ce758b7_Arja_PatchNaturalnessYe$$remove blobStoreNeedsClose check. "Revert "" update to latest patch """. "Revert "" update to latest put token """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch393-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest """. "Revert "" update to latest cache size """. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1327-oak-3ce758b7_Arja_PatchNaturalnessYe$$Remove patch from put token equals method. Remove patch for now. remove closeQuietly. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch4-oak-3ce758b7_Arja_PatchNaturalnessYe$$don ' t write initial commit. "Revert "" update to latest patch """. "Revert "" update patch to match class name """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch155-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" check initialized before attempting to patch """. remove patched commitTS. "Revert "" update to latest oak - mk """. "Revert "" update to latest put token "" after patch """. Remove IOUtils . closeQuietly for patch_248. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch956-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" remove marked "" after readIds ( ) """. "Revert "" update to 2005 - 12 - 29 """. "Revert "" update to latest put token """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch164-oak-3ce758b7_Arja_PatchNaturalnessYe$$remove blobStoreNeedsClose. don ' t write initial commit. Remove patched put token equals method. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch1316-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest cache """. "Revert "" update to latest put token "" after patch """. $$0
oak-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.$$patch967-oak-3ce758b7_Arja_PatchNaturalnessYe$$"Revert "" update to latest put token "" after patch """. remove patch. remove blobStoreNeedsClose. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1160-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove extra backslash. remove patch from jarAccumulo gh test. gh - 454 - removed accidently removed throw. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1193-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove trailing whitespace. gh - 257 - removed empty line. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch315-accumulo-a64151e6_Arja_PatchNaturalnessYe$$gh - 454 - added missing patch. gh - 66 fixed a small bug. gh - 66 not added patch for jar accumulation. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch127-accumulo-a64151e6_Arja_PatchNaturalnessYe$$remove extra loop. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1167-accumulo-a64151e6_Arja_PatchNaturalnessYe$$remove patch from jar accidently removed line. gh - 454 - removed fixed gc rule. gh - 66 fixed a small bug. gh - 66 not added patch for jar accumulation. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch118-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove extra backslash. Fix divide by zero error in GarbageCollectionAlgorithm . java. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1151-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Fix GarbageCollectionAlgorithm uri matcher. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch740-accumulo-a64151e6_Arja_PatchNaturalnessYe$$remove patch for GarbageCollectionAlgorithm. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch53-accumulo-a64151e6_Arja_PatchNaturalnessYe$$gh - 66 fixed a small bug. fixed a small bug. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1159-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove extra backslash. remove patch for GarbageCollectionAlgorithm. Remove patch for jar accumulation. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1353-accumulo-a64151e6_Arja_PatchNaturalnessYe$$remove empty loop. gh - 66 fixed a small bug. gh - 257 - patched < candidates >. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1338-accumulo-a64151e6_Arja_PatchNaturalnessYe$$remove empty loop. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch55-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove a couple incorrect lines from a jar path .. gh - 66 fixed a small bug. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1103-accumulo-a64151e6_Arja_PatchNaturalnessYe$$remove patch for empty collections. Remove patch for GarbageCollectionAlgorithm. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1071-accumulo-a64151e6_Arja_PatchNaturalnessYe$$remove patch for GarbageCollectionAlgorithm. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1047-accumulo-a64151e6_Arja_PatchNaturalnessYe$$remove empty loop. gh - 454 - added missing patch. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch497-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove patch for jar accumulation. Fix divide by zero error in GarbageCollectionAlgorithm . java. gh - 66 fixed a small bug. gh - 66 not added update of GarbageCollectionAlgorithm. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch804-accumulo-a64151e6_Arja_PatchNaturalnessYe$$remove patch for hadoop / accumulo - gc. gh - 66 fixed a small bug. gh - 66 not added update of GarbageCollectionAlgorithm. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1013-accumulo-a64151e6_Arja_PatchNaturalnessYe$$gh - 66 fixed a small bug. gh - 66 not added patch for jar accumulation. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch259-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove patch for jar accumulation. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1484-accumulo-a64151e6_Arja_PatchNaturalnessYe$$remove patch for empty collections. gh - 66 fixed a small bug. gh - 66 not added patch for jar accumulation. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1014-accumulo-a64151e6_Arja_PatchNaturalnessYe$$gh - 66 fixed a small bug. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch406-accumulo-a64151e6_Arja_PatchNaturalnessYe$$remove patch for GarbageCollectionAlgorithm. gh - 454 - removed fixed gc rule. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1446-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove trailing patch. Remove patch for jar accumulation. Fix divide by zero error in GarbageCollectionAlgorithm . java. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1025-accumulo-a64151e6_Arja_PatchNaturalnessYe$$remove patch for GarbageCollectionAlgorithm. gh - 454 - added missing patch. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1217-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove max line length from relPath. remove patch for GarbageCollectionAlgorithm. gh - 454 - added missing patch. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1280-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove a patch for ' Modify ' in ` GarbageCollectionAlgorithm . makeRelative ( ) `. remove patch for GarbageCollectionAlgorithm. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1041-accumulo-a64151e6_Arja_PatchNaturalnessYe$$gh - 66 removed empty line. Remove rethrowing of errors. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch465-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove extra chars. Fix divide by zero error in GarbageCollectionAlgorithm . java. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1048-accumulo-a64151e6_Arja_PatchNaturalnessYe$$gh - 66 fixed a small bug. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch256-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove extra backslash. remove empty loop. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1024-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove extra backslash. Remove patch for jar accumulation. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1023-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove extra backslash. gh - 66 fixed a small bug. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch431-accumulo-a64151e6_Arja_PatchNaturalnessYe$$remove patch for GarbageCollectionAlgorithm. Remove rethrowing of errors. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch409-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove a couple incorrect lines. remove empty loop. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1012-accumulo-a64151e6_Arja_PatchNaturalnessYe$$gh - 66 fixed a small bug in accumulo ' s GC. gh - 454 - added missing patch. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1476-accumulo-a64151e6_Arja_PatchNaturalnessYe$$gh - 66 fixed a small bug. Remove rethrowing of errors. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch873-accumulo-a64151e6_Arja_PatchNaturalnessYe$$gh - 454 - removed fixed gc rule. gh - 66 fixed a small bug. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch887-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove patch for GarbageCollectionAlgorithm. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1037-accumulo-a64151e6_Arja_PatchNaturalnessYe$$gh - 66 fixed bugs. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch273-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove max line length from relPath. Fix GarbageCollectionAlgorithm uri matcher. gh - 66 fixed a small bug. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch628-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove extraneous whitespace. Fix GarbageCollectionAlgorithm uri matcher. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1496-accumulo-a64151e6_Arja_PatchNaturalnessYe$$remove empty loop. gh - 454 - removed new line. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1006-accumulo-a64151e6_Arja_PatchNaturalnessYe$$remove patch for GarbageCollectionAlgorithm. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch449-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove max line length from relPath. Remove patch for empty collections. gh - 454 - added missing patch. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch476-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove patch from relPath. gh - 66 removed empty line. remove patch for GarbageCollectionAlgorithm. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1055-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove patch from relPath. gh - 66 fixed a small bug. gh - 66 not working right now. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch829-accumulo-a64151e6_Arja_PatchNaturalnessYe$$remove patch for GarbageCollectionAlgorithm. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch643-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove patch for jar accumulation. Fix divide by zero error in GarbageCollectionAlgorithm . java. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch478-accumulo-a64151e6_Arja_PatchNaturalnessYe$$remove patch for GarbageCollectionAlgorithm. gh - 454 - removed fixed gc rule. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1407-accumulo-a64151e6_Arja_PatchNaturalnessYe$$remove empty loop. fixed a bug in ' GarbageCollectionAlgorithm '. gh - 66 fixed a small bug. gh - 66 not added patch for jar accumulation. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch211-accumulo-a64151e6_Arja_PatchNaturalnessYe$$remove extra loop. gh - 66 fixed a small bug. gh - 66 not working right now. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1090-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove trailing patch. gh - 66 fixed a small bug. gh - 66 not added patch for jar accumulation. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch440-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove extra lookahead. remove patch for GarbageCollectionAlgorithm. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1400-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove extraneous whitespace. gh - 66 fixed a small bug. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1064-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove a couple incorrect lines from a jar path .. remove patch for GarbageCollectionAlgorithm. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch275-accumulo-a64151e6_Arja_PatchNaturalnessYe$$gh - 66 fixed bugs. gh - 454 - removed fixed gc rule. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch272-accumulo-a64151e6_Arja_PatchNaturalnessYe$$remove patch for GarbageCollectionAlgorithm. gh - 66 fixed a small bug. gh - 66 not working right now. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1000-accumulo-a64151e6_Arja_PatchNaturalnessYe$$gh - 66 - updated patch for jar accumulation. gh - 454 - removed fixed gc rule. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch412-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove patch from relPath. gh - 454 - removed fixed gc rule. gh - 66 fixed a small bug. gh - 66 not added update of GarbageCollectionAlgorithm. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1204-accumulo-a64151e6_Arja_PatchNaturalnessYe$$gh - 66 fixed a small bug. gh - 66 fixed a bug in ' Dir ' directory. gh - 66 - removed patch for class path inAccumulo GC. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1091-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove a couple incorrect lines from a jar path .. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1096-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Fix GarbageCollectionAlgorithm uri matcher. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch219-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove patch for jar accumulation. gh - 454 - removed fixed gc rule. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch689-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove extra loop. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1054-accumulo-a64151e6_Arja_PatchNaturalnessYe$$gh - 66 fixed a small bug. gh - 66 fixed a bug in Groovy. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1098-accumulo-a64151e6_Arja_PatchNaturalnessYe$$remove patch for GarbageCollectionAlgorithm. gh - 454 - added missing patch. gh - 66 fixed a small bug. gh - 66 not added fixed update of candidate map. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch77-accumulo-a64151e6_Arja_PatchNaturalnessYe$$gh - 66 fixed a small bug. gh - 66 fixed a small bug. gh - 66 not added count candidate on line 27. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch505-accumulo-a64151e6_Arja_PatchNaturalnessYe$$gh - 66 fixed a small bug in accumulo ' s GC. gh - 66 fixed a small bug. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1119-accumulo-a64151e6_Arja_PatchNaturalnessYe$$gh - 66 fixed a small bug in accumulo ' s hadoop / gc / GarbageCollectionAlgorithm. gh - 66 fixed a small bug. gh - 454 - removed new line. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1322-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove patch for jar accumulation. remove patch for GarbageCollectionAlgorithm. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1-accumulo-a64151e6_Arja_PatchNaturalnessYe$$gh - 66 fixed a small bug. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch157-accumulo-a64151e6_Arja_PatchNaturalnessYe$$gh - 66 fixed a small bug. gh - 454 - removed fixed gc rule. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1117-accumulo-a64151e6_Arja_PatchNaturalnessYe$$remove patch for GarbageCollectionAlgorithm. gh - 66 fixed a small bug. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch954-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove extraneous whitespace. Remove rethrowing of errors. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1349-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove a couple incorrect lines from a jar path .. remove patch for GarbageCollectionAlgorithm. gh - 257 - added missing patch. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch24-accumulo-a64151e6_Arja_PatchNaturalnessYe$$remove patch for GarbageCollectionAlgorithm. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1116-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove max line length from relPath. gh - 454 - added missing patch. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch134-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove trailing whitespace. remove patch for hadoop / accumulo - gc. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch301-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Fix possible NPE in GarbageCollectionAlgorithm . java. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1173-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove extra backslash. gh - 454 - removed new line. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1377-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Fix possible NPE in GarbageCollectionAlgorithm . java. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1189-accumulo-a64151e6_Arja_PatchNaturalnessYe$$fixed bug in accumulo ' s GarbageCollectionAlgorithm. gh - 454 - added missing patch. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1142-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove patch for jar accumulation. gh - 454 - added missing patch. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch102-accumulo-a64151e6_Arja_PatchNaturalnessYe$$gh - 66 fixed a small bug. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch184-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove the trailing patch. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch775-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove patch from relPath. Fix divide by zero error in GarbageCollectionAlgorithm . java. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch582-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove extraneous whitespace. Remove rethrowing of errors. gh - 66 fixed a small bug. gh - 66 fixed a small bug. gh - 66 not working right now. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch718-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove max line length from relPath. gh - 66 fixed a small bug. gh - 66 fixed a small bug. gh - 66 not working right now. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch93-accumulo-a64151e6_Arja_PatchNaturalnessYe$$remove empty loop. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1136-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove patch for jar accumulation. fixed gc stylesheet. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch182-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove extraneous whitespace. remove unnecessary copy of the array. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch742-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove patch from jar path. gh - 66 fixed a small bug. Remove erroneous throw inside of jar / accumulator. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch584-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove trailing patch. fixed a small bug. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch319-accumulo-a64151e6_Arja_PatchNaturalnessYe$$remove unnecessary copy of the array. gh - 66 fixed a bug in Groovy. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1017-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove extra lookahead. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1010-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove patch for jar accumulation. Remove erroneous throw inside of jar / accumulator. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1474-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove max line length from relPath. gh - 66 fixed bug in accumulo ' s hadoop / gc / GarbageCollectionAlgorithm. fixed gc stylesheet. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch200-accumulo-a64151e6_Arja_PatchNaturalnessYe$$remove patch for GarbageCollectionAlgorithm. gh - 66 fixed a small bug. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1276-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove patch for jar accumulation. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch494-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove a couple incorrect lines from a jar path .. remove patch for GarbageCollectionAlgorithm. gh - 454 - added missing patch. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1420-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove patch from relPath. Remove rethrowing of errors. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1427-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove max line length from relPath. gh - 257 - removed empty line. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1018-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Fix GarbageCollectionAlgorithm uri matcher. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1027-accumulo-a64151e6_Arja_PatchNaturalnessYe$$gh - 454 - removed fixed gc rule. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1011-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove erroneous throw inside of jar / accumulator. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1029-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove max line length from relPath. Fix divide by zero error in GarbageCollectionAlgorithm . java. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1283-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove extraneous whitespace. gh - 66 fixed a small bug. gh - 66 not added update of candidate map. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1277-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove max line length from relPath. Remove an unnecessary empty loop in GarbageCollectionAlgorithm. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch837-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove max line length from relPath. gh - 66 fixed a small bug. gh - 66 not working right now. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1069-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove extra chars. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1297-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove extra lookahead. gh - 66 fixed a small bug. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch481-accumulo-a64151e6_Arja_PatchNaturalnessYe$$gh - 454 - removed accidently removed throw. gh - 66 fixed a small bug. gh - 66 not working right now. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1093-accumulo-a64151e6_Arja_PatchNaturalnessYe$$remove empty loop. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch215-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove extra lookahead. gh - 66 fixed a small bug. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1067-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove patch from relPath. gh - 454 - added patch for jar accumulation. Remove rethrowing of errors. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch649-accumulo-a64151e6_Arja_PatchNaturalnessYe$$remove patch for hadoop / accumulo - gc. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1252-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove max line length from relPath. remove extra loop. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1060-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove max line length from relPath. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1094-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove patch for jar accumulation. Remove rethrowing of errors. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch410-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove max line length from relPath. Remove rethrowing of errors. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch241-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove a couple incorrect lines from a jar path .. remove empty loop. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1468-accumulo-a64151e6_Arja_PatchNaturalnessYe$$gh - 454 - added missing patch. gh - 66 fixed a small bug. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch277-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove extra lookahead. Remove rethrowing of errors. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1005-accumulo-a64151e6_Arja_PatchNaturalnessYe$$gh - 66 fixed a small bug. gh - 454 - removed fixed gc rule. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch284-accumulo-a64151e6_Arja_PatchNaturalnessYe$$remove empty loop. gh - 66 fixed a small bug. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1253-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove max line length from relPath. gh - 66 fixed a small bug. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1050-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove patch for jar accumulation. gh - 454 - removed new line. gh - 66 fixed a small bug. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch814-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Fixed bug in relation to ' AsciiDoc '. gh - 66 fixed a bug in Groovy. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1068-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove extraneous whitespace. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1003-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove patch for jar ' s relative path. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch840-accumulo-a64151e6_Arja_PatchNaturalnessYe$$gh - 66 fixed a small bug. gh - 66 fixed a small bug. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1004-accumulo-a64151e6_Arja_PatchNaturalnessYe$$gh - 454 - added missing patch. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch871-accumulo-a64151e6_Arja_PatchNaturalnessYe$$gh - 66 fixed a bug where jar was empty after adding it. Remove erroneous throw inside of jar / accumulator. gh - 66 fixed a small bug. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1207-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove patch from relPath. gh - 66 fixed bug in accumulo ' s hadoop / gc / GarbageCollectionAlgorithm. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1035-accumulo-a64151e6_Arja_PatchNaturalnessYe$$remove empty loop. gh - 66 fixed a small bug. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch597-accumulo-a64151e6_Arja_PatchNaturalnessYe$$gh - 66 fixed bugs. gh - 66 fixed a bug in ' Dir ' mode. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch11-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove a couple incorrect lines from a jar path .. gh - 454 - added missing patch. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch769-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove max line length from relPath. gh - 66 fixed a bug in Groovy. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch564-accumulo-a64151e6_Arja_PatchNaturalnessYe$$gh - 66 fixed a bug in garbage collection algorithm. gh - 454 - updated patch for jar accumulation. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1386-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove rethrowing of errors. gh - 66 fixed a small bug. gh - 66 not added patch for jar accumulation. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch100-accumulo-a64151e6_Arja_PatchNaturalnessYe$$gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch733-accumulo-a64151e6_Arja_PatchNaturalnessYe$$remove unnecessary copy of the array. Remove rethrowing of errors. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch80-accumulo-a64151e6_Arja_PatchNaturalnessYe$$gh - 66 fixed a small bug. gh - 66 fixed a small bug. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch530-accumulo-a64151e6_Arja_PatchNaturalnessYe$$gh - 66 fixed a small bug in accumulo ' s hadoop / gc / GarbageCollectionAlgorithm. Fix divide by zero error in GarbageCollectionAlgorithm . java. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch42-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove extra backslash. Fix GarbageCollectionAlgorithm uri matcher. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch5-accumulo-a64151e6_Arja_PatchNaturalnessYe$$remove extra loop. gh - 66 fixed a small bug. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1326-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove trailing patch. Remove patch for empty collections. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1319-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove patch from relPath. gh - 66 removed empty line. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1113-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Fix GarbageCollectionAlgorithm uri matcher. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch108-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove accidential modification. gh - 66 fixed a small bug. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1183-accumulo-a64151e6_Arja_PatchNaturalnessYe$$remove patch for GarbageCollectionAlgorithm. Remove erroneous throw inside patch_488. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch902-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove empty loop. gh - 66 fixed a small bug. gh - 66 not added update of GarbageCollectionAlgorithm. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch768-accumulo-a64151e6_Arja_PatchNaturalnessYe$$gh - 66 fixed a small bug. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1373-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove max line length from relPath. Remove patch for GarbageCollectionAlgorithm. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch905-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove extra chars. Remove empty check. gh - 66 fixed a bug in Groovy. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch10-accumulo-a64151e6_Arja_PatchNaturalnessYe$$remove empty loop. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch509-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Fix divide by zero error in GarbageCollectionAlgorithm . java. gh - 66 fixed a small bug. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch1115-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove fixed check for relative GC adjustments. remove patch for GarbageCollectionAlgorithm. gh - 66 fixed a small bug. gh - 257 - removed candidatesSpan . stop ( ) since it ' s still in the. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch393-accumulo-a64151e6_Arja_PatchNaturalnessYe$$remove empty loop. Fix possible NPE in GarbageCollectionAlgorithm . java. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch367-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove extra backslash. gh - 454 - added missing patch. gh - 66 fixed a small bug. $$0
accumulo-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.$$patch86-accumulo-a64151e6_Arja_PatchNaturalnessYe$$Remove patch for jar accumulation. gh - 66 removed patch for jar accumulation. gh - 66 fixed a small bug. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch544-accumulo-15476a0d_Arja_PatchNaturalnessYe$$Remove redundant patch. gh - 66 fixed a small bug. fix missing timestamp in MockTable. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch38-accumulo-15476a0d_Arja_PatchNaturalnessYe$$Fix MockTable count in compareTo. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch572-accumulo-15476a0d_Arja_PatchNaturalnessYe$$Fix MockTable count in compareTo. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch145-accumulo-15476a0d_Arja_PatchNaturalnessYe$$Fix JAR compareTo .. remove erroneous check for table name. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch1105-accumulo-15476a0d_Arja_PatchNaturalnessYe$$Remove erroneous check for table name. gh - 66 fixed a small bug. Remove redundant patch. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch110-accumulo-15476a0d_Arja_PatchNaturalnessYe$$gh - 66 fixed a small bug. remove patched file. remove erroneous check for table name. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch39-accumulo-15476a0d_Arja_PatchNaturalnessYe$$remove erroneous line. gh - 66 fixed a small bug. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch925-accumulo-15476a0d_Arja_PatchNaturalnessYe$$Fix put ( ) method. gh - 66 fixed a small bug. Remove erroneous check for table name. remove erroneous check for table name. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch378-accumulo-15476a0d_Arja_PatchNaturalnessYe$$gh - 66 fixed a small bug. Remove erroneous check for table name. Fix put ( ) method. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch203-accumulo-15476a0d_Arja_PatchNaturalnessYe$$Fix columulo index update patch. Fix MockTable compareTo. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch232-accumulo-15476a0d_Arja_PatchNaturalnessYe$$gh - 66 fixed case with datanucleus scan. remove patch for now. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch1041-accumulo-15476a0d_Arja_PatchNaturalnessYe$$remove erroneous check for table name. gh - 66 fixed a small bug. Fix put ( ). $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch205-accumulo-15476a0d_Arja_PatchNaturalnessYe$$remove patch. gh - 66 fixed a small bug. Fix column visibility for collapsed scans. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch202-accumulo-15476a0d_Arja_PatchNaturalnessYe$$gh - 66 fixed a small bug. Fix columulo index update patch. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch400-accumulo-15476a0d_Arja_PatchNaturalnessYe$$Fix an issue with MockStoreTest . compareTo ( Key ). $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch438-accumulo-15476a0d_Arja_PatchNaturalnessYe$$Remove erroneous check for tableName. remove erroneous check for table name. gh - 66 fixed a small bug. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch258-accumulo-15476a0d_Arja_PatchNaturalnessYe$$remove erroneous check for table name. gh - 66 fixed a small bug. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch1015-accumulo-15476a0d_Arja_PatchNaturalnessYe$$Fix ant build break. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch1030-accumulo-15476a0d_Arja_PatchNaturalnessYe$$Fix put ( ). Fix ant build break. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch619-accumulo-15476a0d_Arja_PatchNaturalnessYe$$Fix an issue with MockStoreTest . compareTo ( Key ). Fix put ( ). $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch245-accumulo-15476a0d_Arja_PatchNaturalnessYe$$Fix put ( ). gh - 66 fixed a small bug. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch1001-accumulo-15476a0d_Arja_PatchNaturalnessYe$$remove erroneous line. gh - 66 fixed a small bug. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch220-accumulo-15476a0d_Arja_PatchNaturalnessYe$$gh - 66 fixed a small bug. remove erroneous check for table name. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch1063-accumulo-15476a0d_Arja_PatchNaturalnessYe$$Fix MockTable age. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch1007-accumulo-15476a0d_Arja_PatchNaturalnessYe$$gh - 66 fixed a small bug. gh - 66 - fixed opengl error. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch1009-accumulo-15476a0d_Arja_PatchNaturalnessYe$$gh - 66 fixed a small bug. remove patch for columulo column update. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch826-accumulo-15476a0d_Arja_PatchNaturalnessYe$$gh - 66 fixed a small bug. gh - 66 fixed opengl error. gh - 66 fixed case with datanucleus scan. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch1065-accumulo-15476a0d_Arja_PatchNaturalnessYe$$Remove erroneous check for table name. Fix put ( ). remove erroneous line. gh - 66 fixed a small bug. fix missing timestamp in MockTable . put ( ). $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch1096-accumulo-15476a0d_Arja_PatchNaturalnessYe$$remove erroneous check for table name. gh - 66 fixed a small bug. gh - 66 fixed opengl error. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch737-accumulo-15476a0d_Arja_PatchNaturalnessYe$$gh - 66 fixed a small bug. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch48-accumulo-15476a0d_Arja_PatchNaturalnessYe$$Fix columulo columulo patch .. fix broken patch. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch1119-accumulo-15476a0d_Arja_PatchNaturalnessYe$$remove erroneous check for table name. gh - 66 fixed a small bug. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch159-accumulo-15476a0d_Arja_PatchNaturalnessYe$$gh - 66 fixed a small bug. gh - 66 - fixed opengl error. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch396-accumulo-15476a0d_Arja_PatchNaturalnessYe$$Fix collocation in MockTable . compareTo ( Key ). gh - 66 fixed opengl mistake. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch1110-accumulo-15476a0d_Arja_PatchNaturalnessYe$$Fix columulo columulo patch. remove patch for now. Remove erroneous check for table name. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch534-accumulo-15476a0d_Arja_PatchNaturalnessYe$$gh - 66 fixed a small bug. gh - 66 fixed opengl error. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch41-accumulo-15476a0d_Arja_PatchNaturalnessYe$$gh - 66 fixed a small bug. Fix put ( ) method. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch1128-accumulo-15476a0d_Arja_PatchNaturalnessYe$$Fix put ( ). gh - 66 fixed a small bug. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch103-accumulo-15476a0d_Arja_PatchNaturalnessYe$$gh - 66 fixed a small bug. remove erroneous line. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch1144-accumulo-15476a0d_Arja_PatchNaturalnessYe$$remove patched addMutation. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch169-accumulo-15476a0d_Arja_PatchNaturalnessYe$$Fix an issue with MockStoreTest . compareTo ( Key ). remove erroneous line. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch738-accumulo-15476a0d_Arja_PatchNaturalnessYe$$gh - 66 fixed a small bug. fix missing timestamp in MockTable . put ( ). remove erroneous check for table name. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch1127-accumulo-15476a0d_Arja_PatchNaturalnessYe$$gh - 66 fixed a small bug. gh - 66 - fixed opengl mistake. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch796-accumulo-15476a0d_Arja_PatchNaturalnessYe$$Fix ant build break. gh - 66 fixed opengl error. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch791-accumulo-15476a0d_Arja_PatchNaturalnessYe$$Fix data file. fix broken patch. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch102-accumulo-15476a0d_Arja_PatchNaturalnessYe$$gh - 66 fixed case with MockConnector. gh - 66 fixed a small bug. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch374-accumulo-15476a0d_Arja_PatchNaturalnessYe$$gh - 66 fixed a small bug. remove patch. remove erroneous check for table name. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch115-accumulo-15476a0d_Arja_PatchNaturalnessYe$$Fix put ( ) method. Fix columulo index update patch. remove erroneous check for table name. gh - 66 fixed a small bug. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch788-accumulo-15476a0d_Arja_PatchNaturalnessYe$$gh - 66 fixed a slight problem. gh - 66 fixed a small bug. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch69-accumulo-15476a0d_Arja_PatchNaturalnessYe$$gh - 66 fixed a small bug. Fix put patch. Fix put ( ). $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch1010-accumulo-15476a0d_Arja_PatchNaturalnessYe$$gh - 66 fixed a small bug. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch1026-accumulo-15476a0d_Arja_PatchNaturalnessYe$$gh - 66 fixed a small bug. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch838-accumulo-15476a0d_Arja_PatchNaturalnessYe$$gh - 66 fixed a small bug. gh - 66 - Updated patch for CASSANDRA - 51. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch1088-accumulo-15476a0d_Arja_PatchNaturalnessYe$$gh - 66 fixed a small bug. gh - 66 - Updated patch for CASSANDRA - 51. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch1029-accumulo-15476a0d_Arja_PatchNaturalnessYe$$Remove erroneous check for tableName. gh - 66 fixed a small bug. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch1016-accumulo-15476a0d_Arja_PatchNaturalnessYe$$Fix put patch. gh - 66 fixed a small bug. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch201-accumulo-15476a0d_Arja_PatchNaturalnessYe$$Remove erroneous check for tableName. gh - 66 fixed a small bug. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch1051-accumulo-15476a0d_Arja_PatchNaturalnessYe$$gh - 66 fixed a small bug. gh - 66 fixed opengl error. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch1093-accumulo-15476a0d_Arja_PatchNaturalnessYe$$gh - 66 fixed a small bug. gh - 66 - fixed opengl error. Remove erroneous check for tableName. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch241-accumulo-15476a0d_Arja_PatchNaturalnessYe$$remove erroneous line. gh - 66 fixed a small bug. Fix put ( ). $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch846-accumulo-15476a0d_Arja_PatchNaturalnessYe$$gh - 66 fixed a small bug. gh - 66 fixed opengl mistake. Remove erroneous check for tableName. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch474-accumulo-15476a0d_Arja_PatchNaturalnessYe$$Fix put ( ). fix broken patch. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch271-accumulo-15476a0d_Arja_PatchNaturalnessYe$$gh - 66 removed patch to fix failing test. remove patch for now. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch282-accumulo-15476a0d_Arja_PatchNaturalnessYe$$gh - 66 fixed a small bug. remove patch for 1 . 5476a0d. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch332-accumulo-15476a0d_Arja_PatchNaturalnessYe$$Fix MockTable compareTo. gh - 66 fixed case with MockConnector. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch1140-accumulo-15476a0d_Arja_PatchNaturalnessYe$$fix merge conflict count bug. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch552-accumulo-15476a0d_Arja_PatchNaturalnessYe$$remove erroneous check for table name. gh - 66 fixed a small bug. remove erroneous line. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch793-accumulo-15476a0d_Arja_PatchNaturalnessYe$$remove erroneous check for table name. Fix MockTable compareTo. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch1125-accumulo-15476a0d_Arja_PatchNaturalnessYe$$gh - 66 fixed a small bug. gh - 66 fixed a small bug. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch162-accumulo-15476a0d_Arja_PatchNaturalnessYe$$Fix MockTable compareTo. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch506-accumulo-15476a0d_Arja_PatchNaturalnessYe$$gh - 66 fixed a small bug. fix missing timestamp in MockTable. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch2-accumulo-15476a0d_Arja_PatchNaturalnessYe$$Fix an issue with MockStoreTest . compareTo ( Key ). Remove erroneous check for tableName. remove erroneous check for table name. $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch1141-accumulo-15476a0d_Arja_PatchNaturalnessYe$$gh - 66 fixed a small bug. gh - 66 - fixed opengl error. Fix put ( ). $$0
accumulo-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.$$patch1115-accumulo-15476a0d_Arja_PatchNaturalnessYe$$gh - 454 - added missing throw. gh - 66 fixed a small bug. gh - 66 - fixed opengl error. $$0
accumulo-b007b22e$$Combiner default behavior is dangerous$$Currently if the users does not give the combiner any columns to work against, it will work against all columns.  This is dangerous, if a user accidentally forgets to specify columns then their data could be unintentionally corrupted.  Something different needs to be done.    Also classes that extend combiner should call super.validateOptions().$$patch120-accumulo-b007b22e_Arja_PatchNaturalnessYe$$Remove unnecessary switch case. remove patched code. $$0
accumulo-b007b22e$$Combiner default behavior is dangerous$$Currently if the users does not give the combiner any columns to work against, it will work against all columns.  This is dangerous, if a user accidentally forgets to specify columns then their data could be unintentionally corrupted.  Something different needs to be done.    Also classes that extend combiner should call super.validateOptions().$$patch1194-accumulo-b007b22e_Arja_PatchNaturalnessYe$$remove old combiner patch. remove accidential modification. remove patch for > => < =. $$0
accumulo-b007b22e$$Combiner default behavior is dangerous$$Currently if the users does not give the combiner any columns to work against, it will work against all columns.  This is dangerous, if a user accidentally forgets to specify columns then their data could be unintentionally corrupted.  Something different needs to be done.    Also classes that extend combiner should call super.validateOptions().$$patch315-accumulo-b007b22e_Arja_PatchNaturalnessYe$$remove patch for > => < =. remove old combiner patch. remove patch for ' findTop ( ) '. $$0
accumulo-b007b22e$$Combiner default behavior is dangerous$$Currently if the users does not give the combiner any columns to work against, it will work against all columns.  This is dangerous, if a user accidentally forgets to specify columns then their data could be unintentionally corrupted.  Something different needs to be done.    Also classes that extend combiner should call super.validateOptions().$$patch1085-accumulo-b007b22e_Arja_PatchNaturalnessYe$$remove unnecessary check for type. remove a redundant check. remove old combiner patch. $$0
accumulo-b007b22e$$Combiner default behavior is dangerous$$Currently if the users does not give the combiner any columns to work against, it will work against all columns.  This is dangerous, if a user accidentally forgets to specify columns then their data could be unintentionally corrupted.  Something different needs to be done.    Also classes that extend combiner should call super.validateOptions().$$patch1076-accumulo-b007b22e_Arja_PatchNaturalnessYe$$remove a / b sign from a combiner. remove patched code. $$0
accumulo-b007b22e$$Combiner default behavior is dangerous$$Currently if the users does not give the combiner any columns to work against, it will work against all columns.  This is dangerous, if a user accidentally forgets to specify columns then their data could be unintentionally corrupted.  Something different needs to be done.    Also classes that extend combiner should call super.validateOptions().$$patch1047-accumulo-b007b22e_Arja_PatchNaturalnessYe$$remove old erroneous fall through for long combiner. remove max / min entries found by accident. remove old combiner patch. $$0
accumulo-b007b22e$$Combiner default behavior is dangerous$$Currently if the users does not give the combiner any columns to work against, it will work against all columns.  This is dangerous, if a user accidentally forgets to specify columns then their data could be unintentionally corrupted.  Something different needs to be done.    Also classes that extend combiner should call super.validateOptions().$$patch1083-accumulo-b007b22e_Arja_PatchNaturalnessYe$$remove max / min entries in jar because they ' re not compatible with java 1 . 5. remove old combiner patch. $$0
accumulo-b007b22e$$Combiner default behavior is dangerous$$Currently if the users does not give the combiner any columns to work against, it will work against all columns.  This is dangerous, if a user accidentally forgets to specify columns then their data could be unintentionally corrupted.  Something different needs to be done.    Also classes that extend combiner should call super.validateOptions().$$patch1084-accumulo-b007b22e_Arja_PatchNaturalnessYe$$remove old combiner patch. remove unnecessary check for type. remove max / min entries found by accident. $$0
accumulo-b007b22e$$Combiner default behavior is dangerous$$Currently if the users does not give the combiner any columns to work against, it will work against all columns.  This is dangerous, if a user accidentally forgets to specify columns then their data could be unintentionally corrupted.  Something different needs to be done.    Also classes that extend combiner should call super.validateOptions().$$patch273-accumulo-b007b22e_Arja_PatchNaturalnessYe$$remove old combiner patch. remove patch for > => < =. $$0
accumulo-b007b22e$$Combiner default behavior is dangerous$$Currently if the users does not give the combiner any columns to work against, it will work against all columns.  This is dangerous, if a user accidentally forgets to specify columns then their data could be unintentionally corrupted.  Something different needs to be done.    Also classes that extend combiner should call super.validateOptions().$$patch1052-accumulo-b007b22e_Arja_PatchNaturalnessYe$$remove aSign == 0 for greater than or equal to 0. remove patch from top level. remove patch for column family iterators. $$0
accumulo-b007b22e$$Combiner default behavior is dangerous$$Currently if the users does not give the combiner any columns to work against, it will work against all columns.  This is dangerous, if a user accidentally forgets to specify columns then their data could be unintentionally corrupted.  Something different needs to be done.    Also classes that extend combiner should call super.validateOptions().$$patch1090-accumulo-b007b22e_Arja_PatchNaturalnessYe$$remove old break. remove old combiner patch. remove findTop ( ) from Combiner . java. $$0
accumulo-b007b22e$$Combiner default behavior is dangerous$$Currently if the users does not give the combiner any columns to work against, it will work against all columns.  This is dangerous, if a user accidentally forgets to specify columns then their data could be unintentionally corrupted.  Something different needs to be done.    Also classes that extend combiner should call super.validateOptions().$$patch1-accumulo-b007b22e_Arja_PatchNaturalnessYe$$remove patch for > => < =. remove old combiner patch. $$0
accumulo-b007b22e$$Combiner default behavior is dangerous$$Currently if the users does not give the combiner any columns to work against, it will work against all columns.  This is dangerous, if a user accidentally forgets to specify columns then their data could be unintentionally corrupted.  Something different needs to be done.    Also classes that extend combiner should call super.validateOptions().$$patch1172-accumulo-b007b22e_Arja_PatchNaturalnessYe$$remove patched code. remove patch for column family iterators. remove patch for > > >. $$0
accumulo-b007b22e$$Combiner default behavior is dangerous$$Currently if the users does not give the combiner any columns to work against, it will work against all columns.  This is dangerous, if a user accidentally forgets to specify columns then their data could be unintentionally corrupted.  Something different needs to be done.    Also classes that extend combiner should call super.validateOptions().$$patch135-accumulo-b007b22e_Arja_PatchNaturalnessYe$$remove old erroneous fall through for long combiner. remove old combiner patch. remove patched findTop. $$0
accumulo-b007b22e$$Combiner default behavior is dangerous$$Currently if the users does not give the combiner any columns to work against, it will work against all columns.  This is dangerous, if a user accidentally forgets to specify columns then their data could be unintentionally corrupted.  Something different needs to be done.    Also classes that extend combiner should call super.validateOptions().$$patch194-accumulo-b007b22e_Arja_PatchNaturalnessYe$$remove old combiner patch. remove old erroneous fall through for long combiner. $$0
accumulo-b007b22e$$Combiner default behavior is dangerous$$Currently if the users does not give the combiner any columns to work against, it will work against all columns.  This is dangerous, if a user accidentally forgets to specify columns then their data could be unintentionally corrupted.  Something different needs to be done.    Also classes that extend combiner should call super.validateOptions().$$patch76-accumulo-b007b22e_Arja_PatchNaturalnessYe$$remove old combiner patch. remove old erroneous fall through for long combiner. remove max / min implementation of LongCombiner. $$0
accumulo-b007b22e$$Combiner default behavior is dangerous$$Currently if the users does not give the combiner any columns to work against, it will work against all columns.  This is dangerous, if a user accidentally forgets to specify columns then their data could be unintentionally corrupted.  Something different needs to be done.    Also classes that extend combiner should call super.validateOptions().$$patch61-accumulo-b007b22e_Arja_PatchNaturalnessYe$$remove old combiner patch. remove patch for column family iterators. remove max / min entries found by accident. $$0
accumulo-b007b22e$$Combiner default behavior is dangerous$$Currently if the users does not give the combiner any columns to work against, it will work against all columns.  This is dangerous, if a user accidentally forgets to specify columns then their data could be unintentionally corrupted.  Something different needs to be done.    Also classes that extend combiner should call super.validateOptions().$$patch585-accumulo-b007b22e_Arja_PatchNaturalnessYe$$remove old combiner patch. remove patch. remove patch for > => < =. $$0
accumulo-b007b22e$$Combiner default behavior is dangerous$$Currently if the users does not give the combiner any columns to work against, it will work against all columns.  This is dangerous, if a user accidentally forgets to specify columns then their data could be unintentionally corrupted.  Something different needs to be done.    Also classes that extend combiner should call super.validateOptions().$$patch972-accumulo-b007b22e_Arja_PatchNaturalnessYe$$remove old combiner patch. remove unnecessary check for type. gh - 66 fixed a bug. $$0
accumulo-b007b22e$$Combiner default behavior is dangerous$$Currently if the users does not give the combiner any columns to work against, it will work against all columns.  This is dangerous, if a user accidentally forgets to specify columns then their data could be unintentionally corrupted.  Something different needs to be done.    Also classes that extend combiner should call super.validateOptions().$$patch1196-accumulo-b007b22e_Arja_PatchNaturalnessYe$$remove old combiner patch. remove patch for column family iterators. $$0
accumulo-b007b22e$$Combiner default behavior is dangerous$$Currently if the users does not give the combiner any columns to work against, it will work against all columns.  This is dangerous, if a user accidentally forgets to specify columns then their data could be unintentionally corrupted.  Something different needs to be done.    Also classes that extend combiner should call super.validateOptions().$$patch1010-accumulo-b007b22e_Arja_PatchNaturalnessYe$$remove old combiner patch. remove patch for column family iterators. $$0
accumulo-b007b22e$$Combiner default behavior is dangerous$$Currently if the users does not give the combiner any columns to work against, it will work against all columns.  This is dangerous, if a user accidentally forgets to specify columns then their data could be unintentionally corrupted.  Something different needs to be done.    Also classes that extend combiner should call super.validateOptions().$$patch1044-accumulo-b007b22e_Arja_PatchNaturalnessYe$$remove old combiner patch. remove patch for > => < =. $$0
accumulo-b007b22e$$Combiner default behavior is dangerous$$Currently if the users does not give the combiner any columns to work against, it will work against all columns.  This is dangerous, if a user accidentally forgets to specify columns then their data could be unintentionally corrupted.  Something different needs to be done.    Also classes that extend combiner should call super.validateOptions().$$patch231-accumulo-b007b22e_Arja_PatchNaturalnessYe$$remove patch from top level jar. remove old erroneous fall through for long combiner. $$0
accumulo-b007b22e$$Combiner default behavior is dangerous$$Currently if the users does not give the combiner any columns to work against, it will work against all columns.  This is dangerous, if a user accidentally forgets to specify columns then their data could be unintentionally corrupted.  Something different needs to be done.    Also classes that extend combiner should call super.validateOptions().$$patch636-accumulo-b007b22e_Arja_PatchNaturalnessYe$$remove a redundant check. remove old combiner patch. remove patch for column family iterators. $$0
accumulo-b007b22e$$Combiner default behavior is dangerous$$Currently if the users does not give the combiner any columns to work against, it will work against all columns.  This is dangerous, if a user accidentally forgets to specify columns then their data could be unintentionally corrupted.  Something different needs to be done.    Also classes that extend combiner should call super.validateOptions().$$patch801-accumulo-b007b22e_Arja_PatchNaturalnessYe$$remove patch for hadoop / accumulo / hadoop / fs / LengthFileChecksum . java. $$0
accumulo-b007b22e$$Combiner default behavior is dangerous$$Currently if the users does not give the combiner any columns to work against, it will work against all columns.  This is dangerous, if a user accidentally forgets to specify columns then their data could be unintentionally corrupted.  Something different needs to be done.    Also classes that extend combiner should call super.validateOptions().$$patch239-accumulo-b007b22e_Arja_PatchNaturalnessYe$$remove old combiner patch. remove accidential modification. remove aSign > bSign. $$0
accumulo-b007b22e$$Combiner default behavior is dangerous$$Currently if the users does not give the combiner any columns to work against, it will work against all columns.  This is dangerous, if a user accidentally forgets to specify columns then their data could be unintentionally corrupted.  Something different needs to be done.    Also classes that extend combiner should call super.validateOptions().$$patch1002-accumulo-b007b22e_Arja_PatchNaturalnessYe$$remove old combiner patch. remove patch for > > >. $$0
accumulo-b007b22e$$Combiner default behavior is dangerous$$Currently if the users does not give the combiner any columns to work against, it will work against all columns.  This is dangerous, if a user accidentally forgets to specify columns then their data could be unintentionally corrupted.  Something different needs to be done.    Also classes that extend combiner should call super.validateOptions().$$patch100-accumulo-b007b22e_Arja_PatchNaturalnessYe$$remove patch for reduce ( ). $$0
accumulo-b007b22e$$Combiner default behavior is dangerous$$Currently if the users does not give the combiner any columns to work against, it will work against all columns.  This is dangerous, if a user accidentally forgets to specify columns then their data could be unintentionally corrupted.  Something different needs to be done.    Also classes that extend combiner should call super.validateOptions().$$patch590-accumulo-b007b22e_Arja_PatchNaturalnessYe$$remove patch for reduce ( ). remove max / min entries in jar because they ' re not compatible with java 1 . 5. $$0
accumulo-b007b22e$$Combiner default behavior is dangerous$$Currently if the users does not give the combiner any columns to work against, it will work against all columns.  This is dangerous, if a user accidentally forgets to specify columns then their data could be unintentionally corrupted.  Something different needs to be done.    Also classes that extend combiner should call super.validateOptions().$$patch10-accumulo-b007b22e_Arja_PatchNaturalnessYe$$remove old combiner patch. remove old erroneous fall through for long combiner. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch912-flink-45fb6d82_Arja_PatchNaturalnessYe$$Remove old patch. Remove unused incompilable code. Remove inconsistent check for group strategy. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch189-flink-45fb6d82_Arja_PatchNaturalnessYe$$Updated toString patch. Remove badly placed patch. Remove inconsistent check for group strategy. Remove redundant initProperties call. Remove redundant merge of GroupReduceWithCombineProperties .. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch941-flink-45fb6d82_Arja_PatchNaturalnessYe$$Remove old patch. Removed patch from source files. Remove inconsistent check for group strategy. Remove redundant patch. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch983-flink-45fb6d82_Arja_PatchNaturalnessYe$$Remove old patch. update marker. Removed patch from source files. Remove inconsistent check for group strategy. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch914-flink-45fb6d82_Arja_PatchNaturalnessYe$$Remove inconsistent check for group strategy. update marker. Remove forced rebalancing from PartitionNode. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch121-flink-45fb6d82_Arja_PatchNaturalnessYe$$Remove local strategy from group reduce node. Remove patch for compiler error. Removed patch for compiler node. Remove unused patch. Remove patch from rgps. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch1103-flink-45fb6d82_Arja_PatchNaturalnessYe$$Remove redundant patch. Remove old patch. Remove redundant merge of GroupReduceWithCombineProperties .. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch455-flink-45fb6d82_Arja_PatchNaturalnessYe$$Updated patch for compiler toString. Remove forced rebalancing from PartitionNode. Remove inconsistent check for group strategy. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch832-flink-45fb6d82_Arja_PatchNaturalnessYe$$Remove redundant patch. Remove old merge conflict. Remove patch from rgps. Updated patch for compiler debug information. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch1082-flink-45fb6d82_Arja_PatchNaturalnessYe$$Allow restrictive plan hints at compile time. Remove redundant patch. Remove inconsistent check for group strategy. Remove redundant initProperties call. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch1049-flink-45fb6d82_Arja_PatchNaturalnessYe$$Remove old patch. Remove inconsistent check for group strategy. Remove unnecessary call to setGroupedFields ( ) in GroupReduceWithCombineProperties. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch1047-flink-45fb6d82_Arja_PatchNaturalnessYe$$Remove forced rebalancing from PartitionNode. update tmp. Fix inconsistent sort for group strategy. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch1013-flink-45fb6d82_Arja_PatchNaturalnessYe$$Remove old patch. Remove inconsistent check for group strategy. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch437-flink-45fb6d82_Arja_PatchNaturalnessYe$$Remove badly placed patch. Fix inconsistent sort for group strategy. Remove old combiner node setDegreeOfParallelism. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch602-flink-45fb6d82_Arja_PatchNaturalnessYe$$Remove inconsistent check for group strategy. Remove old patch. Removed break line. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch406-flink-45fb6d82_Arja_PatchNaturalnessYe$$Removed patch from source files. Remove forced rebalancing from PartitionNode. Fix inconsistent sort for group strategy. Fix bug in GroupReduceWithCombineProperties. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch1048-flink-45fb6d82_Arja_PatchNaturalnessYe$$Remove inconsistent check for group strategy. Remove unnecessary call to setGroupedFields ( ) in GroupReduceWithCombineProperties. Remove patch from rgps. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch833-flink-45fb6d82_Arja_PatchNaturalnessYe$$Remove old patch. Removed two redundant lines. Remove patch .. Fix broken patch. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch856-flink-45fb6d82_Arja_PatchNaturalnessYe$$Remove old patch. Remove inconsistent check for group strategy. Remove over - aggressive merge of GroupReduceWithCombineProperties .. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch1015-flink-45fb6d82_Arja_PatchNaturalnessYe$$Fix inconsistent sort for group strategy. Remove badly placed patch. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch851-flink-45fb6d82_Arja_PatchNaturalnessYe$$Remove a redundant check. Remove old patch. Remove unused incompilable code. Remove inconsistent check for group strategy. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch409-flink-45fb6d82_Arja_PatchNaturalnessYe$$Fix inconsistent sort for group strategy. remove old combiner init property. Remove forced rebalancing from PartitionNode. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch414-flink-45fb6d82_Arja_PatchNaturalnessYe$$Remove old merge conflict. Removed local improvements. Remove patch from rgps. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch220-flink-45fb6d82_Arja_PatchNaturalnessYe$$Remove old patch. remove redundant initProperties call. Remove unused incompilable code. Remove redundant patch. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch471-flink-45fb6d82_Arja_PatchNaturalnessYe$$remove redundant initProperties call. Remove patch from rgps. Remove unused incompilable code. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch1063-flink-45fb6d82_Arja_PatchNaturalnessYe$$Remove inconsistent check for group strategy. Remove old patch. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch1090-flink-45fb6d82_Arja_PatchNaturalnessYe$$Remove redundant patch. update tmp. Remove old patch. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch616-flink-45fb6d82_Arja_PatchNaturalnessYe$$remove old combiner init property. Remove unused patch .. Remove old patch. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch843-flink-45fb6d82_Arja_PatchNaturalnessYe$$remove old combiner init properties. Remove old patch. Remove redundant patch. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch288-flink-45fb6d82_Arja_PatchNaturalnessYe$$Fix a bug inOptimizerNode . toString ( ). Remove old patch. Remove redundant merge of GroupReduceWithCombineProperties .. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch826-flink-45fb6d82_Arja_PatchNaturalnessYe$$Remove redundant merge of GroupReduceWithCombineProperties .. Removed local testing patch. Remove unused incompilable code. Remove old patch. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch70-flink-45fb6d82_Arja_PatchNaturalnessYe$$Remove old patch. Remove inconsistent check for group strategy. remove old combiner init properties. Remove redundant merge of GroupReduceWithCombineProperties .. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch1188-flink-45fb6d82_Arja_PatchNaturalnessYe$$Removed patch for compiler class name. Remove forced rebalancing from PartitionNode. Remove inconsistent check for group strategy. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch76-flink-45fb6d82_Arja_PatchNaturalnessYe$$Remove inconsistent check for group strategy. Remove old patch. Updated patch for compiler test. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch339-flink-45fb6d82_Arja_PatchNaturalnessYe$$Fix inconsistent sort for group strategy. Allow restrictive plan hints at compile time. Remove old patch. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch13-flink-45fb6d82_Arja_PatchNaturalnessYe$$Fix inconsistent sort for group strategy. Remove patched code .. Remove old patch. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch330-flink-45fb6d82_Arja_PatchNaturalnessYe$$Remove inconsistent check for group strategy. Remove patch .. Remove patch from rgps. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch717-flink-45fb6d82_Arja_PatchNaturalnessYe$$Remove redundant patch. Remove old patch. Remove old toString patch. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch373-flink-45fb6d82_Arja_PatchNaturalnessYe$$Remove badly placed patch. Remove inconsistent check for group strategy. Remove old merge conflict. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch345-flink-45fb6d82_Arja_PatchNaturalnessYe$$Removed patch from source files. Remove old patch. Remove inconsistent check for group strategy. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch540-flink-45fb6d82_Arja_PatchNaturalnessYe$$Removed patch for compiler class name. Remove patch from rgps. Remove redundant patch. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch32-flink-45fb6d82_Arja_PatchNaturalnessYe$$Remove old patch. Remove unused incompilable code. Remove local strategy from group reduce node. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch1163-flink-45fb6d82_Arja_PatchNaturalnessYe$$Remove old patch. Remove unused incompilable code. Remove redundant patch. Remove old merge conflict. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch1136-flink-45fb6d82_Arja_PatchNaturalnessYe$$Remove inconsistent check for group strategy. Removed patch from source files. Updated toString patch. Remove old patch. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch321-flink-45fb6d82_Arja_PatchNaturalnessYe$$Remove redundant patch. Remove old patch. Remove unused patch. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch742-flink-45fb6d82_Arja_PatchNaturalnessYe$$Remove old patch. Removed local testing. Remove redundant patch. Remove redundant merge of GroupReduceWithCombineProperties .. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch1017-flink-45fb6d82_Arja_PatchNaturalnessYe$$Fix swapped joins. Remove redundant patch. Remove old patch. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch891-flink-45fb6d82_Arja_PatchNaturalnessYe$$Removed patch fromOptimizerNode. Remove old patch. Remove old merge conflict. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch1072-flink-45fb6d82_Arja_PatchNaturalnessYe$$Remove local strategy from group reduce node. Remove old patch. Updated patch for compiler test. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch655-flink-45fb6d82_Arja_PatchNaturalnessYe$$Remove old patch. Allow restrictive plan hints at compile time. Remove unused incompilable code. Remove inconsistent check for group strategy. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch1043-flink-45fb6d82_Arja_PatchNaturalnessYe$$Fix inconsistent sort for group strategy. Remove old patch. Removed local improvements. Remove unused patch. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch863-flink-45fb6d82_Arja_PatchNaturalnessYe$$Remove inconsistent check for group strategy. Remove patch from plan graph. Remove unused patch .. Remove patch from rgps. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch1027-flink-45fb6d82_Arja_PatchNaturalnessYe$$Removed two redundant lines. Remove unused patch. Remove old patch. Remove redundant merge of GroupReduceWithCombineProperties .. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch1029-flink-45fb6d82_Arja_PatchNaturalnessYe$$Remove forced rebalancing from PartitionNode. Removed patch from source files. Remove inconsistent check for group strategy. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch230-flink-45fb6d82_Arja_PatchNaturalnessYe$$Remove old patch. Removed local improvements. remove redundant initProperties call. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch812-flink-45fb6d82_Arja_PatchNaturalnessYe$$Remove inconsistent check for group strategy. Fix bug in GroupReduceWithCombineProperties. Removed a couple of unused lines. Remove patch from rgps. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch1060-flink-45fb6d82_Arja_PatchNaturalnessYe$$Remove old patch. Remove redundant patch. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch1206-flink-45fb6d82_Arja_PatchNaturalnessYe$$Remove redundant initProperties call. Updated patch for compiler test. Remove old patch. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch241-flink-45fb6d82_Arja_PatchNaturalnessYe$$Remove old patch. Remove unused patch. Fix inconsistent sort for group strategy. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch417-flink-45fb6d82_Arja_PatchNaturalnessYe$$Remove forced rebalancing from PartitionNode. update marker. Remove redundant patch. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch213-flink-45fb6d82_Arja_PatchNaturalnessYe$$Fix inconsistent sort for group strategy. Remove forced rebalancing from PartitionNode. Removed break line. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch1050-flink-45fb6d82_Arja_PatchNaturalnessYe$$Remove inconsistent check for group strategy. Remove old patch. update marker. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch474-flink-45fb6d82_Arja_PatchNaturalnessYe$$Fix inconsistent sort for group strategy. Remove old patch. Removed local improvements. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch1003-flink-45fb6d82_Arja_PatchNaturalnessYe$$Remove redundant initProperties call. Remove forced rebalancing from PartitionNode. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch1207-flink-45fb6d82_Arja_PatchNaturalnessYe$$Remove redundant patch. Remove forced rebalancing from PartitionNode. Remove inconsistent check for group strategy. Remove redundant patch. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch165-flink-45fb6d82_Arja_PatchNaturalnessYe$$Remove unused incompilable code. Remove old patch. Remove redundant patch. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch705-flink-45fb6d82_Arja_PatchNaturalnessYe$$Remove old patch. Remove redundant patch. Updated toString patch. Remove inconsistent check for group strategy. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch792-flink-45fb6d82_Arja_PatchNaturalnessYe$$Fix inconsistent sort for group strategy. Fix bug in GroupReduceWithCombineProperties. update tmp. Remove old patch. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch768-flink-45fb6d82_Arja_PatchNaturalnessYe$$Remove redundant patch. Allow restrictive plan hints at compile time. Removed break line. Remove old patch. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch101-flink-45fb6d82_Arja_PatchNaturalnessYe$$Remove inconsistent check for group strategy. Remove old patch. Allow restrictive plan hints at compile time. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch139-flink-45fb6d82_Arja_PatchNaturalnessYe$$Remove redundant mark on GroupReduceWithCombineProperties node. Remove forced rebalancing from PartitionNode. Allow restrictive plan hints at compile time. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch704-flink-45fb6d82_Arja_PatchNaturalnessYe$$Fix bug in GroupReduceWithCombineProperties. Remove patch from rgps. Updated patch for compiler toString. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch394-flink-45fb6d82_Arja_PatchNaturalnessYe$$Remove forced rebalancing from PartitionNode. add a heuristic disk cost comparison to the disk cost comparison. Remove redundant initProperties call. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch43-flink-45fb6d82_Arja_PatchNaturalnessYe$$Remove old patch. Remove patch. Fix inconsistent sort for group strategy. Remove redundant patch. Remove redundant merge of GroupReduceWithCombineProperties .. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch507-flink-45fb6d82_Arja_PatchNaturalnessYe$$Remove forced rebalancing from PartitionNode. Remove redundant patch. Remove redundant merge of GroupReduceWithCombineProperties .. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch190-flink-45fb6d82_Arja_PatchNaturalnessYe$$Remove inconsistent check for group strategy. Remove old patch. Removed a couple incorrect lines from a previous commit that prevents you from using an old one. Removed a couple of unused lines. $$0
flink-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}$$patch86-flink-45fb6d82_Arja_PatchNaturalnessYe$$Remove a redundant check. Allow restrictive plan hints at compile time. Remove patch from rgps. Remove redundant patch. $$0
flink-0cfa43d7$$Chained stream tasks share the same RuntimeContext$$Chained stream operators currently share the same runtimecontext, this will certainly lead to problems in the future.   We should create separate runtime contexts for each operator in the chain.$$patch521-flink-0cfa43d7_Arja_PatchNaturalnessYe$$update tmp. Remove patch 1434. $$0
flink-0cfa43d7$$Chained stream tasks share the same RuntimeContext$$Chained stream operators currently share the same runtimecontext, this will certainly lead to problems in the future.   We should create separate runtime contexts for each operator in the chain.$$patch341-flink-0cfa43d7_Arja_PatchNaturalnessYe$$Fix a bug in the BroadcastOutputSelectorWrapper . addCollector. update patched jar. Remove a redundant call to put in a map. Remove patched code. fix broken patch. $$0
flink-0cfa43d7$$Chained stream tasks share the same RuntimeContext$$Chained stream operators currently share the same runtimecontext, this will certainly lead to problems in the future.   We should create separate runtime contexts for each operator in the chain.$$patch313-flink-0cfa43d7_Arja_PatchNaturalnessYe$$Remove unnecessary code. fix broken patch. update patch. $$0
flink-0cfa43d7$$Chained stream tasks share the same RuntimeContext$$Chained stream operators currently share the same runtimecontext, this will certainly lead to problems in the future.   We should create separate runtime contexts for each operator in the chain.$$patch347-flink-0cfa43d7_Arja_PatchNaturalnessYe$$Remove patched code. update patched jar. fix broken patch. $$0
flink-0cfa43d7$$Chained stream tasks share the same RuntimeContext$$Chained stream operators currently share the same runtimecontext, this will certainly lead to problems in the future.   We should create separate runtime contexts for each operator in the chain.$$patch835-flink-0cfa43d7_Arja_PatchNaturalnessYe$$Removed patch for codenarc. update patch for code improvement. Remove wrapper from chained configuration ; it is not needed .. $$0
flink-0cfa43d7$$Chained stream tasks share the same RuntimeContext$$Chained stream operators currently share the same runtimecontext, this will certainly lead to problems in the future.   We should create separate runtime contexts for each operator in the chain.$$patch401-flink-0cfa43d7_Arja_PatchNaturalnessYe$$Remove patch from code. Fix a bug in the CollectorWrapper class .. fix broken patch. $$0
flink-0cfa43d7$$Chained stream tasks share the same RuntimeContext$$Chained stream operators currently share the same runtimecontext, this will certainly lead to problems in the future.   We should create separate runtime contexts for each operator in the chain.$$patch233-flink-0cfa43d7_Arja_PatchNaturalnessYe$$fix broken patch. update patched jar. Remove patch from jar. $$0
flink-0cfa43d7$$Chained stream tasks share the same RuntimeContext$$Chained stream operators currently share the same runtimecontext, this will certainly lead to problems in the future.   We should create separate runtime contexts for each operator in the chain.$$patch409-flink-0cfa43d7_Arja_PatchNaturalnessYe$$Fix a bug in the BroadcastOutputSelectorWrapper . addCollector. fix broken patch. Remove wrapper from chained configuration ; it is not needed .. $$0
flink-0cfa43d7$$Chained stream tasks share the same RuntimeContext$$Chained stream operators currently share the same runtimecontext, this will certainly lead to problems in the future.   We should create separate runtime contexts for each operator in the chain.$$patch436-flink-0cfa43d7_Arja_PatchNaturalnessYe$$fix broken patch. Remove wrapper from chained configuration ; it is not needed .. Remove patched code. $$0
flink-0cfa43d7$$Chained stream tasks share the same RuntimeContext$$Chained stream operators currently share the same runtimecontext, this will certainly lead to problems in the future.   We should create separate runtime contexts for each operator in the chain.$$patch1205-flink-0cfa43d7_Arja_PatchNaturalnessYe$$Remove patched code. update patched jar. $$0
flink-0cfa43d7$$Chained stream tasks share the same RuntimeContext$$Chained stream operators currently share the same runtimecontext, this will certainly lead to problems in the future.   We should create separate runtime contexts for each operator in the chain.$$patch1006-flink-0cfa43d7_Arja_PatchNaturalnessYe$$fix broken patch. $$0
flink-0cfa43d7$$Chained stream tasks share the same RuntimeContext$$Chained stream operators currently share the same runtimecontext, this will certainly lead to problems in the future.   We should create separate runtime contexts for each operator in the chain.$$patch221-flink-0cfa43d7_Arja_PatchNaturalnessYe$$Remove patched code. Removed patch for code improvement. $$0
flink-0cfa43d7$$Chained stream tasks share the same RuntimeContext$$Chained stream operators currently share the same runtimecontext, this will certainly lead to problems in the future.   We should create separate runtime contexts for each operator in the chain.$$patch569-flink-0cfa43d7_Arja_PatchNaturalnessYe$$update patched jar. Remove patch from jar. Fix broken patch. $$0
flink-0cfa43d7$$Chained stream tasks share the same RuntimeContext$$Chained stream operators currently share the same runtimecontext, this will certainly lead to problems in the future.   We should create separate runtime contexts for each operator in the chain.$$patch990-flink-0cfa43d7_Arja_PatchNaturalnessYe$$Remove wrapper from chained configuration ; it is not needed .. update patch for code improvement. $$0
flink-0cfa43d7$$Chained stream tasks share the same RuntimeContext$$Chained stream operators currently share the same runtimecontext, this will certainly lead to problems in the future.   We should create separate runtime contexts for each operator in the chain.$$patch76-flink-0cfa43d7_Arja_PatchNaturalnessYe$$fix broken patch. Remove unnecessary code. $$0
flink-0cfa43d7$$Chained stream tasks share the same RuntimeContext$$Chained stream operators currently share the same runtimecontext, this will certainly lead to problems in the future.   We should create separate runtime contexts for each operator in the chain.$$patch592-flink-0cfa43d7_Arja_PatchNaturalnessYe$$fix broken patch. update patch for closure issues. Remove patch from code. $$0
flink-0cfa43d7$$Chained stream tasks share the same RuntimeContext$$Chained stream operators currently share the same runtimecontext, this will certainly lead to problems in the future.   We should create separate runtime contexts for each operator in the chain.$$patch1137-flink-0cfa43d7_Arja_PatchNaturalnessYe$$Remove unnecessary code. fix broken patch. $$0
flink-0cfa43d7$$Chained stream tasks share the same RuntimeContext$$Chained stream operators currently share the same runtimecontext, this will certainly lead to problems in the future.   We should create separate runtime contexts for each operator in the chain.$$patch1130-flink-0cfa43d7_Arja_PatchNaturalnessYe$$Remove patched code. fix broken patch. $$0
flink-0cfa43d7$$Chained stream tasks share the same RuntimeContext$$Chained stream operators currently share the same runtimecontext, this will certainly lead to problems in the future.   We should create separate runtime contexts for each operator in the chain.$$patch582-flink-0cfa43d7_Arja_PatchNaturalnessYe$$update patched jar. Remove patch from jar. Updated patch for code improvement. $$0
flink-0cfa43d7$$Chained stream tasks share the same RuntimeContext$$Chained stream operators currently share the same runtimecontext, this will certainly lead to problems in the future.   We should create separate runtime contexts for each operator in the chain.$$patch1131-flink-0cfa43d7_Arja_PatchNaturalnessYe$$fix a bug in the code style. update patch for streaming job graph generator. Added patch for streaming job graph generation. $$0
flink-0cfa43d7$$Chained stream tasks share the same RuntimeContext$$Chained stream operators currently share the same runtimecontext, this will certainly lead to problems in the future.   We should create separate runtime contexts for each operator in the chain.$$patch1028-flink-0cfa43d7_Arja_PatchNaturalnessYe$$update tmp. $$0
flink-0cfa43d7$$Chained stream tasks share the same RuntimeContext$$Chained stream operators currently share the same runtimecontext, this will certainly lead to problems in the future.   We should create separate runtime contexts for each operator in the chain.$$patch291-flink-0cfa43d7_Arja_PatchNaturalnessYe$$Remove patch for class loader reset. Remove wrapper from chained configurations. Remove unused patch. $$0
flink-0cfa43d7$$Chained stream tasks share the same RuntimeContext$$Chained stream operators currently share the same runtimecontext, this will certainly lead to problems in the future.   We should create separate runtime contexts for each operator in the chain.$$patch899-flink-0cfa43d7_Arja_PatchNaturalnessYe$$update patched jar. update patched jar. $$0
flink-0cfa43d7$$Chained stream tasks share the same RuntimeContext$$Chained stream operators currently share the same runtimecontext, this will certainly lead to problems in the future.   We should create separate runtime contexts for each operator in the chain.$$patch492-flink-0cfa43d7_Arja_PatchNaturalnessYe$$Remove patched code. fix broken patch. Fix a bug in the BroadcastOutputSelectorWrapper . addCollector. $$0
flink-0cfa43d7$$Chained stream tasks share the same RuntimeContext$$Chained stream operators currently share the same runtimecontext, this will certainly lead to problems in the future.   We should create separate runtime contexts for each operator in the chain.$$patch1067-flink-0cfa43d7_Arja_PatchNaturalnessYe$$update patched jar. Added patch for streaming job graph generation. $$0
flink-0cfa43d7$$Chained stream tasks share the same RuntimeContext$$Chained stream operators currently share the same runtimecontext, this will certainly lead to problems in the future.   We should create separate runtime contexts for each operator in the chain.$$patch1094-flink-0cfa43d7_Arja_PatchNaturalnessYe$$fix broken patch. Remove wrapper from chained configuration ; it is not needed .. Remove patched code. Removed patched code. update patched jar. $$0
flink-0cfa43d7$$Chained stream tasks share the same RuntimeContext$$Chained stream operators currently share the same runtimecontext, this will certainly lead to problems in the future.   We should create separate runtime contexts for each operator in the chain.$$patch1034-flink-0cfa43d7_Arja_PatchNaturalnessYe$$Remove patched code. update patched jar. $$0
flink-0cfa43d7$$Chained stream tasks share the same RuntimeContext$$Chained stream operators currently share the same runtimecontext, this will certainly lead to problems in the future.   We should create separate runtime contexts for each operator in the chain.$$patch1057-flink-0cfa43d7_Arja_PatchNaturalnessYe$$fix broken patch. Remove wrapper from chained configuration ; it is not needed .. $$0
flink-0cfa43d7$$Chained stream tasks share the same RuntimeContext$$Chained stream operators currently share the same runtimecontext, this will certainly lead to problems in the future.   We should create separate runtime contexts for each operator in the chain.$$patch882-flink-0cfa43d7_Arja_PatchNaturalnessYe$$update patch for code improvement. Remove patch from jar. update patch for closure issues. Moving to a new patch. $$0
flink-0cfa43d7$$Chained stream tasks share the same RuntimeContext$$Chained stream operators currently share the same runtimecontext, this will certainly lead to problems in the future.   We should create separate runtime contexts for each operator in the chain.$$patch87-flink-0cfa43d7_Arja_PatchNaturalnessYe$$Remove patch from jar. Fix a bug in the BroadcastOutputSelectorWrapper . addCollector. update tmp. $$0
flink-0cfa43d7$$Chained stream tasks share the same RuntimeContext$$Chained stream operators currently share the same runtimecontext, this will certainly lead to problems in the future.   We should create separate runtime contexts for each operator in the chain.$$patch45-flink-0cfa43d7_Arja_PatchNaturalnessYe$$Removed patch .. update patched jar. $$0
flink-0cfa43d7$$Chained stream tasks share the same RuntimeContext$$Chained stream operators currently share the same runtimecontext, this will certainly lead to problems in the future.   We should create separate runtime contexts for each operator in the chain.$$patch933-flink-0cfa43d7_Arja_PatchNaturalnessYe$$fix broken patch. update patch. Remove wrapper from chained configuration ; it is not needed .. $$0
flink-0cfa43d7$$Chained stream tasks share the same RuntimeContext$$Chained stream operators currently share the same runtimecontext, this will certainly lead to problems in the future.   We should create separate runtime contexts for each operator in the chain.$$patch562-flink-0cfa43d7_Arja_PatchNaturalnessYe$$Remove wrapper from tmp. fix broken patch. update patch for closure issues. update tmp. $$0
math-e2dc384d$$LevenburgMaquardt switched evaluation and iterations$$None$$patch31-math-e2dc384d_Arja_PatchNaturalnessYe$$Added a backprop to the patch .. $$0
math-e2dc384d$$LevenburgMaquardt switched evaluation and iterations$$None$$patch30-math-e2dc384d_Arja_PatchNaturalnessYe$$Fix a bug in leastsquares optimizer .. $$0
math-e2dc384d$$LevenburgMaquardt switched evaluation and iterations$$None$$patch8-math-e2dc384d_Arja_PatchNaturalnessYe$$Added a line of code to track the number of times we iterate over the two - . Remove an unnecessary call to incrementCount .. $$0
math-e2dc384d$$LevenburgMaquardt switched evaluation and iterations$$None$$patch1-math-e2dc384d_Arja_PatchNaturalnessYe$$Fix NPE in LevenbergMarquardtOptimizer . java. $$0
math-e2dc384d$$LevenburgMaquardt switched evaluation and iterations$$None$$patch6-math-e2dc384d_Arja_PatchNaturalnessYe$$Updated variable name. $$0
math-e2dc384d$$LevenburgMaquardt switched evaluation and iterations$$None$$patch15-math-e2dc384d_Arja_PatchNaturalnessYe$$Updated variable name. Fix a bug in the leastsquares optimizer .. $$0
math-e2dc384d$$LevenburgMaquardt switched evaluation and iterations$$None$$patch12-math-e2dc384d_Arja_PatchNaturalnessYe$$Added a line of code to track the number of times we think we ' ve done the. Remove an unnecessary call to incrementCount .. $$0
math-e2dc384d$$LevenburgMaquardt switched evaluation and iterations$$None$$patch23-math-e2dc384d_Arja_PatchNaturalnessYe$$Fix a bug in the leastsquares optimizer .. Fix the patch for one more test. $$0
math-e2dc384d$$LevenburgMaquardt switched evaluation and iterations$$None$$patch7-math-e2dc384d_Arja_PatchNaturalnessYe$$Updated variable name. $$0
math-e2dc384d$$LevenburgMaquardt switched evaluation and iterations$$None$$patch9-math-e2dc384d_Arja_PatchNaturalnessYe$$Put back the initial value of the function in the same thread as the other one .. $$0
math-e2dc384d$$LevenburgMaquardt switched evaluation and iterations$$None$$patch22-math-e2dc384d_Arja_PatchNaturalnessYe$$Added step to increase xNorm .. Fix variable name error. $$0
math-e2dc384d$$LevenburgMaquardt switched evaluation and iterations$$None$$patch25-math-e2dc384d_Arja_PatchNaturalnessYe$$Fix variable name variable. Fix xNorm reset during minimized least squares test. Fix the case for svn patch .. $$0
math-e2dc384d$$LevenburgMaquardt switched evaluation and iterations$$None$$patch13-math-e2dc384d_Arja_PatchNaturalnessYe$$Added a line of code to mark the end of a line of code. Remove an unnecessary call to incrementCount .. Fix the patch for one more test. $$0
math-e2dc384d$$LevenburgMaquardt switched evaluation and iterations$$None$$patch14-math-e2dc384d_Arja_PatchNaturalnessYe$$Updated one of the patched code .. Fix a bug in the same way as in the other one .. $$0
math-e2dc384d$$LevenburgMaquardt switched evaluation and iterations$$None$$patch32-math-e2dc384d_Arja_PatchNaturalnessYe$$Updated one of the work items for the patch .. Fix one more test. $$0
math-e2dc384d$$LevenburgMaquardt switched evaluation and iterations$$None$$patch5-math-e2dc384d_Arja_PatchNaturalnessYe$$Added a line of code to track the number of times we iterate over the two - . Fix a bug in the inverse function so that it doesn ' t become an error in the inverse. $$0
math-e2dc384d$$LevenburgMaquardt switched evaluation and iterations$$None$$patch2-math-e2dc384d_Arja_PatchNaturalnessYe$$Added a line of code that was accidentally left in the last commit .. Remove an unnecessary call to incrementCount .. Fix xNorm reset during minimized least squares test. $$0
math-e2dc384d$$LevenburgMaquardt switched evaluation and iterations$$None$$patch19-math-e2dc384d_Arja_PatchNaturalnessYe$$Fix variable variable. Fix the case for svn patch .. $$0
math-e2dc384d$$LevenburgMaquardt switched evaluation and iterations$$None$$patch26-math-e2dc384d_Arja_PatchNaturalnessYe$$Fix variable name error in LevenbergMarquardtOptimizer. $$0
math-e2dc384d$$LevenburgMaquardt switched evaluation and iterations$$None$$patch21-math-e2dc384d_Arja_PatchNaturalnessYe$$Added missing incrementCount ( ) to LevenbergMarquardtOptimizer. Fix a bug in the inverse search. $$0
math-e2dc384d$$LevenburgMaquardt switched evaluation and iterations$$None$$patch28-math-e2dc384d_Arja_PatchNaturalnessYe$$Added a line of code to track the number of times we iterate over the two - . Remove an unnecessary call to incrementCount .. Fix patch. $$0
math-e2dc384d$$LevenburgMaquardt switched evaluation and iterations$$None$$patch17-math-e2dc384d_Arja_PatchNaturalnessYe$$Updated variable name. Fix a bug in the likelihood function. $$0
math-e2dc384d$$LevenburgMaquardt switched evaluation and iterations$$None$$patch10-math-e2dc384d_Arja_PatchNaturalnessYe$$Added one more test to see if the function is still working .. $$0
math-e2dc384d$$LevenburgMaquardt switched evaluation and iterations$$None$$patch3-math-e2dc384d_Arja_PatchNaturalnessYe$$Fix a bug in the inverse function to save energy .. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch120-math-f6dd42b4_Arja_PatchNaturalnessYe$$fixed NPE in UnivariateRealSolverImpl . verifyInterval. Fix broken patch. Add missing patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch36-math-f6dd42b4_Arja_PatchNaturalnessYe$$Set the result of the patch as it was clear before the patch was added .. Add missing patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch31-math-f6dd42b4_Arja_PatchNaturalnessYe$$Reset the inverse of the patch to the last result .. Add missing patch. changed max iterations count. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch118-math-f6dd42b4_Arja_PatchNaturalnessYe$$Remove redundant clearResult ( ) call. Added a solution if the function value is good enough. Add missing patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch111-math-f6dd42b4_Arja_PatchNaturalnessYe$$Remove redundant clearResult ( ) call. Add missing patch. Added patch to root cause error .. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch38-math-f6dd42b4_Arja_PatchNaturalnessYe$$Add missing patch. Fix swapped vec3s in BrentSolver . java. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch116-math-f6dd42b4_Arja_PatchNaturalnessYe$$Add missing exception. Fix swapped vecs in BrentSolver . java. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch54-math-f6dd42b4_Arja_PatchNaturalnessYe$$Remove redundant clearResult ( ) call. Add missing patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch98-math-f6dd42b4_Arja_PatchNaturalnessYe$$Reduce min and initial bracket the root .. Add missing patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch53-math-f6dd42b4_Arja_PatchNaturalnessYe$$Remove a couple incorrect code lines. Set default function value accuracy back to 0 . 5 . 4. Add missing patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch62-math-f6dd42b4_Arja_PatchNaturalnessYe$$BrentSolver does not clear the result flag at the end of a test. Add missing patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch117-math-f6dd42b4_Arja_PatchNaturalnessYe$$Add missing patch. Added missing patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch110-math-f6dd42b4_Arja_PatchNaturalnessYe$$BrentSolver now returns 0 if yMin is < 0 .. Add missing patch. Fix swapped deltas. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch39-math-f6dd42b4_Arja_PatchNaturalnessYe$$Removed a redundant line. Add missing patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch126-math-f6dd42b4_Arja_PatchNaturalnessYe$$added fix from pull request. Set function value accuracy to default function value accuracy. Add missing patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch30-math-f6dd42b4_Arja_PatchNaturalnessYe$$Add missing exception. Set default function value accuracy to 1 . 0e - 15 .. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch119-math-f6dd42b4_Arja_PatchNaturalnessYe$$throw exception if function to solve cannot be null. Add missing patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch63-math-f6dd42b4_Arja_PatchNaturalnessYe$$BrentSolver now returns 0 if yMin is < 0 .. Add missing patch. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch97-math-f6dd42b4_Arja_PatchNaturalnessYe$$Fix broken patch. Add missing patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch90-math-f6dd42b4_Arja_PatchNaturalnessYe$$Added a throw exception if the function value at endpoints do not have the same signs .. Set default function value accuracy to 1 . 0e - 15 .. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch64-math-f6dd42b4_Arja_PatchNaturalnessYe$$Add missing patch. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch99-math-f6dd42b4_Arja_PatchNaturalnessYe$$Set function value accuracy to default value accuracy. Add missing patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch52-math-f6dd42b4_Arja_PatchNaturalnessYe$$Fix broken patch. Add missing patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch55-math-f6dd42b4_Arja_PatchNaturalnessYe$$Reset the inverse of the patch if it is better than the function value .. Add missing patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch77-math-f6dd42b4_Arja_PatchNaturalnessYe$$Add missing patch. BrentSolver . setResult ( x1 , i ) ;. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch83-math-f6dd42b4_Arja_PatchNaturalnessYe$$Reset the computed result as false in BrentSolver .. Add missing patch. Fix broken patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch48-math-f6dd42b4_Arja_PatchNaturalnessYe$$Add missing patch. Add a throw exception if the function to solve cannot be null. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch84-math-f6dd42b4_Arja_PatchNaturalnessYe$$Added verifyInterval to BrentSolver. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch70-math-f6dd42b4_Arja_PatchNaturalnessYe$$BrentSolver doesn ' t clear the result before attempting to use it. Added a throw to the console. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch8-math-f6dd42b4_Arja_PatchNaturalnessYe$$BrentSolver doesn ' t clear the result before attempting to use it. Add missing patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch1-math-f6dd42b4_Arja_PatchNaturalnessYe$$Added verifyInterval ( ) to function value function. Add missing patch. Fix a minor bug in BrentSolver. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch79-math-f6dd42b4_Arja_PatchNaturalnessYe$$Add missing patch. changed max iterations count. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch103-math-f6dd42b4_Arja_PatchNaturalnessYe$$Removed erroneous test. Added a throw to the console. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch15-math-f6dd42b4_Arja_PatchNaturalnessYe$$Add missing patch. revert accidently change. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch104-math-f6dd42b4_Arja_PatchNaturalnessYe$$added update of erroneous line. Added a throw to the console. BrentSolver . setResult ( x1 , i ) ;. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch12-math-f6dd42b4_Arja_PatchNaturalnessYe$$Remove a redundant patch. Add missing patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch24-math-f6dd42b4_Arja_PatchNaturalnessYe$$added fix from pull request. Fix broken patch. Add missing patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch23-math-f6dd42b4_Arja_PatchNaturalnessYe$$Fix broken patch. Add missing patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch40-math-f6dd42b4_Arja_PatchNaturalnessYe$$Added a solution if the solution is good enough. Add missing patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch7-math-f6dd42b4_Arja_PatchNaturalnessYe$$Added missing exception check. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch47-math-f6dd42b4_Arja_PatchNaturalnessYe$$Added a throw to the console. Set default function value accuracy to 1 . 0e - 15 .. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch78-math-f6dd42b4_Arja_PatchNaturalnessYe$$add a throw exception if one of the interval is not a sequence. Add missing exception. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch85-math-f6dd42b4_Arja_PatchNaturalnessYe$$Set function value accuracy to 0 .. Add missing patch. BrentSolver setResult ( x1 , i ) ;. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch9-math-f6dd42b4_Arja_PatchNaturalnessYe$$Added a throw to the console. revert accidently change to the old one. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch76-math-f6dd42b4_Arja_PatchNaturalnessYe$$Set function value accuracy to 0 if yInitial is good enough .. Add missing patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch49-math-f6dd42b4_Arja_PatchNaturalnessYe$$Added a result computed so that it can be used as the sole endpoint .. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch22-math-f6dd42b4_Arja_PatchNaturalnessYe$$Fix broken patch. Add missing patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch25-math-f6dd42b4_Arja_PatchNaturalnessYe$$Added a throw exception if the function value at endpoints do not have the same signs .. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch105-math-f6dd42b4_Arja_PatchNaturalnessYe$$Set function value accuracy to default function value accuracy. Add missing patch. Add a throw exception if the function to solve cannot be null. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch13-math-f6dd42b4_Arja_PatchNaturalnessYe$$added backprop to verifySequence ( ). Add missing patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch102-math-f6dd42b4_Arja_PatchNaturalnessYe$$Set function value accuracy to default , as it is done with the default function value accuracy. Fix broken patch. Add missing patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch14-math-f6dd42b4_Arja_PatchNaturalnessYe$$I had left it as a last resort .. Add missing patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch50-math-f6dd42b4_Arja_PatchNaturalnessYe$$Reset the iterator count as we proceeded to the next test. Add missing patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch68-math-f6dd42b4_Arja_PatchNaturalnessYe$$throw exception if function to solve is null. Added missing exception check. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch57-math-f6dd42b4_Arja_PatchNaturalnessYe$$fixed a minor bug in UnivariateRealSolverImpl . verifyInterval. Added a throw to the console. revert accidently change to the old one. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch95-math-f6dd42b4_Arja_PatchNaturalnessYe$$Fix the case for the min function to be good enough. Add missing patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch61-math-f6dd42b4_Arja_PatchNaturalnessYe$$Set function value accuracy to 0 if endpoints are not good enough. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch59-math-f6dd42b4_Arja_PatchNaturalnessYe$$Add missing patch. Fixing a minor bug in BrentSolver .. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch66-math-f6dd42b4_Arja_PatchNaturalnessYe$$Add missing patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch92-math-f6dd42b4_Arja_PatchNaturalnessYe$$Removed a redundant line. Added a throw to the console. Fix erroneous cross - sentence error in BrentSolver. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch124-math-f6dd42b4_Arja_PatchNaturalnessYe$$Set function value accuracy to default function value accuracy. Add missing patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch32-math-f6dd42b4_Arja_PatchNaturalnessYe$$Set this . resultComputed to false in BrentSolver . java. Add missing patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch123-math-f6dd42b4_Arja_PatchNaturalnessYe$$Fix the case for the min function to work properly. Add missing patch. changed max iter count. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch115-math-f6dd42b4_Arja_PatchNaturalnessYe$$Reset the inverse of the patch if it is better than the function value .. Add missing patch. BrentSolver . setResult ( x1 , i ) ;. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch58-math-f6dd42b4_Arja_PatchNaturalnessYe$$Add missing patch. Added patch to prevent adding iterations to the same thread. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch67-math-f6dd42b4_Arja_PatchNaturalnessYe$$throw exception if function to solve is null. Add missing patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch60-math-f6dd42b4_Arja_PatchNaturalnessYe$$BrentSolver now uses 0 . 0 solution. Add missing patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch94-math-f6dd42b4_Arja_PatchNaturalnessYe$$Fix broken patch. Add missing patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch69-math-f6dd42b4_Arja_PatchNaturalnessYe$$Add missing exception. changed parallel patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch56-math-f6dd42b4_Arja_PatchNaturalnessYe$$Set function value accuracy to 0 if y < min && y > max. Add missing patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch51-math-f6dd42b4_Arja_PatchNaturalnessYe$$throw exception if function to solve is null. Add missing patch. changed line. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch113-math-f6dd42b4_Arja_PatchNaturalnessYe$$Added missing exception check. Fix broken patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch114-math-f6dd42b4_Arja_PatchNaturalnessYe$$Fix test. Add missing patch. Add a throw exception if the function to solve cannot be null. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch122-math-f6dd42b4_Arja_PatchNaturalnessYe$$BrentSolver now returns 0 if yMin is < 0 .. Add missing patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch34-math-f6dd42b4_Arja_PatchNaturalnessYe$$Fix broken patch. Add missing exception. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch125-math-f6dd42b4_Arja_PatchNaturalnessYe$$Added a throw exception if the function value at endpoints do not have the same signs .. revert accidently change. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch33-math-f6dd42b4_Arja_PatchNaturalnessYe$$throw exception if function to solve cannot be null. Remove a redundant check. Added a throw to the console. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch107-math-f6dd42b4_Arja_PatchNaturalnessYe$$Add missing patch. Fix broken patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch11-math-f6dd42b4_Arja_PatchNaturalnessYe$$Add a throw exception if we can ' t find a solution. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch29-math-f6dd42b4_Arja_PatchNaturalnessYe$$Set function value accuracy to default value accuracy. Add missing exception. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch100-math-f6dd42b4_Arja_PatchNaturalnessYe$$Set function value accuracy to default function value accuracy. Add missing patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch16-math-f6dd42b4_Arja_PatchNaturalnessYe$$fixed a minor bug in UnivariateRealSolverImpl . verifyInterval. Add missing patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch109-math-f6dd42b4_Arja_PatchNaturalnessYe$$Add missing patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch18-math-f6dd42b4_Arja_PatchNaturalnessYe$$Removed a redundant line. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch87-math-f6dd42b4_Arja_PatchNaturalnessYe$$Add missing exception. BrentSolver had a clear result ; removed redundant clearResult calls. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch73-math-f6dd42b4_Arja_PatchNaturalnessYe$$Set default function value accuracy to 1 . 0e - 15 .. Add missing patch. Fixing a minor bug in BrentSolver .. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch74-math-f6dd42b4_Arja_PatchNaturalnessYe$$Fix incorrect patch. Add missing patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch80-math-f6dd42b4_Arja_PatchNaturalnessYe$$added missing line. Add missing patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch42-math-f6dd42b4_Arja_PatchNaturalnessYe$$Set function value accuracy to default function value accuracy. Add missing patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch5-math-f6dd42b4_Arja_PatchNaturalnessYe$$Set function value accuracy to default value accuracy. Add missing patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch89-math-f6dd42b4_Arja_PatchNaturalnessYe$$added verifyBracketing. Add missing patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch45-math-f6dd42b4_Arja_PatchNaturalnessYe$$Fix broken patch. Add missing patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch19-math-f6dd42b4_Arja_PatchNaturalnessYe$$BrentSolver now uses the result of running function with a lower tolerance .. Added a throw to the console. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch108-math-f6dd42b4_Arja_PatchNaturalnessYe$$Add missing patch. revert patch. Add a throw exception if the function to solve cannot be null. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch21-math-f6dd42b4_Arja_PatchNaturalnessYe$$Add missing exception. BrentSolver . setResult ( x1 , i ). $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch28-math-f6dd42b4_Arja_PatchNaturalnessYe$$Set function value accuracy to default function value accuracy. I had made a mistake of failing test. Add missing patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch101-math-f6dd42b4_Arja_PatchNaturalnessYe$$Set default function value accuracy. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch17-math-f6dd42b4_Arja_PatchNaturalnessYe$$Add missing patch. Fix changed delta .. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch106-math-f6dd42b4_Arja_PatchNaturalnessYe$$Set default function value accuracy back to 0 . 1 . 0. Fix broken patch. Add missing patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch44-math-f6dd42b4_Arja_PatchNaturalnessYe$$Fix the case for the min function to return the first endpoint. Add missing patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch3-math-f6dd42b4_Arja_PatchNaturalnessYe$$Added a throw exception if the function value at endpoints do not have the same signs .. changed max iterations count. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch4-math-f6dd42b4_Arja_PatchNaturalnessYe$$Set function value accuracy to default function value accuracy. Add missing patch. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch88-math-f6dd42b4_Arja_PatchNaturalnessYe$$Fix the merge of one - to - one calls into conferences. Add missing patch. Fix a minor bug in the BrentSolver such that yMin and yMax are not close to. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch81-math-f6dd42b4_Arja_PatchNaturalnessYe$$Remove redundant clearResult ( ) call. Set function value accuracy to default function value accuracy. Add missing patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch75-math-f6dd42b4_Arja_PatchNaturalnessYe$$update inverse patch. Added a throw to the console. revert patch. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch72-math-f6dd42b4_Arja_PatchNaturalnessYe$$Add missing patch. improve stability of BrentSolver. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch86-math-f6dd42b4_Arja_PatchNaturalnessYe$$Reset the cached result of the BrentSolver such that it is not possible to reduce the interval of. Add missing patch. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch120-math-596ccd59_Arja_PatchNaturalnessYe$$Remove a redundant empty line. Fix membership test. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch36-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patch for max membership in FuzzyKMeansClusterer. Remove over - aggressive clustering. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch127-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patch from test script. Fix a bug in FuzzyKMeansClusterer .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch31-math-596ccd59_Arja_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer .. Removed patched code. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch118-math-596ccd59_Arja_PatchNaturalnessYe$$If fuzziness < 1 . 0d then throw an exception. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch111-math-596ccd59_Arja_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer .. Removed patch for max membership in FuzzyKMeansClusterer. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch116-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patch from test script. Remove oversampling .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch129-math-596ccd59_Arja_PatchNaturalnessYe$$Fix membership matrix from previous fix. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch142-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patch for max membership in FuzzyKMeansClusterer. Fix new test. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch54-math-596ccd59_Arja_PatchNaturalnessYe$$Fix membership matrix. Remove oversampling. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch145-math-596ccd59_Arja_PatchNaturalnessYe$$added dataPoints to become unmodifiable. If we can ' t find a clusterer with fuzziness < 1 . 0d , throw. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch53-math-596ccd59_Arja_PatchNaturalnessYe$$Readded missing updateClusterCenters method. Removed patch from fuzzykmeans clusterer. Remove spurious j ++. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch65-math-596ccd59_Arja_PatchNaturalnessYe$$If fuzziness < 1 . 0d then throw an exception. added new centroid clustering test. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch91-math-596ccd59_Arja_PatchNaturalnessYe$$Fix membership matrix after membership update .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch96-math-596ccd59_Arja_PatchNaturalnessYe$$throw exception if size < k. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch62-math-596ccd59_Arja_PatchNaturalnessYe$$added dataPoints to become unmodifiable. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch117-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patch for max membership in FuzzyKMeansClusterer. Readded missing assignment. Remove spurious j ++. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch128-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patch for max membership in FuzzyKMeansClusterer. Fix membership matrix for newly added k - means clusterer .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch110-math-596ccd59_Arja_PatchNaturalnessYe$$Remove a couple of unused patches. Removed patch for max membership in FuzzyKMeansClusterer. Readded missing patch. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch39-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patch from test. Fixing a small issue in FuzzyKMeansClusterer .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch126-math-596ccd59_Arja_PatchNaturalnessYe$$Remove a couple incorrect lines. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch30-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patch from test. Fix a bug in FuzzyKMeansClusterer. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch119-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patch for max membership in FuzzyKMeansClusterer. added centroid clustering test. Remove i ++ to fix the build .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch121-math-596ccd59_Arja_PatchNaturalnessYe$$make dataPoints unmodifiable. Removed patch from test script. Remove over - aggressive clustering. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch37-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patch from test. Remove patch for newly added clusterer. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch63-math-596ccd59_Arja_PatchNaturalnessYe$$Fix County Change. throw exception if fuzziness < 1 . 0d. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch97-math-596ccd59_Arja_PatchNaturalnessYe$$Fix whitespace. Fix single threaded update of cluster center in case of changed max membership. Fix a bug in FuzzyKMeansClusterer .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch90-math-596ccd59_Arja_PatchNaturalnessYe$$Fix membership matrix not being modified by accident. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch64-math-596ccd59_Arja_PatchNaturalnessYe$$added dataPoints to become unmodifiable. Remove spurious j ++. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch99-math-596ccd59_Arja_PatchNaturalnessYe$$Fix whitespace. Removed patch for max membership in FuzzyKMeansClusterer. Remove spurious increment of the array index. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch144-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patch from test. Readded a clear hack .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch52-math-596ccd59_Arja_PatchNaturalnessYe$$Fix NPE in FuzzyKMeansClusterer. Removed patch for max membership in FuzzyKMeansClusterer. If we have not found a match throw exception. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch143-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patch from test. Fix a small bug in FuzzyKMeansClusterer. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch55-math-596ccd59_Arja_PatchNaturalnessYe$$Remove a couple of unused patches. Removed patch for now .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch77-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patched code. Fix scaleInPlace for newly added centroid clusterer. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch83-math-596ccd59_Arja_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer .. Removed patched code. Fix the bug. Readded missing patch. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch8-math-596ccd59_Arja_PatchNaturalnessYe$$throw exception if data is too small. Removed patch from test. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch150-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patched code. Throw the right exception if fuzziness is not 1 . 0d. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch1-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patch from test script. Fix a bug in FuzzyKMeansClusterer .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch6-math-596ccd59_Arja_PatchNaturalnessYe$$added dataPoints to become unmodifiable. Remove over - aggressive loop. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch103-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patch from fuzzy kmeans clusterer. Fix a bug in FuzzyKMeansClusterer .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch15-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patch from test. Fix a small bug. Remove i ++ to fix the build .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch104-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patch for max membership in FuzzyKMeansClusterer. Readded missing assignment. Remove over - aggressive loop. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch12-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patched code. Readded a clear line to test the clustering code. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch24-math-596ccd59_Arja_PatchNaturalnessYe$$added points = Collections . unmodifiable ( new ArrayList < T > ( dataPoints ));. Removed patch for max membership in FuzzyKMeansClusterer. Normalize membership matrix for newly added point. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch132-math-596ccd59_Arja_PatchNaturalnessYe$$Fix membershipMatrix. Remove over - aggressive clustering. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch23-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patch from test. Fix a bug in FuzzyKMeansClusterer where point is not added to the points. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch135-math-596ccd59_Arja_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer .. Removed patch from test script. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch7-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patch for clustering test. added new centroid clustering test. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch151-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patch from test. Fix the bug. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch9-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patch for max membership in FuzzyKMeansClusterer. Fix the bug. Readded missing patch. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch134-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patched code. Remove spurious change. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch133-math-596ccd59_Arja_PatchNaturalnessYe$$Remove a couple of unused patches. Fix membership matrix not being modified by accident. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch105-math-596ccd59_Arja_PatchNaturalnessYe$$added dataPoints to become unmodifiable. Remove oversampling .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch102-math-596ccd59_Arja_PatchNaturalnessYe$$Fix NPE in FuzzyKMeansClusterer. Removed patch from test script. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch146-math-596ccd59_Arja_PatchNaturalnessYe$$Fix membershipMatrix. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch50-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patched code. Remove scaleInPlace from newly added patch. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch68-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patch from test script. Set membershipMatrix [ i ][ j ] = 1 . 0 / sum ;. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch141-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patch from test. added missing patch. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch57-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patch for max membership in FuzzyKMeansClusterer. Readded missing patch. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch148-math-596ccd59_Arja_PatchNaturalnessYe$$Added a throw if we are too small to split. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch95-math-596ccd59_Arja_PatchNaturalnessYe$$Remove a redundant empty line. Removed patch from test. Remove oversampling .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch61-math-596ccd59_Arja_PatchNaturalnessYe$$Reverted the change that the clustering test was using. Remove a couple incorrect entries. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch59-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patch from test script. Fix scaleInPlace .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch92-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patch from test. Fix a bug in FuzzyKMeansClusterer. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch32-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patched code. Fix a bug in FuzzyKMeansClusterer .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch123-math-596ccd59_Arja_PatchNaturalnessYe$$Reverted the change that the clustering test was using. Fix FuzzyKMeansClusterer .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch35-math-596ccd59_Arja_PatchNaturalnessYe$$Remove one more test. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch112-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patch for max membership in FuzzyKMeansClusterer. Remove oversampling .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch58-math-596ccd59_Arja_PatchNaturalnessYe$$Reverted the change that the clustering test was using. If we have not found a clusterer , throw an exception if we can ' t find a. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch93-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patch from test. Remove scaleInPlace ( ) from FuzzyKMeansClusterer. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch149-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patch from test. Set membershipMatrix [ i ] [ j ] for new cluster. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch60-math-596ccd59_Arja_PatchNaturalnessYe$$Added a throw if we are too small to split. Remove oversampling. Remove over - aggressive clustering. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch94-math-596ccd59_Arja_PatchNaturalnessYe$$throw exception if size < k. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch69-math-596ccd59_Arja_PatchNaturalnessYe$$Fix single threaded update of cluster center after speedup .. added centroid clustering test. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch140-math-596ccd59_Arja_PatchNaturalnessYe$$Fix a small bug in FuzzyKMeansClusterer. Removed patch for max membership in FuzzyKMeansClusterer. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch56-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patch from test script. Remove i ++ to fix the build .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch147-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patched code. Fix a bug in FuzzyKMeansClusterer .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch51-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patched code. Remove oversampling .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch114-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patched code. If we can ' t find a clusterer with fuzziness < 1 . 0d , throw. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch122-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patch from test. Fix single line. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch34-math-596ccd59_Arja_PatchNaturalnessYe$$Fix NPE in FuzzyKMeansClusterer. Removed patch from test script. Remove oversampling .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch125-math-596ccd59_Arja_PatchNaturalnessYe$$Remove too small exception. Removed patch from test script. Readded missing patch. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch33-math-596ccd59_Arja_PatchNaturalnessYe$$Added throw if size < k. Fix a small bug. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch107-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patch from test. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch11-math-596ccd59_Arja_PatchNaturalnessYe$$Fix whitespace. Removed patch from fuzzykmeans clusterer. Remove oversampling .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch29-math-596ccd59_Arja_PatchNaturalnessYe$$Remove a redundant empty line. Removed patch for max membership in FuzzyKMeansClusterer. Reset membership matrix after point reduction. If fuzziness < = 1 . 0d , throw an exception if we cannot do anything. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch100-math-596ccd59_Arja_PatchNaturalnessYe$$Added a throw. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch16-math-596ccd59_Arja_PatchNaturalnessYe$$Readded missing updateClusterCenters method. Removed patch for max membership in FuzzyKMeansClusterer. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch109-math-596ccd59_Arja_PatchNaturalnessYe$$Fix membership matrix after merge of patched JAR. Removed patch from test. Remove patch from tmp. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch20-math-596ccd59_Arja_PatchNaturalnessYe$$Remove a redundant empty line. Removed patch from test. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch136-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patch for max membership in FuzzyKMeansClusterer. added centroid clustering fix. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch27-math-596ccd59_Arja_PatchNaturalnessYe$$If fuzziness < 1 . 0d , throw an exception if it is too small .. Remove oversampling. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch131-math-596ccd59_Arja_PatchNaturalnessYe$$added dataPoints back into view as a list. If we have a max fuzziness , throw an exception if we can ' t find a dataset. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch87-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patch from test. Remove over - updated code. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch73-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patch from test. Remove spurious j ++. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch42-math-596ccd59_Arja_PatchNaturalnessYe$$throw exception if size < k. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch5-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patch for max membership in FuzzyKMeansClusterer. Reverted the changes related to multiple memberships .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch89-math-596ccd59_Arja_PatchNaturalnessYe$$Readded missing updateClusterCenters method. Removed patch from test. Remove oversampling .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch2-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patched code. Remove a couple incorrect lines. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch45-math-596ccd59_Arja_PatchNaturalnessYe$$Fix membership matrix. Remove unused line. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch19-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patch from test. Fix a bug in FuzzyKMeansClusterer .. Readded missing patch. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch26-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patch from test script. Throw the right exception if fuzziness is not 1 . 0d. Fix a bug in FuzzyKMeansClusterer. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch130-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patched code. Readded clear line. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch108-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patch from fuzzykmeans clusterer. Fix the bug. Remove over - aggressive loop. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch21-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patch for max membership in FuzzyKMeansClusterer. Fixing small issue in FuzzyKMeansClusterer .. Remove over - aggressive clustering. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch137-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patched code. Remove over - aggressive clustering. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch28-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patch from test. Fix membership matrix after adding point to pointArray. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch101-math-596ccd59_Arja_PatchNaturalnessYe$$Fix membership matrix not being modified by accident. Fix a small bug in FuzzyKMeansClusterer. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch17-math-596ccd59_Arja_PatchNaturalnessYe$$Fix membershipMatrix. Remove oversampling .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch139-math-596ccd59_Arja_PatchNaturalnessYe$$Fix membershipMatrix. Remove scaleInPlace from newly added test case. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch106-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patch from test. Remove scaleInPlace from FuzzyKMeansClusterer. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch10-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patch from fuzzykmeans clusterer. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch44-math-596ccd59_Arja_PatchNaturalnessYe$$If fuzziness < 1 . 0d then throw an exception. If we have a max fuzziness value , throw an exception if it is too small .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch3-math-596ccd59_Arja_PatchNaturalnessYe$$Fix membership matrix after membership update. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch4-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patch for max membership in FuzzyKMeansClusterer. added missing patch. Remove scaleInPlace from newly added patch. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch43-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patch from test. Remove scaleInPlace from newly added piece. Set membershipMatrix [ i ] [ j ] for new cluster. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch88-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patch for max membership in FuzzyKMeansClusterer. Remove spurious increment of clustering code. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch81-math-596ccd59_Arja_PatchNaturalnessYe$$Removed patch from test. If fuzziness < = 1 . 0d , throw an exception if we cannot do anything. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch72-math-596ccd59_Arja_PatchNaturalnessYe$$throw exception if size < k. Fix a bug in FuzzyKMeansClusterer. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch86-math-596ccd59_Arja_PatchNaturalnessYe$$Fix NPE in FuzzyKMeansClusterer. Removed patch from test script. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch120-math-328513f3_Arja_PatchNaturalnessYe$$Remove one more test. Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch127-math-328513f3_Arja_PatchNaturalnessYe$$Reverted accidental throw of MathUtils . doubleValue ( ). Fix CMAESOptimizer logMu2 = Math . log ( mu + 0 . 5 ) ;. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch31-math-328513f3_Arja_PatchNaturalnessYe$$Fix a bug in CMAESOptimizer. Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch118-math-328513f3_Arja_PatchNaturalnessYe$$Revert this. update marker file. Fix C = diag ( x ) .. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch111-math-328513f3_Arja_PatchNaturalnessYe$$Fix C = fix. Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch38-math-328513f3_Arja_PatchNaturalnessYe$$Reverted accidental throw of MathUtils . doubleValue ( ). remove patch. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch116-math-328513f3_Arja_PatchNaturalnessYe$$Reverted accidental throw of MathUtils . doubleValue ( ). Fix single line error in CMAESOptimizer. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch129-math-328513f3_Arja_PatchNaturalnessYe$$Reverted accidental throw of MathUtils . doubleValue ( ). update marker. Remove a couple incorrect lines. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch142-math-328513f3_Arja_PatchNaturalnessYe$$Reverted accidental throw of MathUtils . doubleValue ( ). Fix NPE. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch54-math-328513f3_Arja_PatchNaturalnessYe$$Reset repaired [ i ] to 0 ; we don ' t need it. Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch98-math-328513f3_Arja_PatchNaturalnessYe$$CCC = > RC. Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch145-math-328513f3_Arja_PatchNaturalnessYe$$Use the latest log4j step for CMAES optimization. Fix a bug in CMAESOptimizer caused by incorrect patch .. Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch53-math-328513f3_Arja_PatchNaturalnessYe$$Reverted accidental throw of MathUtils . doubleValue ( ). Remove a couple incorrect lines. Fix a bug in CMAESOptimizer where the boundaries are not null .. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch65-math-328513f3_Arja_PatchNaturalnessYe$$update marker. CCC = > RBF ( min mueff value ). Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch91-math-328513f3_Arja_PatchNaturalnessYe$$Reverted accidental throw of MathUtils . doubleValue ( ). Fix NPE in CMAESOptimizer. Fix single line error in CMAESOptimizer. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch96-math-328513f3_Arja_PatchNaturalnessYe$$Fix single line exception. CANCER patch attempted fix. Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch62-math-328513f3_Arja_PatchNaturalnessYe$$Reverted accidental throw of MathUtils . doubleValue ( ). Fix a look for a better error message. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch117-math-328513f3_Arja_PatchNaturalnessYe$$Fix a minor bug in CMAESOptimizer such that it does not set a value. Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch128-math-328513f3_Arja_PatchNaturalnessYe$$Fix a look for a better exception in ' cmAESOptimizer '. fix merge conflict resolution. Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch110-math-328513f3_Arja_PatchNaturalnessYe$$Reverted accidental throw of MathUtils . doubleValue ( ). Updated patch for MathRuntimeException . getLocalizedMessage ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch39-math-328513f3_Arja_PatchNaturalnessYe$$Missing context variable. Fix possible NPE in CMAESOptimizer. Revert previous patch. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch126-math-328513f3_Arja_PatchNaturalnessYe$$Fix single line error in CMAESOptimizer . repair ( ) .. Fix possible NPE in CMAESOptimizer. Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch30-math-328513f3_Arja_PatchNaturalnessYe$$Reverted accidental throw of MathUtils . doubleValue ( ). Use the latest log4j step for CMAES optimization. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch119-math-328513f3_Arja_PatchNaturalnessYe$$update marker. Fix NPE in CMAESOptimizer. Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch121-math-328513f3_Arja_PatchNaturalnessYe$$Reverted accidental throw of MathUtils . doubleValue ( ). Remove a couple incorrect lines. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch37-math-328513f3_Arja_PatchNaturalnessYe$$Remove stray patch .. Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch63-math-328513f3_Arja_PatchNaturalnessYe$$Use the latest log4j step for CMAES optimization. Fix CMAESOptimizer patch .. Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch97-math-328513f3_Arja_PatchNaturalnessYe$$Reverted accidental throw of MathUtils . doubleValue ( ). update marker. Fix a look for a better error message. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch90-math-328513f3_Arja_PatchNaturalnessYe$$Add a backprop to CMAESOptimizer logMu2 .. Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch64-math-328513f3_Arja_PatchNaturalnessYe$$Reverted accidental throw of MathUtils . doubleValue ( ). CCC = min mueff for CMAES optimization. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch99-math-328513f3_Arja_PatchNaturalnessYe$$Reverted accidental throw of MathUtils . doubleValue ( ). readded missing context object .. Remove unnecessary line. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch144-math-328513f3_Arja_PatchNaturalnessYe$$Reverted accidental throw of MathUtils . doubleValue ( ). Fix NPE in CMAESOptimizer. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch52-math-328513f3_Arja_PatchNaturalnessYe$$Use the latest log4j step for CMAES optimization. Fix a bug in CMAESOptimizer where the boundaries are not null. Revert previous patch. Revert previous patch. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch143-math-328513f3_Arja_PatchNaturalnessYe$$Fix diagD. Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch55-math-328513f3_Arja_PatchNaturalnessYe$$Revert previous patch. update marker. CAMEL - 2833 fixed the inverse of square root. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch161-math-328513f3_Arja_PatchNaturalnessYe$$Fix NPE in CMAESOptimizer. Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch77-math-328513f3_Arja_PatchNaturalnessYe$$Reverted accidental throw of MathUtils . doubleValue ( ). Fix diagD = sqrt ( diagD ) ;. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch48-math-328513f3_Arja_PatchNaturalnessYe$$Reverted accidental throw of MathUtils . doubleValue ( ). update penalty from patched PR. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch84-math-328513f3_Arja_PatchNaturalnessYe$$Reverted accidental throw of MathUtils . doubleValue ( ). CAMAESOptimizer now uses one - to - one because it is not compatible with current A. CAMEL - 102 fixed the indentation of the objective function. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch70-math-328513f3_Arja_PatchNaturalnessYe$$update marker. CAMAESOptimizer now uses one - to - one because it is not compatible with current A. Revert MathUtils rounding method. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch8-math-328513f3_Arja_PatchNaturalnessYe$$Reverted accidental throw of MathUtils . doubleValue ( ). Set historySize for CMAESOptimizer . penalty ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch159-math-328513f3_Arja_PatchNaturalnessYe$$Revert MathUtils rounding method. Fix single line error in CMAESOptimizer . repair ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch46-math-328513f3_Arja_PatchNaturalnessYe$$Revert MathUtils rounding method. remove patch from CMAESOptimizer. Fix single line error in CMAESOptimizer . repair ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch150-math-328513f3_Arja_PatchNaturalnessYe$$Fix NPE in CMAESOptimizer. Fix NPE in CMAESOptimizer . repair ( ) .. Revert previous patch. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch1-math-328513f3_Arja_PatchNaturalnessYe$$update patch for CMAESOptimizer. Fix a bug in CMAESOptimizer where the boundaries are not null. Revert previous patch. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch79-math-328513f3_Arja_PatchNaturalnessYe$$Reverted accidental throw of MathUtils . doubleValue ( ). Remove one more test. Remove a couple incorrect lines. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch6-math-328513f3_Arja_PatchNaturalnessYe$$Reverted accidental throw of MathUtils . doubleValue ( ). Fix a minor bug in CMAESOptimizer . java. Fix CMAESOptimizer penalty ( fixed in 1 . 6 . 2 ) .. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch41-math-328513f3_Arja_PatchNaturalnessYe$$Remove ccovmuSep from CMAESOptimizer. Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch157-math-328513f3_Arja_PatchNaturalnessYe$$Reverted accidental throw of MathUtils . doubleValue ( ). Reset fitness history after 1 . 0 value was found. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch103-math-328513f3_Arja_PatchNaturalnessYe$$Reverted accidental throw of MathUtils . doubleValue ( ). Fix CMAESOptimizer patch. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch15-math-328513f3_Arja_PatchNaturalnessYe$$Revert MathUtils rounding method. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch104-math-328513f3_Arja_PatchNaturalnessYe$$Stop the CMAES optimization from running in a war. Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch12-math-328513f3_Arja_PatchNaturalnessYe$$Revert previous patch. Fix single line exception. Fix NPE in CMAESOptimizer. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch24-math-328513f3_Arja_PatchNaturalnessYe$$Fix a bug in CMAESOptimizer where lambda was not applied , this . lambda was not used. Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch132-math-328513f3_Arja_PatchNaturalnessYe$$Reverted accidental throw of MathUtils . doubleValue ( ). update patch. Remove a couple incorrect lines. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch23-math-328513f3_Arja_PatchNaturalnessYe$$Reverted accidental throw of MathUtils . doubleValue ( ). Set context to empty string so it doesn ' t get put into the same place as the other. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch135-math-328513f3_Arja_PatchNaturalnessYe$$Updated patch for MathRuntimeException . getMessage. Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch40-math-328513f3_Arja_PatchNaturalnessYe$$Reverted accidental throw of MathUtils . doubleValue ( ). Fix possible NPE in CMAESOptimizer. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch156-math-328513f3_Arja_PatchNaturalnessYe$$readded missing context object .. Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch7-math-328513f3_Arja_PatchNaturalnessYe$$Set historySize for CMAESOptimizer . penalty ( ). Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch47-math-328513f3_Arja_PatchNaturalnessYe$$update patch. Fix CMAESOptimizer patch. Fix C = diag ( x ) .. Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch78-math-328513f3_Arja_PatchNaturalnessYe$$update normps after convergence check. Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch71-math-328513f3_Arja_PatchNaturalnessYe$$Reverted accidental throw of MathUtils . doubleValue ( ). Updated patch for 118. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch85-math-328513f3_Arja_PatchNaturalnessYe$$Fix CMAESOptimizer patch. Revert previous patch. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch158-math-328513f3_Arja_PatchNaturalnessYe$$increment evaluations . incrementCount ( ). Fix incorrect C = > 1 . 0 in CMAESOptimizer. Fix NPE in CMAESOptimizer. Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch9-math-328513f3_Arja_PatchNaturalnessYe$$Fix unnecessarily non - minimal penalty in CMAESOptimizer. Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch82-math-328513f3_Arja_PatchNaturalnessYe$$Reverted accidental throw of MathUtils . doubleValue ( ). Fix diagD .. Set historySize for CMAESOptimizer . penalty ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch160-math-328513f3_Arja_PatchNaturalnessYe$$Reverted accidental throw of MathUtils . doubleValue ( ). Fix broken patch. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch76-math-328513f3_Arja_PatchNaturalnessYe$$update patch for CMAESOptimizer. Fix CMAESOptimizer penalty ( fixed in 1 . 6 . 2 ) .. Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch49-math-328513f3_Arja_PatchNaturalnessYe$$Remove a couple incorrect lines. Set historySize for CMAESOptimizer . penalty ( ). Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch22-math-328513f3_Arja_PatchNaturalnessYe$$Fix C = diag ( x ) .. Fix NPE in CMAESOptimizer. Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch134-math-328513f3_Arja_PatchNaturalnessYe$$Reverted accidental throw of MathUtils . doubleValue ( ). update patch. Set historySize for CMAES optimization patch .. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch25-math-328513f3_Arja_PatchNaturalnessYe$$Reverted accidental throw of MathUtils . doubleValue ( ). Missing context variable. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch133-math-328513f3_Arja_PatchNaturalnessYe$$Fix uninitialized variable. Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch13-math-328513f3_Arja_PatchNaturalnessYe$$Remove a redundant check. update CMAESOptimizer with a new patch. Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch102-math-328513f3_Arja_PatchNaturalnessYe$$update prev guess after convergence check. Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch14-math-328513f3_Arja_PatchNaturalnessYe$$Revert previous patch. CAMEL - 328513f3 fixed a small bug. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch146-math-328513f3_Arja_PatchNaturalnessYe$$Fix a minor bug in CMAESOptimizer. Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch68-math-328513f3_Arja_PatchNaturalnessYe$$Fix single dimension exception. Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch141-math-328513f3_Arja_PatchNaturalnessYe$$Reverted accidental throw of MathUtils . doubleValue ( ). Added missing patch. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch57-math-328513f3_Arja_PatchNaturalnessYe$$Set iterations = 0 in CMAESOptimizer . java. Fix NPE in CMAESOptimizer. Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch148-math-328513f3_Arja_PatchNaturalnessYe$$Fix a bug in CMAESOptimizer. Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch95-math-328513f3_Arja_PatchNaturalnessYe$$Fix a bug in CMAESOptimizer. Fix a minor bug in CMAESOptimizer. Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch61-math-328513f3_Arja_PatchNaturalnessYe$$Reverted accidental throw of MathUtils . doubleValue ( ). Fix C = diag ( x ) .. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch59-math-328513f3_Arja_PatchNaturalnessYe$$Use the latest log4j step for CMAES optimization. Revert this. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch66-math-328513f3_Arja_PatchNaturalnessYe$$Use a new evaluations object added by accident. Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch92-math-328513f3_Arja_PatchNaturalnessYe$$Reverted accidental throw of MathUtils . doubleValue ( ). update marker. Fix single line exception. Fix single line error in CMAESOptimizer. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch124-math-328513f3_Arja_PatchNaturalnessYe$$Reverted accidental throw of MathUtils . doubleValue ( ). update marker. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch32-math-328513f3_Arja_PatchNaturalnessYe$$update marker. CANCEO - 1216 , adjust for the fix. Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch123-math-328513f3_Arja_PatchNaturalnessYe$$Set historySize for CMAESOptimizer. Fix C = diag ( x ) .. Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch35-math-328513f3_Arja_PatchNaturalnessYe$$Reverted accidental throw of MathUtils . doubleValue ( ). CAMAESOptimizer now uses one - to - one because it is not compatible with current A. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch115-math-328513f3_Arja_PatchNaturalnessYe$$Revert previous patch. Allow null values in CMAESOptimizer. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch112-math-328513f3_Arja_PatchNaturalnessYe$$Reset diagonalonly after each loop. Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch93-math-328513f3_Arja_PatchNaturalnessYe$$Fix a bug in CMAESOptimizer where the boundaries are not null .. Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch67-math-328513f3_Arja_PatchNaturalnessYe$$Revert this. Fix a bug in CMAESOptimizer. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch149-math-328513f3_Arja_PatchNaturalnessYe$$Reverted accidental throw of MathUtils . doubleValue ( ). Fix CMAESOptimizer patch .. Fix incorrect patch. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch60-math-328513f3_Arja_PatchNaturalnessYe$$Set iterations = 0 in CMAESOptimizer . java. Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch94-math-328513f3_Arja_PatchNaturalnessYe$$Reverted accidental throw of MathUtils . doubleValue ( ). Set historySize for CMAESOptimizer. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch69-math-328513f3_Arja_PatchNaturalnessYe$$Reverted accidental throw of MathUtils . doubleValue ( ). readded missing context object .. update patch. Fix cmAESOptimizer patch. Remove stray patch from CMAESOptimizer. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch140-math-328513f3_Arja_PatchNaturalnessYe$$Fix CMAESOptimizer patch. Fix diagD .. Updated patch for MathRuntimeException . getLocalizedMessage ( ). Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch51-math-328513f3_Arja_PatchNaturalnessYe$$Add missing patch. Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch113-math-328513f3_Arja_PatchNaturalnessYe$$Reverted accidental throw of MathUtils . doubleValue ( ). Fix cmAESOptimizer patch. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch114-math-328513f3_Arja_PatchNaturalnessYe$$Fix a bug in CMAESOptimizer caused by incorrect patch .. Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch122-math-328513f3_Arja_PatchNaturalnessYe$$Reverted accidental throw of MathUtils . doubleValue ( ). CANCEO - 1216 , adjust for the fix. Fix CMAESOptimizer patch. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch34-math-328513f3_Arja_PatchNaturalnessYe$$update patch. Fix single line error in CMAESOptimizer. Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch125-math-328513f3_Arja_PatchNaturalnessYe$$CCC = > RBF ( min mueff value ). Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch33-math-328513f3_Arja_PatchNaturalnessYe$$Reverted accidental throw of MathUtils . doubleValue ( ). Fix possible NPE in CMAESOptimizer. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch11-math-328513f3_Arja_PatchNaturalnessYe$$Reverted accidental throw of MathUtils . doubleValue ( ). update marker. Fix a possible NPE in CMAESOptimizer. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch29-math-328513f3_Arja_PatchNaturalnessYe$$CAMEL - 6209 Fixed a bug in CMAESOptimizer such that lambda > .. Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch16-math-328513f3_Arja_PatchNaturalnessYe$$CAMEL - 6209 Fixed the inverse of square root .. CANCER patch attempted fix. Revert previous patch. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch20-math-328513f3_Arja_PatchNaturalnessYe$$Reverted accidental throw of MathUtils . doubleValue ( ). Updated patch for MathRuntimeException. Use the latest log4j step for CMAES optimization. increment evaluations . incrementCount ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch18-math-328513f3_Arja_PatchNaturalnessYe$$Fix uninitialized variable. CAMEL - 7209 Fixed a minor issue with the inverse function of some of the arguments. Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch27-math-328513f3_Arja_PatchNaturalnessYe$$CAMEL - 7209 Fixed a minor issue with the objective function of CMAESOptimizer at. Revert previous patch. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch87-math-328513f3_Arja_PatchNaturalnessYe$$Reverted accidental throw of MathUtils . doubleValue ( ). Remove one more test. Fix diagD .. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch73-math-328513f3_Arja_PatchNaturalnessYe$$Fix NPE in CMAESOptimizer. Set historySize for CMAESOptimizer . penalty ( ). Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch80-math-328513f3_Arja_PatchNaturalnessYe$$Reset fitness history after 1 . 0 value was found. Fix NPE in CMAESOptimizer. Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch42-math-328513f3_Arja_PatchNaturalnessYe$$CANCER patch attempted fix. Remove unnecessary line. Revert previous patch. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch5-math-328513f3_Arja_PatchNaturalnessYe$$Reverted accidental throw of MathUtils . doubleValue ( ). update patch. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch89-math-328513f3_Arja_PatchNaturalnessYe$$Reverted accidental throw of MathUtils . doubleValue ( ). update CMAESOptimizer with a new one - liner. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch2-math-328513f3_Arja_PatchNaturalnessYe$$Fix single line exception. Remove a couple incorrect lines. Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch45-math-328513f3_Arja_PatchNaturalnessYe$$Reverted accidental throw of MathUtils . doubleValue ( ). Fix NPE in CMAESOptimizer. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch19-math-328513f3_Arja_PatchNaturalnessYe$$Reverted accidental throw of MathUtils . doubleValue ( ). Fix NPE in CMAESOptimizer. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch26-math-328513f3_Arja_PatchNaturalnessYe$$Revert previous patch. update patch. Add ccov1 back into repaired objective function. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch21-math-328513f3_Arja_PatchNaturalnessYe$$Fix single line exception. Fix a bug in CMAESOptimizer where the boundaries are not null .. Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch28-math-328513f3_Arja_PatchNaturalnessYe$$Reverted accidental throw of MathUtils . doubleValue ( ). update marker. Use the latest log4j step for CMAES optimization. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch17-math-328513f3_Arja_PatchNaturalnessYe$$Revert MathUtils rounding method. Remove a couple incorrect lines. Fix possible NPE in CMAESOptimizer. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch10-math-328513f3_Arja_PatchNaturalnessYe$$Remove a couple incorrect lines. CCC = > RBF ( min mueff value ). Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch44-math-328513f3_Arja_PatchNaturalnessYe$$CANCER patched , so that we can use the ccov1 function as well. Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch152-math-328513f3_Arja_PatchNaturalnessYe$$Reverted accidental throw of MathUtils . doubleValue ( ). Fix a bug in CMAESOptimizer. Fix NPE in CMAESOptimizer. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch3-math-328513f3_Arja_PatchNaturalnessYe$$Revert MathUtils rounding method. Reset the value of the penalty column to 0 ; the new penalty column would be better placed on. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch4-math-328513f3_Arja_PatchNaturalnessYe$$update penalty from patched PR. Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch43-math-328513f3_Arja_PatchNaturalnessYe$$Reverted accidental throw of MathUtils . doubleValue ( ). update patch. Fix single line exception. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch155-math-328513f3_Arja_PatchNaturalnessYe$$Fix a bug in CMAESOptimizer where the boundaries are not null. Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch88-math-328513f3_Arja_PatchNaturalnessYe$$Fix diagD .. Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch81-math-328513f3_Arja_PatchNaturalnessYe$$Reverted accidental throw of MathUtils . doubleValue ( ). CAMEL - 2833 fixed lambda = 4 + ( int ) ( 3 . log (. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch163-math-328513f3_Arja_PatchNaturalnessYe$$Reverted accidental throw of MathUtils . doubleValue ( ). Fix C = diag ( x ) .. CCC = > RBF ( min mueff value ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch75-math-328513f3_Arja_PatchNaturalnessYe$$Fix cmAESOptimizer patch. Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch164-math-328513f3_Arja_PatchNaturalnessYe$$update lambda / 2. Reverted accidental throw of MathUtils . doubleValue ( ). $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch72-math-328513f3_Arja_PatchNaturalnessYe$$Reverted accidental throw of MathUtils . doubleValue ( ). Add a logMu2 variable to CMAESOptimizer . java. $$0
math-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.$$patch86-math-328513f3_Arja_PatchNaturalnessYe$$Fix a bug in CMAESOptimizer. Added CCC to the patch for CMAES optimization. Revert previous patch. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch31-math-69273dca_Arja_PatchNaturalnessYe$$Fix NPE in refactoring , see. Fix a minor bug in the code. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch38-math-69273dca_Arja_PatchNaturalnessYe$$Reset stepStart after each runge test .. Fix test in parallel. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch53-math-69273dca_Arja_PatchNaturalnessYe$$Fix bug in OE. Fix for OE. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch65-math-69273dca_Arja_PatchNaturalnessYe$$Fix a bug in the code that was not working right now. Fix a bug in the interpolator .. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch62-math-69273dca_Arja_PatchNaturalnessYe$$Remove sanity checks from test script. Set stepSize to Double . NaN for nonstiff integration. Fix NPE in runge kutta integator. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch39-math-69273dca_Arja_PatchNaturalnessYe$$Add one more test patch. Fix a bug in the interpolator .. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch30-math-69273dca_Arja_PatchNaturalnessYe$$Fix a bug in the coherency checks of ANT ( and OE ). $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch37-math-69273dca_Arja_PatchNaturalnessYe$$Fix step size of JAR in reverse order ( fixes # 77 ). Fix a bug in the interpolator .. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch63-math-69273dca_Arja_PatchNaturalnessYe$$Add stepStart variable to variable yDotTmp. Fix test in parallel. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch64-math-69273dca_Arja_PatchNaturalnessYe$$Set stepSize to Double . NaN for parallel execution. Fix a bug in the interpolator .. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch52-math-69273dca_Arja_PatchNaturalnessYe$$Fix a bug in the code that was failing the other day .. Fix a bug in the interpolator .. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch55-math-69273dca_Arja_PatchNaturalnessYe$$Fix minor bug in runge kutta integration loop. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch77-math-69273dca_Arja_PatchNaturalnessYe$$Fix a bug in the interpolator .. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch83-math-69273dca_Arja_PatchNaturalnessYe$$Remove sanity checks from test script. Fix a bug in the step interpolator ' s definition .. Fix a bug in the interpolator .. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch48-math-69273dca_Arja_PatchNaturalnessYe$$Fix for step size > = t. Fix a bug in the interpolator .. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch84-math-69273dca_Arja_PatchNaturalnessYe$$Remove sanity checks from test script. Fix a bug in the step method. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch70-math-69273dca_Arja_PatchNaturalnessYe$$Remove sanity checks from test script. Fix minor bug in runge kutta integration. Fix a bug in the step method. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch8-math-69273dca_Arja_PatchNaturalnessYe$$Fix for OE. Fix a bug in the step method. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch46-math-69273dca_Arja_PatchNaturalnessYe$$Fix a bug in the code that binaries together with the others .. Fix a bug in the step method. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch1-math-69273dca_Arja_PatchNaturalnessYe$$Fix NPE in OE. Fix minor bug in runge kutta integration loop. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch79-math-69273dca_Arja_PatchNaturalnessYe$$Fixed a bug in the runge kutta integration test. Fix a bug in the step method. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch6-math-69273dca_Arja_PatchNaturalnessYe$$added patch for runge kutta integator. Fix for OE. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch41-math-69273dca_Arja_PatchNaturalnessYe$$Fixed the bug where the polynomials were not being used .. Fix a bug in the runge kutta integator .. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch15-math-69273dca_Arja_PatchNaturalnessYe$$Fix a bug in the code that binaries together with the others .. Fix for OE. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch12-math-69273dca_Arja_PatchNaturalnessYe$$Fix step size not being greater than t. Fix a bug where the step was not being applied , as the code now requires new interpolator .. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch24-math-69273dca_Arja_PatchNaturalnessYe$$Fix a bug in the code that was failing the other day .. Fix test in parallel. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch23-math-69273dca_Arja_PatchNaturalnessYe$$fixed a small bug. Fix for OE. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch40-math-69273dca_Arja_PatchNaturalnessYe$$Fix step size not being able to move between 0 and t - stepStart. Fix a bug in the interpolator .. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch78-math-69273dca_Arja_PatchNaturalnessYe$$minimize step size for nonstiffed rune kutta integrator. added missing patch. Fix for OE. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch85-math-69273dca_Arja_PatchNaturalnessYe$$Fix bug in step number. Fix a bug in the step method. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch9-math-69273dca_Arja_PatchNaturalnessYe$$Add sanity checks back. added missing patch. Fix a bug in the interpolator .. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch82-math-69273dca_Arja_PatchNaturalnessYe$$Fix minor bug in runge kutta integration loop. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch76-math-69273dca_Arja_PatchNaturalnessYe$$Fix a bug in the step method. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch49-math-69273dca_Arja_PatchNaturalnessYe$$added patch for runge kutta integration. Fix a bug in the step method. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch22-math-69273dca_Arja_PatchNaturalnessYe$$Fix a minor bug in the code. Fix a bug in the step method. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch25-math-69273dca_Arja_PatchNaturalnessYe$$Remove sanity checks from test script. Fix a bug in the interpolator .. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch13-math-69273dca_Arja_PatchNaturalnessYe$$Fix a minor bug in the code. Fix a bug in the interpolator .. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch14-math-69273dca_Arja_PatchNaturalnessYe$$Remove sanity checks from test script. added patch for runge kutta integration. Fix a bug in the step method. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch50-math-69273dca_Arja_PatchNaturalnessYe$$reset isLastStep = false ;. Fix minor bug in runge kutta integration loop. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch68-math-69273dca_Arja_PatchNaturalnessYe$$Fix a bug in the code that binaries together with the others .. Fix a bug in the coherency checks of ANT ( and OE ). $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch57-math-69273dca_Arja_PatchNaturalnessYe$$Fix bug in step number. Fix a bug in the interpolator .. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch61-math-69273dca_Arja_PatchNaturalnessYe$$Fix NPE in AE Rigid Body < - > world collisions. Fix a minor bug in the code. Fix a bug in the step method. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch59-math-69273dca_Arja_PatchNaturalnessYe$$adding stepSize to code. Fix a bug in the interpolator .. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch66-math-69273dca_Arja_PatchNaturalnessYe$$Fix step size not being greater than t. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch32-math-69273dca_Arja_PatchNaturalnessYe$$added patch for @@. Fix a bug in the interpolator .. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch35-math-69273dca_Arja_PatchNaturalnessYe$$Fix NPE in AE Rigid Body < - > world collisions. Fix bug in step interpolator for backwards compatibility. Fix a bug in the step method. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch58-math-69273dca_Arja_PatchNaturalnessYe$$Add one more patch. Fix double step loop. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch67-math-69273dca_Arja_PatchNaturalnessYe$$NPE in runge kutta integators. Fix a bug in the interpolator .. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch60-math-69273dca_Arja_PatchNaturalnessYe$$Fix a bug in the code that binaries together with the others .. Fix a bug in the interpolator .. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch56-math-69273dca_Arja_PatchNaturalnessYe$$Fix a bug in the step interpolator patch .. Fix a bug in the interpolator .. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch51-math-69273dca_Arja_PatchNaturalnessYe$$Fix minor bug in runge kutta integration loop. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch34-math-69273dca_Arja_PatchNaturalnessYe$$Fix NPE in OE. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch33-math-69273dca_Arja_PatchNaturalnessYe$$Reset stepStart after each runge test .. Fix a bug in the interpolator .. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch11-math-69273dca_Arja_PatchNaturalnessYe$$Add stepStart variable to equations , for rebase. Fix a minor bug in the code. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch29-math-69273dca_Arja_PatchNaturalnessYe$$Add sanity checks back. Fix for OE. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch16-math-69273dca_Arja_PatchNaturalnessYe$$Set stepSize to Double . NaN for nonstiff integration. Fix a bug in the interpolator .. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch20-math-69273dca_Arja_PatchNaturalnessYe$$Fix minor bug in runge kutta integration loop. Fix a bug where the step was not being applied properly .. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch18-math-69273dca_Arja_PatchNaturalnessYe$$Fix a bug in the yDotK [] field. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch27-math-69273dca_Arja_PatchNaturalnessYe$$Fix step size = t - stepStart if we are not in the last step. Fix a bug in the interpolator not last step. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch73-math-69273dca_Arja_PatchNaturalnessYe$$added patch for parallel execution. Fix a bug in the interpolator .. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch74-math-69273dca_Arja_PatchNaturalnessYe$$added patch for parallel execution. Fix a bug in the interpolator .. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch80-math-69273dca_Arja_PatchNaturalnessYe$$Fix a bug in the step method where the reverse is not the intentional .. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch42-math-69273dca_Arja_PatchNaturalnessYe$$Set stepSize to Double . NaN for nonstiff integration. Fix for OE. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch5-math-69273dca_Arja_PatchNaturalnessYe$$Added missing super call to sanity checks. Fix a bug in the interpolator .. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch2-math-69273dca_Arja_PatchNaturalnessYe$$added patch for runge kutta integator. Fix minor typo in test. Fix a bug in the step method. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch45-math-69273dca_Arja_PatchNaturalnessYe$$Remove sanity checks from test script. added patch for CI integration. Fix a bug in the interpolator .. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch19-math-69273dca_Arja_PatchNaturalnessYe$$Remove sanity checks from opengl. Fix step size for parallel builds. Fix a bug in the step method. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch26-math-69273dca_Arja_PatchNaturalnessYe$$Set stepSize to Double . NaN for nonstiffed runge kutta integration. Fix a bug in the interpolator .. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch21-math-69273dca_Arja_PatchNaturalnessYe$$Set stepSize = forward ? - step : 1 . step. Fix a bug in the interpolator .. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch28-math-69273dca_Arja_PatchNaturalnessYe$$Fix test in parallel. Fix a bug in the step method. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch17-math-69273dca_Arja_PatchNaturalnessYe$$Fix NPE in parallel. Fix a bug in the interpolator .. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch3-math-69273dca_Arja_PatchNaturalnessYe$$I had left it harwired to the edges , but I noticed this got an NPE. Fix a bug in the interpolator .. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch4-math-69273dca_Arja_PatchNaturalnessYe$$Add sanity checks back. Fix a bug in the interpolator .. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch43-math-69273dca_Arja_PatchNaturalnessYe$$Remove sanity checks from opengl. Fix max step size reached in DL. Fix a minor bug in the interpolator .. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch81-math-69273dca_Arja_PatchNaturalnessYe$$Fix a bug in the code of RungeKuttaIntegator . reinitialize (. Fix a bug in the interpolator .. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch75-math-69273dca_Arja_PatchNaturalnessYe$$Remove sanity checks from opengl. Fix for OE. Fix a bug in the step method. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch72-math-69273dca_Arja_PatchNaturalnessYe$$Fix a bug in the code. Fix a bug in the interpolator .. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch86-math-69273dca_Arja_PatchNaturalnessYe$$Add sanity checks back. Fix for OE. $$0
math-2123f780$$Complex Add and Subtract handle NaN arguments differently, but javadoc contracts are the same$$For both Complex add and subtract, the javadoc states that  {code}      * If either this or <code>rhs</code> has a NaN value in either part,      * {@link #NaN} is returned; otherwise Inifinite and NaN values are      * returned in the parts of the result according to the rules for      * {@link java.lang.Double} arithmetic {code}  Subtract includes an isNaN test and returns Complex.NaN if either complex argument isNaN; but add omits this test.  The test should be added to the add implementation (actually restored, since this looks like a code merge problem going back to 1.1).$$patch8-math-2123f780_Arja_PatchNaturalnessYe$$Add the missing null check. $$0
math-2123f780$$Complex Add and Subtract handle NaN arguments differently, but javadoc contracts are the same$$For both Complex add and subtract, the javadoc states that  {code}      * If either this or <code>rhs</code> has a NaN value in either part,      * {@link #NaN} is returned; otherwise Inifinite and NaN values are      * returned in the parts of the result according to the rules for      * {@link java.lang.Double} arithmetic {code}  Subtract includes an isNaN test and returns Complex.NaN if either complex argument isNaN; but add omits this test.  The test should be added to the add implementation (actually restored, since this looks like a code merge problem going back to 1.1).$$patch15-math-2123f780_Arja_PatchNaturalnessYe$$Add a missing if / else in Complex . add ( ). $$0
math-2123f780$$Complex Add and Subtract handle NaN arguments differently, but javadoc contracts are the same$$For both Complex add and subtract, the javadoc states that  {code}      * If either this or <code>rhs</code> has a NaN value in either part,      * {@link #NaN} is returned; otherwise Inifinite and NaN values are      * returned in the parts of the result according to the rules for      * {@link java.lang.Double} arithmetic {code}  Subtract includes an isNaN test and returns Complex.NaN if either complex argument isNaN; but add omits this test.  The test should be added to the add implementation (actually restored, since this looks like a code merge problem going back to 1.1).$$patch12-math-2123f780_Arja_PatchNaturalnessYe$$Add the missing NaN check. $$0
math-2123f780$$Complex Add and Subtract handle NaN arguments differently, but javadoc contracts are the same$$For both Complex add and subtract, the javadoc states that  {code}      * If either this or <code>rhs</code> has a NaN value in either part,      * {@link #NaN} is returned; otherwise Inifinite and NaN values are      * returned in the parts of the result according to the rules for      * {@link java.lang.Double} arithmetic {code}  Subtract includes an isNaN test and returns Complex.NaN if either complex argument isNaN; but add omits this test.  The test should be added to the add implementation (actually restored, since this looks like a code merge problem going back to 1.1).$$patch7-math-2123f780_Arja_PatchNaturalnessYe$$Add NaN to complex add ( ). $$0
math-2123f780$$Complex Add and Subtract handle NaN arguments differently, but javadoc contracts are the same$$For both Complex add and subtract, the javadoc states that  {code}      * If either this or <code>rhs</code> has a NaN value in either part,      * {@link #NaN} is returned; otherwise Inifinite and NaN values are      * returned in the parts of the result according to the rules for      * {@link java.lang.Double} arithmetic {code}  Subtract includes an isNaN test and returns Complex.NaN if either complex argument isNaN; but add omits this test.  The test should be added to the add implementation (actually restored, since this looks like a code merge problem going back to 1.1).$$patch9-math-2123f780_Arja_PatchNaturalnessYe$$Add NaN check. $$0
math-2123f780$$Complex Add and Subtract handle NaN arguments differently, but javadoc contracts are the same$$For both Complex add and subtract, the javadoc states that  {code}      * If either this or <code>rhs</code> has a NaN value in either part,      * {@link #NaN} is returned; otherwise Inifinite and NaN values are      * returned in the parts of the result according to the rules for      * {@link java.lang.Double} arithmetic {code}  Subtract includes an isNaN test and returns Complex.NaN if either complex argument isNaN; but add omits this test.  The test should be added to the add implementation (actually restored, since this looks like a code merge problem going back to 1.1).$$patch13-math-2123f780_Arja_PatchNaturalnessYe$$Add NaN check. $$0
math-2123f780$$Complex Add and Subtract handle NaN arguments differently, but javadoc contracts are the same$$For both Complex add and subtract, the javadoc states that  {code}      * If either this or <code>rhs</code> has a NaN value in either part,      * {@link #NaN} is returned; otherwise Inifinite and NaN values are      * returned in the parts of the result according to the rules for      * {@link java.lang.Double} arithmetic {code}  Subtract includes an isNaN test and returns Complex.NaN if either complex argument isNaN; but add omits this test.  The test should be added to the add implementation (actually restored, since this looks like a code merge problem going back to 1.1).$$patch14-math-2123f780_Arja_PatchNaturalnessYe$$Add NaN to Complex . add ( ). $$0
math-2123f780$$Complex Add and Subtract handle NaN arguments differently, but javadoc contracts are the same$$For both Complex add and subtract, the javadoc states that  {code}      * If either this or <code>rhs</code> has a NaN value in either part,      * {@link #NaN} is returned; otherwise Inifinite and NaN values are      * returned in the parts of the result according to the rules for      * {@link java.lang.Double} arithmetic {code}  Subtract includes an isNaN test and returns Complex.NaN if either complex argument isNaN; but add omits this test.  The test should be added to the add implementation (actually restored, since this looks like a code merge problem going back to 1.1).$$patch11-math-2123f780_Arja_PatchNaturalnessYe$$Add the missing checkNotNull check. $$0
math-2123f780$$Complex Add and Subtract handle NaN arguments differently, but javadoc contracts are the same$$For both Complex add and subtract, the javadoc states that  {code}      * If either this or <code>rhs</code> has a NaN value in either part,      * {@link #NaN} is returned; otherwise Inifinite and NaN values are      * returned in the parts of the result according to the rules for      * {@link java.lang.Double} arithmetic {code}  Subtract includes an isNaN test and returns Complex.NaN if either complex argument isNaN; but add omits this test.  The test should be added to the add implementation (actually restored, since this looks like a code merge problem going back to 1.1).$$patch10-math-2123f780_Arja_PatchNaturalnessYe$$Add NaN to fix # 1796. $$0
math-cc4ab51e$$BinomialDistribution deals with degenerate cases incorrectly$$The following calculation returns false results:  {{new BinomialDistribution(0, 0.01).logProbability(0)}}  It evaluates to Double.NaN when it should be 0 (cf., for example, "dbinom(0, 0, 0.01, log=T)" in R).  I attach a patch dealing with the problem. The patch also adds a test for this bug.$$patch25-math-cc4ab51e_Arja_PatchNaturalnessYe$$Fix SaddlePointExpansion test. Fix NPE in SaddlePointExpansion. $$0
math-cc4ab51e$$BinomialDistribution deals with degenerate cases incorrectly$$The following calculation returns false results:  {{new BinomialDistribution(0, 0.01).logProbability(0)}}  It evaluates to Double.NaN when it should be 0 (cf., for example, "dbinom(0, 0, 0.01, log=T)" in R).  I attach a patch dealing with the problem. The patch also adds a test for this bug.$$patch2-math-cc4ab51e_Arja_PatchNaturalnessYe$$Fix SaddlePointExpansion with a max of 1 , min = - 1 , max = -. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch120-math-a06a1584_Arja_PatchNaturalnessYe$$fixed a small bug. removed a redundant line. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch36-math-a06a1584_Arja_PatchNaturalnessYe$$Add a missing + in StorelessCovariance. Remove a bit of a bit of code. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch127-math-a06a1584_Arja_PatchNaturalnessYe$$Add StorelessCovariance patch. Remove a bit of a bit of code. Remove stray return statement. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch31-math-a06a1584_Arja_PatchNaturalnessYe$$removed a redundant line. Remove stray return statement. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch111-math-a06a1584_Arja_PatchNaturalnessYe$$removed patch from accidently removed pull request. fixed a small bug. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch116-math-a06a1584_Arja_PatchNaturalnessYe$$fixed a small bug. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch129-math-a06a1584_Arja_PatchNaturalnessYe$$Fixed a bug in PolyhedronsSet .. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch189-math-a06a1584_Arja_PatchNaturalnessYe$$fixed a small bug. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch142-math-a06a1584_Arja_PatchNaturalnessYe$$added missing visit to fix # 12. Remove inside attribute from patch. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch54-math-a06a1584_Arja_PatchNaturalnessYe$$Set the size of the cut hyperplane if it was not found .. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch98-math-a06a1584_Arja_PatchNaturalnessYe$$fixed a small bug. cleaned up code. Remove stray check for edge effect. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch53-math-a06a1584_Arja_PatchNaturalnessYe$$Removed a bit of debug code. Remove a bit of code that was accidentally left in the last commit. Remove stray return statement. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch65-math-a06a1584_Arja_PatchNaturalnessYe$$Reset the size of the cut node to zero. Removed a couple of debug code. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch173-math-a06a1584_Arja_PatchNaturalnessYe$$Set barycenter again .. Remove stray check for edge effect. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch187-math-a06a1584_Arja_PatchNaturalnessYe$$added cut null check. Remove a redundant patch. Changed default size of the patch .. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch180-math-a06a1584_Arja_PatchNaturalnessYe$$Add a missing + in StorelessCovariance. fixed a small bug. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch62-math-a06a1584_Arja_PatchNaturalnessYe$$Revert previous fix. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch174-math-a06a1584_Arja_PatchNaturalnessYe$$Remove a bit of a cruft. Readded missing visit to fix merge issue. Add a missing + in StorelessCovariance. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch128-math-a06a1584_Arja_PatchNaturalnessYe$$Removed a couple of debug code. Set size to 0 when checking in - place ( fixes # 2362 ). $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch110-math-a06a1584_Arja_PatchNaturalnessYe$$Removed a redundant call to setBarycenter ( ). Remove stray return statement. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch39-math-a06a1584_Arja_PatchNaturalnessYe$$removed a redundant line. Remove stray return statement. Add StorelessCovariance patch. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch126-math-a06a1584_Arja_PatchNaturalnessYe$$Added missing patch. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch30-math-a06a1584_Arja_PatchNaturalnessYe$$Removed a redundant call to setBarycenter ( ). $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch119-math-a06a1584_Arja_PatchNaturalnessYe$$Set the barycenter to be 1 . 0 / ( 4 * getSize ( ) ). added missing visit to fix # 1796. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch313-math-a06a1584_Arja_PatchNaturalnessYe$$Added visit to fix # 1296. Fixed a minor issue with fixed geometry size .. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch121-math-a06a1584_Arja_PatchNaturalnessYe$$removed a couple of lines from the edge case. Set size ( ) to 3 . 0 for parallel crossings. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch37-math-a06a1584_Arja_PatchNaturalnessYe$$Fixed a bug in PolyhedronsSet .. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch63-math-a06a1584_Arja_PatchNaturalnessYe$$fixed null cut. fixed a small bug. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch175-math-a06a1584_Arja_PatchNaturalnessYe$$Resetting frame opengl opengl opengl opengl opengl opengl opengl opengl opengl opengl opengl opengl opengl opengl opengl opengl. Remove a bit of code that was accidentally left in the last commit. Readded missing visit. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch97-math-a06a1584_Arja_PatchNaturalnessYe$$Set the size of the cut node to zero if it wasn ' t found. Remove stray patch code. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch181-math-a06a1584_Arja_PatchNaturalnessYe$$Remove a couple of tries to fix a merge conflict .. Readded missing visit. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch90-math-a06a1584_Arja_PatchNaturalnessYe$$added cut null check. Remove a bit of code that was accidentally left in the last commit. Changed default size of the PolyhedronsSet to 3 . 0. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch186-math-a06a1584_Arja_PatchNaturalnessYe$$Fixed a minor issue with B + Rigid Body < - > world crossing. Remove stray return statement. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch172-math-a06a1584_Arja_PatchNaturalnessYe$$Added fix for FacetsContributionVisitor. Remove stray return statement. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch99-math-a06a1584_Arja_PatchNaturalnessYe$$Set the barycenter of the PolyhedronsSet object in firstIntersection. Remove stray return statement. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch144-math-a06a1584_Arja_PatchNaturalnessYe$$removed patch from firstIntersection. Removed patch from PolyhedronsSet. Remove stray return statement. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch188-math-a06a1584_Arja_PatchNaturalnessYe$$Added a visit to fix a bug. Remove a bit of code that was accidentally left in the last commit. Remove stray check for inside attribute. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch143-math-a06a1584_Arja_PatchNaturalnessYe$$Removed patch from pull request. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch203-math-a06a1584_Arja_PatchNaturalnessYe$$Remove a bit of a cruft. Remove inside check. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch204-math-a06a1584_Arja_PatchNaturalnessYe$$added cut null check. removed patch from accidently removed line. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch232-math-a06a1584_Arja_PatchNaturalnessYe$$fixed null cut. Remove stray check for edge effect. Set originOffset to 0 ; @ Deprecated. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch235-math-a06a1584_Arja_PatchNaturalnessYe$$added missing patch. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch295-math-a06a1584_Arja_PatchNaturalnessYe$$fixed erroneous test. added missing visit to fix # 12. Remove inside attribute from patch .. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch259-math-a06a1584_Arja_PatchNaturalnessYe$$added missing patch. Remove stray check for edge effect. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch266-math-a06a1584_Arja_PatchNaturalnessYe$$Set barycenter of sub - set. Fixed a bug in PolyhedronsSet caused by b - r8b0e. Remove stray patch. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch292-math-a06a1584_Arja_PatchNaturalnessYe$$Fixed a minor issue with points in the cut hyperplane. removed a redundant line. Readded missing visit. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch268-math-a06a1584_Arja_PatchNaturalnessYe$$Fixed a bug in the cut hyperplane search. Remove stray return statement. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch257-math-a06a1584_Arja_PatchNaturalnessYe$$Added visit to fix # 1803. added missing visit to fix # 1803. Changed boundary facet point size to 3 . 0. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch234-math-a06a1584_Arja_PatchNaturalnessYe$$Modified line speed and line length. updated patch. Remove stray check for inside attribute. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch233-math-a06a1584_Arja_PatchNaturalnessYe$$added missing visit to fix # 77. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch205-math-a06a1584_Arja_PatchNaturalnessYe$$Remove a couple of tries to fix a merge issue. Readded missing visit. Plane intersection method used for 1 . 8. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch202-math-a06a1584_Arja_PatchNaturalnessYe$$Reverted accidental change of walk ( ). $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch269-math-a06a1584_Arja_PatchNaturalnessYe$$Add StorelessCovariance patch. Remove a bit of a cruft. Set barycenter of Euclidean3D .. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch256-math-a06a1584_Arja_PatchNaturalnessYe$$Add a boundary facet test to PolyhedronsSet. Remove a bit of code that was accidentally left in the last commit. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch251-math-a06a1584_Arja_PatchNaturalnessYe$$added missing visit to fix # 12. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch258-math-a06a1584_Arja_PatchNaturalnessYe$$Add a patch for testing EP .. Remove a bit of code that was accidentally left in the last commit. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch293-math-a06a1584_Arja_PatchNaturalnessYe$$Set the barycenter of the PolyhedronsSet object to the same value as. Fixed a bug in PolyhedronsSet. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch267-math-a06a1584_Arja_PatchNaturalnessYe$$added missing visit. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch260-math-a06a1584_Arja_PatchNaturalnessYe$$Fixed a bug in the - - fixed - search order .. reset size if not in case of cut. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch242-math-a06a1584_Arja_PatchNaturalnessYe$$Set the barycenter of the PolyhedronsSet object in firstIntersection. Remove a bit of code that was accidentally left in the last commit. fixed a minor issue with the plane method. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch289-math-a06a1584_Arja_PatchNaturalnessYe$$Remove a problematic empty line. Set barycenter attribute value to null. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch245-math-a06a1584_Arja_PatchNaturalnessYe$$Fixed a minor issue with fixed geometry size .. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch287-math-a06a1584_Arja_PatchNaturalnessYe$$fixed a small bug. Remove a bit of code that was accidentally left in the last commit. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch273-math-a06a1584_Arja_PatchNaturalnessYe$$Fixed a bug in the - - fixed - search order .. Remove a bit of a test code. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch274-math-a06a1584_Arja_PatchNaturalnessYe$$Set barycenter attribute value to null. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch280-math-a06a1584_Arja_PatchNaturalnessYe$$Fix a minor issue with missing patch. removed patch from accidently removed line. Set size = 0 after checking for in - line evidence. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch220-math-a06a1584_Arja_PatchNaturalnessYe$$Added missing visit to fix .. Remove a bit of code that was accidentally left in the last commit. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch218-math-a06a1584_Arja_PatchNaturalnessYe$$Reset the size of the PolyhedronsSet in case of a small issue. Remove stray return statement. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch227-math-a06a1584_Arja_PatchNaturalnessYe$$fixed a small bug. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch211-math-a06a1584_Arja_PatchNaturalnessYe$$Remove a bit of code that was accidentally left in the last commit. fixed a minor issue with BSP tree edges. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch216-math-a06a1584_Arja_PatchNaturalnessYe$$Fixed a bug in PolyhedronsSet with a smaller size .. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch281-math-a06a1584_Arja_PatchNaturalnessYe$$fixed a minor issue with the plane method. Add a fix for crossing on the plane. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch275-math-a06a1584_Arja_PatchNaturalnessYe$$Set the size of the cut node to zero in case of a geometry change. fixed null cut. updated patch. Removed the size from the patch .. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch286-math-a06a1584_Arja_PatchNaturalnessYe$$fixed null cut. Fixed a minor issue with missing patch .. Remove stray check for inside attribute. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch244-math-a06a1584_Arja_PatchNaturalnessYe$$Fixed a bug in the PolyhedronsSet .. Remove a bit of code that was accidentally left in the last commit. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch243-math-a06a1584_Arja_PatchNaturalnessYe$$added cut null check. Fixed a bug in PolyhedronsSet where the hit3D crossed the plane. Remove stray patch. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch228-math-a06a1584_Arja_PatchNaturalnessYe$$Fixed a bug in the - - fixed - search order .. Removed a redundant call to setBarycenter ( ). $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch210-math-a06a1584_Arja_PatchNaturalnessYe$$Fixed a bug in the - - fixed - search order .. Remove a bit of code that was accidentally left in the last commit. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch226-math-a06a1584_Arja_PatchNaturalnessYe$$Set barycenter to 1 . 0 / ( 4 * getSize ( ) ). Remove a bit of code that was accidentally left in the last commit. Remove stray return statement. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch161-math-a06a1584_Arja_PatchNaturalnessYe$$Set barycenter manually .. Remove a bit of code that was accidentally left in the last commit. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch77-math-a06a1584_Arja_PatchNaturalnessYe$$Set barycenter manually .. set barycenter again. Remove a bit of code that was accidentally left in the last commit. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch83-math-a06a1584_Arja_PatchNaturalnessYe$$fixed a bug. Remove a bit of a hack. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch48-math-a06a1584_Arja_PatchNaturalnessYe$$update patch for euclidean line. fixed a small bug. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch192-math-a06a1584_Arja_PatchNaturalnessYe$$removed intermittent crossed point search. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch84-math-a06a1584_Arja_PatchNaturalnessYe$$set barycenter again. Remove a bit of code that was accidentally left in the last commit. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch166-math-a06a1584_Arja_PatchNaturalnessYe$$fixed a small bug. Remove stray return statement. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch70-math-a06a1584_Arja_PatchNaturalnessYe$$fixed a small bug. Removed patch from pull request. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch8-math-a06a1584_Arja_PatchNaturalnessYe$$throw exception if crossed line is not found. Remove a bit of a cruft. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch159-math-a06a1584_Arja_PatchNaturalnessYe$$Add StorelessCovariance patch. Remove a bit of a cruft. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch46-math-a06a1584_Arja_PatchNaturalnessYe$$fixed a small bug. Remove a bit of code that was accidentally left in the last commit. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch150-math-a06a1584_Arja_PatchNaturalnessYe$$Remove inside check. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch1-math-a06a1584_Arja_PatchNaturalnessYe$$Remove a bit of code that was accidentally left in the last commit. Remove stray check for inside attribute. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch79-math-a06a1584_Arja_PatchNaturalnessYe$$fixed a small bug. Removed the size from the patch .. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch6-math-a06a1584_Arja_PatchNaturalnessYe$$added missing visit to fix # 183. Remove a redundant patch. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch157-math-a06a1584_Arja_PatchNaturalnessYe$$Add StorelessCovariance . initializeMatrix ( ). Remove a bit of a bit of code. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch168-math-a06a1584_Arja_PatchNaturalnessYe$$Added fix on FacetsContribution. Fixed a bug in PolyhedronsSet .. Remove a bit of code that was accidentally left in the last commit. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch15-math-a06a1584_Arja_PatchNaturalnessYe$$Remove a couple of tries to fix a merge issue. fixed a minor issue with the plane method. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch12-math-a06a1584_Arja_PatchNaturalnessYe$$increased the size of the PolyhedronsSet to 3 . 0. Remove a bit of code that was accidentally left in the last commit. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch24-math-a06a1584_Arja_PatchNaturalnessYe$$fixed null cut. Remove boundaryFacet from PolyhedronsSet. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch23-math-a06a1584_Arja_PatchNaturalnessYe$$Set barycenter to 1 . 0 / ( 4 * getSize ( ) ). Remove a bit of code that was accidentally left in the last commit. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch135-math-a06a1584_Arja_PatchNaturalnessYe$$Set barycenter of diff node. Remove a bit of code that was accidentally left in the last commit. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch47-math-a06a1584_Arja_PatchNaturalnessYe$$Added visit to fix # 1896. Remove a couple of debug code. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch71-math-a06a1584_Arja_PatchNaturalnessYe$$ plane cross product erroneously removed from line .. added cut null check. Remove a bit of code that was accidentally left in the last commit. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch85-math-a06a1584_Arja_PatchNaturalnessYe$$Remove a problematic empty line. Added a visit to fix # 12. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch82-math-a06a1584_Arja_PatchNaturalnessYe$$Fix euclidean intersection bug. Remove a bit of a cruft. Remove stray return statement. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch76-math-a06a1584_Arja_PatchNaturalnessYe$$removed a redundant line. Resetting frame opengl opengl opengl opengl opengl opengl opengl opengl opengl opengl opengl opengl opengl opengl opengl opengl. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch49-math-a06a1584_Arja_PatchNaturalnessYe$$removed a redundant line. Set barycenter attribute value. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch22-math-a06a1584_Arja_PatchNaturalnessYe$$Fixed a minor issue with the far branch search .. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch13-math-a06a1584_Arja_PatchNaturalnessYe$$Added missing visit to fix .. Set barycenter. Remove a bit of code that was accidentally left in the last commit. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch179-math-a06a1584_Arja_PatchNaturalnessYe$$Added a patch for testing forbidden intersections. Remove a bit of code that was accidentally left in the last commit. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch146-math-a06a1584_Arja_PatchNaturalnessYe$$Changed max size of the PolyhedronsSet if it was less than 0 .. removed a redundant line. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch50-math-a06a1584_Arja_PatchNaturalnessYe$$fixed null cut. cleaned up code. Remove stray check for inside attribute. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch68-math-a06a1584_Arja_PatchNaturalnessYe$$Set the barycenter of the PolyhedronsSet if it could not find a. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch141-math-a06a1584_Arja_PatchNaturalnessYe$$Fixed a bug in the far branch search .. Remove stray check for edge effect. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch57-math-a06a1584_Arja_PatchNaturalnessYe$$fixed a small bug. Remove a bit of code that was accidentally left in the last commit. Remove stray return statement. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch148-math-a06a1584_Arja_PatchNaturalnessYe$$Added visit to fix # 1896. Added patch for crossing point on fixed plane. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch183-math-a06a1584_Arja_PatchNaturalnessYe$$added missing visit to fix # 183. fixed a small bug. Reverted accidental change of boundary facet .. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch66-math-a06a1584_Arja_PatchNaturalnessYe$$Fixed a bug in PolyhedronsSet .. Remove a bit of code that was accidentally left in the last commit. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch170-math-a06a1584_Arja_PatchNaturalnessYe$$Changed boundary facet point to be in - place .. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch92-math-a06a1584_Arja_PatchNaturalnessYe$$Set the barycenter of the PolyhedronsSet object in firstIntersection. Set size of PolyhedronsSet .. Remove a redundant patch. Remove stray return statement. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch124-math-a06a1584_Arja_PatchNaturalnessYe$$fixed a small bug. Fixed a bug in the PolyhedronsSet constructor. Set the barycenter of the PolyhedronsSet if it could not find a. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch32-math-a06a1584_Arja_PatchNaturalnessYe$$Removed a couple of debug code. Reduce size of the PolyhedronsSet barycenter to 3 . 0. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch311-math-a06a1584_Arja_PatchNaturalnessYe$$Fixed a bug in PolyhedronsSet .. removed patch from accidently removed line. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch123-math-a06a1584_Arja_PatchNaturalnessYe$$Fixed a small bug in PolyhedronsSet. Remove stray patch. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch35-math-a06a1584_Arja_PatchNaturalnessYe$$Fixed a bug in PolyhedronsSet where the hit3D crossed the plane. Remove stray patch. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch115-math-a06a1584_Arja_PatchNaturalnessYe$$Remove a bit of a bit of code. Remove inside attribute from patch .. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch58-math-a06a1584_Arja_PatchNaturalnessYe$$fixed a small bug. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch93-math-a06a1584_Arja_PatchNaturalnessYe$$fixed a small bug. removed a couple of tries to fix the build. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch185-math-a06a1584_Arja_PatchNaturalnessYe$$ handle erroneously in plane constructor. added missing visit to fix # 12. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch176-math-a06a1584_Arja_PatchNaturalnessYe$$Fixed a bug in PolyhedronsSet where the boundaryFacet ( point , node ). Remove a bit of code that was accidentally left in the last commit. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch94-math-a06a1584_Arja_PatchNaturalnessYe$$fixed a minor issue with closed opengl operations. Remove a bit of code that was accidentally left in the last commit. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch182-math-a06a1584_Arja_PatchNaturalnessYe$$Added fix for added facets contribution. Remove a bit of code that was accidentally left in the last commit. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch69-math-a06a1584_Arja_PatchNaturalnessYe$$fixed a small bug. Remove stray check for edge effect. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch140-math-a06a1584_Arja_PatchNaturalnessYe$$added missing visit to fix # 12. Add StorelessCovariance patch. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch56-math-a06a1584_Arja_PatchNaturalnessYe$$Set barycenter manually .. Remove a bit of code that was accidentally left in the last commit. Remove stray return statement. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch178-math-a06a1584_Arja_PatchNaturalnessYe$$fixed a bug in plane constructor. fixed a small bug. Remove a bit of code that was accidentally left in the last commit. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch51-math-a06a1584_Arja_PatchNaturalnessYe$$Fixed a bug in the - - fixed - search order .. Set the size of the cut hyperplane if it was not found .. Remove stray patch. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch113-math-a06a1584_Arja_PatchNaturalnessYe$$Add a missing line. Remove a bit of code that was accidentally left in the last commit. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch114-math-a06a1584_Arja_PatchNaturalnessYe$$Set the barycenter of the PolyhedronsSet if it could not find a. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch122-math-a06a1584_Arja_PatchNaturalnessYe$$fixed a small bug. Remove a redundant patch. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch34-math-a06a1584_Arja_PatchNaturalnessYe$$Add missing patch. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch310-math-a06a1584_Arja_PatchNaturalnessYe$$Fixed a bug in PolyhedronsSet with fixed size .. Remove stray check for edge effect. fixed erroneous fallthrough in plane constructor. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch125-math-a06a1584_Arja_PatchNaturalnessYe$$Set barycenter. Removed a couple of debug code. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch265-math-a06a1584_Arja_PatchNaturalnessYe$$Added missing patch. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch296-math-a06a1584_Arja_PatchNaturalnessYe$$Added missing patch. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch262-math-a06a1584_Arja_PatchNaturalnessYe$$added missing visit. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch254-math-a06a1584_Arja_PatchNaturalnessYe$$removed a redundant line. Remove stray check for inside attribute. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch298-math-a06a1584_Arja_PatchNaturalnessYe$$Added missing visit to fix # 183. Remove a bit of code that was accidentally left in the last commit. Remove stray return statement. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch207-math-a06a1584_Arja_PatchNaturalnessYe$$Set barycenter to 1 . 0 / ( 4 * getSize ( ) ). Remove a bit of a hack. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch238-math-a06a1584_Arja_PatchNaturalnessYe$$Set the barycenter of the PolyhedronsSet object in patch .. fixed a small bug. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch236-math-a06a1584_Arja_PatchNaturalnessYe$$Added a fix for the near branch search. fixed a small bug. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch209-math-a06a1584_Arja_PatchNaturalnessYe$$Set barycenter manually .. removed a redundant check. Remove stray return statement. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch299-math-a06a1584_Arja_PatchNaturalnessYe$$Added a line of code to check for crossed edges. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch252-math-a06a1584_Arja_PatchNaturalnessYe$$set barycenter again. removed patch for now. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch255-math-a06a1584_Arja_PatchNaturalnessYe$$Remove a bit of code that was accidentally left in the last commit. Removed fixed bug in JSF. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch263-math-a06a1584_Arja_PatchNaturalnessYe$$added missing visit to fix # 1568. Remove inside attribute from patch .. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch297-math-a06a1584_Arja_PatchNaturalnessYe$$added missing visit. Remove stray return statement. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch290-math-a06a1584_Arja_PatchNaturalnessYe$$added missing visit to fix # 1796. Remove stray check for inside attribute. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch237-math-a06a1584_Arja_PatchNaturalnessYe$$increased the size of the PolyhedronsSet to 3 . 0. removed a redundant line. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch208-math-a06a1584_Arja_PatchNaturalnessYe$$removed a redundant line. Fix NPE in code. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch201-math-a06a1584_Arja_PatchNaturalnessYe$$Fixed a bug in PolyhedronsSet .. Readded missing visit. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch206-math-a06a1584_Arja_PatchNaturalnessYe$$Removed a couple of debug code. Removed fixed size .. Plane intersection method used for testing build break. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch239-math-a06a1584_Arja_PatchNaturalnessYe$$fixed a small bug. Set size to 0 when point is not contained. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch224-math-a06a1584_Arja_PatchNaturalnessYe$$Set cut size to 0. fixed a small bug. Remove stray check for edge effect. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch223-math-a06a1584_Arja_PatchNaturalnessYe$$fixed null cut. Remove a bit of code that was accidentally left in the last commit. Remove stray patch. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch215-math-a06a1584_Arja_PatchNaturalnessYe$$removed patch from accidently removed PR. Remove stray return statement. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch212-math-a06a1584_Arja_PatchNaturalnessYe$$Set the size of the cut node to zero in case of a geometry change. Remove boundaryFacet from PolyhedronsSet. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch279-math-a06a1584_Arja_PatchNaturalnessYe$$fixed a crash. Remove stray check for edge effect. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch241-math-a06a1584_Arja_PatchNaturalnessYe$$Fixed a bug in PolyhedronsSet .. Remove stray patch. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch277-math-a06a1584_Arja_PatchNaturalnessYe$$Set the size of the cut hyperplane if it was not found. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch248-math-a06a1584_Arja_PatchNaturalnessYe$$Remove a bit of code that was accidentally left in the last commit. Removed patch from pull request. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch284-math-a06a1584_Arja_PatchNaturalnessYe$$Fixed a minor issue with fixed geometry size .. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch214-math-a06a1584_Arja_PatchNaturalnessYe$$Set the barycenter of the PolyhedronsSet object in firstIntersection. Remove a bit of a hack. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch222-math-a06a1584_Arja_PatchNaturalnessYe$$throw exception if crossed line is not found. Fixed a bug in PolyhedronsSet with negative size .. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch225-math-a06a1584_Arja_PatchNaturalnessYe$$Fix accidently removed line segment .. Remove stray return statement. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch285-math-a06a1584_Arja_PatchNaturalnessYe$$fixed a small bug. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch282-math-a06a1584_Arja_PatchNaturalnessYe$$Remove a bit of code that was accidentally left in the last commit. Set size to 0 when point is not contained. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch240-math-a06a1584_Arja_PatchNaturalnessYe$$Set barycenter of diff node. Removed a couple of debug code. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch247-math-a06a1584_Arja_PatchNaturalnessYe$$fixed a small bug. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch278-math-a06a1584_Arja_PatchNaturalnessYe$$Removed a couple incorrect lines. Remove stray return statement. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch107-math-a06a1584_Arja_PatchNaturalnessYe$$Remove stray check for edge effect. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch11-math-a06a1584_Arja_PatchNaturalnessYe$$Plane intersection method used for 1 . 5. Set the size of the cut node to 3 . 0. Remove a bit of code that was accidentally left in the last commit. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch100-math-a06a1584_Arja_PatchNaturalnessYe$$Set the barycenter of the PolyhedronsSet object to the same value as. Removed a couple of debug code. Remove stray return statement. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch16-math-a06a1584_Arja_PatchNaturalnessYe$$fixed a minor issue with closed opengl operations. Remove stray patch. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch109-math-a06a1584_Arja_PatchNaturalnessYe$$Revert previous patch. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch304-math-a06a1584_Arja_PatchNaturalnessYe$$Resetting frame opengl opengl opengl opengl opengl opengl opengl opengl opengl opengl opengl opengl opengl opengl opengl opengl. Added visit to fix # 12. Remove a bit of code that was accidentally left in the last commit. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch20-math-a06a1584_Arja_PatchNaturalnessYe$$fixed a small bug. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch136-math-a06a1584_Arja_PatchNaturalnessYe$$Added fix for # 274. Remove a bit of code that was accidentally left in the last commit. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch18-math-a06a1584_Arja_PatchNaturalnessYe$$Added fix for FacetsContributionVisitor. added missing visit to fix # 12. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch27-math-a06a1584_Arja_PatchNaturalnessYe$$added missing visit to fix # 12. Readded missing visit. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch303-math-a06a1584_Arja_PatchNaturalnessYe$$Remove a couple of debug code. Added missing visit to fix # 30. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch191-math-a06a1584_Arja_PatchNaturalnessYe$$Set barycenter again .. Remove a bit of code that was accidentally left in the last commit. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch87-math-a06a1584_Arja_PatchNaturalnessYe$$Add a boundary facet test to PolyhedronsSet. Remove a bit of code that was accidentally left in the last commit. Remove stray return statement. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch162-math-a06a1584_Arja_PatchNaturalnessYe$$Remove a bit of code that was accidentally left in the last commit. Reverted accidental change of walk ( ). $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch74-math-a06a1584_Arja_PatchNaturalnessYe$$Set the size of the cut node to 3 . 0. Removed patch to prevent null error in PolyhedronsSet. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch196-math-a06a1584_Arja_PatchNaturalnessYe$$Set barycenter for removed nodes. fixed a small bug. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch80-math-a06a1584_Arja_PatchNaturalnessYe$$Remove a bit of a cruft. Remove stray patch. Add a missing + in StorelessCovariance. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch154-math-a06a1584_Arja_PatchNaturalnessYe$$fixed a small bug. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch5-math-a06a1584_Arja_PatchNaturalnessYe$$ handle erroneously in plane constructor. Fixed a minor issue with missing setBarycenter. removed a redundant line. Remove stray return statement. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch89-math-a06a1584_Arja_PatchNaturalnessYe$$Remove a problematic empty line. Removed patch for setting barycenter. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch2-math-a06a1584_Arja_PatchNaturalnessYe$$Added visit to fix added facets contribution. added missing visit to fix # 12. Remove stray return statement. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch45-math-a06a1584_Arja_PatchNaturalnessYe$$fixed a small bug. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch19-math-a06a1584_Arja_PatchNaturalnessYe$$fixed a small bug. Remove stray patch. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch26-math-a06a1584_Arja_PatchNaturalnessYe$$Reset the size of the PolyhedronsSet as well. Remove a redundant patch. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch108-math-a06a1584_Arja_PatchNaturalnessYe$$added cut null check. removed a couple of lines from the edge case. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch21-math-a06a1584_Arja_PatchNaturalnessYe$$Removed a couple of debug code. Removed patch for inside attribute. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch28-math-a06a1584_Arja_PatchNaturalnessYe$$fixed a small bug. Removed a couple of debug code. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch101-math-a06a1584_Arja_PatchNaturalnessYe$$added missing visit to getTree ( true ). added missing visit to fix # 12. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch17-math-a06a1584_Arja_PatchNaturalnessYe$$@@ fix_euclidean_to_world refactoring. Remove a bit of a bit of code. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch139-math-a06a1584_Arja_PatchNaturalnessYe$$Remove a bit of code that was accidentally left in the last commit. Revert accidently removed patch. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch106-math-a06a1584_Arja_PatchNaturalnessYe$$Set barycenter manually .. Fixed a bug in PolyhedronsSet where the cut hyperplane was not found .. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch10-math-a06a1584_Arja_PatchNaturalnessYe$$removed a redundant line. Changed boundary facet point to be in the same region as the point .. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch3-math-a06a1584_Arja_PatchNaturalnessYe$$Fixed a bug in PolyhedronsSet .. Remove stray check for inside attribute. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch199-math-a06a1584_Arja_PatchNaturalnessYe$$Fixed a bug in the - - fixed - search order .. Remove a bit of code that was accidentally left in the last commit. Remove inside attribute from patch .. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch4-math-a06a1584_Arja_PatchNaturalnessYe$$fixed a small bug. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch155-math-a06a1584_Arja_PatchNaturalnessYe$$Remove a couple of debug code. Set the size of the PolyhedronsSet attribute to zero if the region is not. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch88-math-a06a1584_Arja_PatchNaturalnessYe$$fixed a small bug. Remove a redundant patch. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch197-math-a06a1584_Arja_PatchNaturalnessYe$$Add a fix for crossing the line. Remove stray check for inside attribute. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch81-math-a06a1584_Arja_PatchNaturalnessYe$$fixed bug. Remove a bit of code that was accidentally left in the last commit. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch163-math-a06a1584_Arja_PatchNaturalnessYe$$Add a missing + in StorelessCovariance. Set barycenter manually .. Remove a bit of code that was accidentally left in the last commit. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch75-math-a06a1584_Arja_PatchNaturalnessYe$$increased the size of the PolyhedronsSet to 3 . 0. Removed a redundant call to setBarycenter ( ). $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch164-math-a06a1584_Arja_PatchNaturalnessYe$$Remove stray return statement. Add StorelessCovariance patch. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch190-math-a06a1584_Arja_PatchNaturalnessYe$$Oops , now search in the far branch , but found a null pointer. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch86-math-a06a1584_Arja_PatchNaturalnessYe$$fixed a small bug. Remove a bit of code that was accidentally left in the last commit. $$0
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch36-math-2a6c6409_Arja_PatchNaturalnessYe$$removed throw. fixed a bug in OrderedTuple. $$0
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch54-math-2a6c6409_Arja_PatchNaturalnessYe$$update pending array. removed throw. $$0
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch53-math-2a6c6409_Arja_PatchNaturalnessYe$$removed throw new MathInternalError ( ). Fix swapped inf / inf in OrderedTuple. $$0
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch62-math-2a6c6409_Arja_PatchNaturalnessYe$$removed throw. BSPTree now uses the modified cut property. $$0
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch39-math-2a6c6409_Arja_PatchNaturalnessYe$$removed extraneous whitespace. removed throw new MathInternalError ( ). $$0
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch37-math-2a6c6409_Arja_PatchNaturalnessYe$$Fix euclidean intersections , added missing patch. Fix swapped offset values in OrderedTuple. removed throw. $$0
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch63-math-2a6c6409_Arja_PatchNaturalnessYe$$Fix parallel hyperplanes , added patch. removed patch for BSPTree. removed throw. $$0
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch64-math-2a6c6409_Arja_PatchNaturalnessYe$$removed throw. Set exception context to empty string context for now. $$0
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch52-math-2a6c6409_Arja_PatchNaturalnessYe$$removed patch from BSPTree. removed throw. $$0
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch55-math-2a6c6409_Arja_PatchNaturalnessYe$$removed throw. BSPTree now cuts off - the - minus sign .. $$0
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch8-math-2a6c6409_Arja_PatchNaturalnessYe$$removed throw. removed patch from BSPTree. $$0
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch1-math-2a6c6409_Arja_PatchNaturalnessYe$$Add blank line. removed throw new MathInternalError ( ). $$0
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch6-math-2a6c6409_Arja_PatchNaturalnessYe$$removed throw new MathInternalError ( ). fixed a bug in OrderedTuple. $$0
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch41-math-2a6c6409_Arja_PatchNaturalnessYe$$fixed a small bug in OrderedTuple. removed throw. $$0
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch24-math-2a6c6409_Arja_PatchNaturalnessYe$$Missing patch for getting message from default locale .. removed throw new MathInternalError ( ). $$0
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch7-math-2a6c6409_Arja_PatchNaturalnessYe$$fixed a bug in OrderedTuple. removed throw. $$0
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch9-math-2a6c6409_Arja_PatchNaturalnessYe$$BSPTree no longer links ( cut = null ). removed throw new MathInternalError ( ). $$0
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch49-math-2a6c6409_Arja_PatchNaturalnessYe$$Fix Euclidean geometry patch. removed throw new MathInternalError ( ). $$0
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch22-math-2a6c6409_Arja_PatchNaturalnessYe$$Fix swapped inf / inf in OrderedTuple. removed throw new MathInternalError ( ). $$0
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch25-math-2a6c6409_Arja_PatchNaturalnessYe$$removed throw. Fixed a typo in code. $$0
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch14-math-2a6c6409_Arja_PatchNaturalnessYe$$removed throw new MathInternalError ( ). Fix swapped offset values in OrderedTuple. $$0
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch50-math-2a6c6409_Arja_PatchNaturalnessYe$$removed throw. reset parent links. $$0
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch57-math-2a6c6409_Arja_PatchNaturalnessYe$$removed patch for BSPTree. fixed a bug in OrderedTuple. removed throw new MathInternalError ( ). $$0
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch61-math-2a6c6409_Arja_PatchNaturalnessYe$$removed throw new MathInternalError ( ). Fix swapped offset values in OrderedTuple. Revert accidently removed patch .. $$0
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch59-math-2a6c6409_Arja_PatchNaturalnessYe$$BSPTree now uses the same cut value as the others. fixed a small bug in OrderedTuple. removed throw new MathInternalError ( ). $$0
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch32-math-2a6c6409_Arja_PatchNaturalnessYe$$Fix swapped offset values in OrderedTuple. removed throw new MathInternalError ( ). $$0
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch58-math-2a6c6409_Arja_PatchNaturalnessYe$$removed throw. Revert since it was removed as part of a revu 58396 .. $$0
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch56-math-2a6c6409_Arja_PatchNaturalnessYe$$removed throw. Fix euclidean intersections , add patch. remove patch. $$0
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch34-math-2a6c6409_Arja_PatchNaturalnessYe$$BSPTree now has a null pointer. removed throw. $$0
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch2-math-2a6c6409_Arja_PatchNaturalnessYe$$fixed possible NPE in OrderedTuple. removed throw. $$0
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch26-math-2a6c6409_Arja_PatchNaturalnessYe$$removed throw. Fix swapped inf / inf in OrderedTuple. $$0
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch21-math-2a6c6409_Arja_PatchNaturalnessYe$$Set lsb to Integer . MAX_VALUE for OrderedTuple. removed throw. $$0
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch17-math-2a6c6409_Arja_PatchNaturalnessYe$$removed throw. removed patch from BSPTree. $$0
math-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)$$patch3-math-2a6c6409_Arja_PatchNaturalnessYe$$removed throw new MathInternalError ( ). Reset inf / negInf after first try. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch36-math-faf99727_Arja_PatchNaturalnessYe$$Added extreme . add ( 0 , farthest ) to ball .. removed patch for E3E. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch111-math-faf99727_Arja_PatchNaturalnessYe$$removed patch for E3E. Fix Reverted the patch .. removed patch from extremes. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch116-math-faf99727_Arja_PatchNaturalnessYe$$Remove a couple of errors. moving the first extreme point to the front. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch189-math-faf99727_Arja_PatchNaturalnessYe$$Fix a minor issue with the + 1 . 5 error in AffectEntityClass .. moving ball to the front of the extreme list. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch145-math-faf99727_Arja_PatchNaturalnessYe$$Fix a typo in code. removed patch for E3E. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch62-math-faf99727_Arja_PatchNaturalnessYe$$Fix a typo in code. Fix NPE in RegenerateBallOfPoints , fixes # 1615. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch110-math-faf99727_Arja_PatchNaturalnessYe$$Fix the merge of AffectLineEnd to trigger the rebase. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch39-math-faf99727_Arja_PatchNaturalnessYe$$Remove a couple of errors. removed extraneous line. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch30-math-faf99727_Arja_PatchNaturalnessYe$$Remove a couple of errors. Revert previous patch. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch37-math-faf99727_Arja_PatchNaturalnessYe$$Fix a potential NPE in AffectEntityClassPath. removed extra extreme loop. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch63-math-faf99727_Arja_PatchNaturalnessYe$$Reverted the patch of MathIllegalStateException. Fix E3e bug. removed a couple incorrect lines. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch181-math-faf99727_Arja_PatchNaturalnessYe$$Remove a couple of errors. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch186-math-faf99727_Arja_PatchNaturalnessYe$$removed patch from refactoring , see. Fixed a bug in the extremes of WelzlEncloser .. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch188-math-faf99727_Arja_PatchNaturalnessYe$$Fix a minor issue with the patch .. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch143-math-faf99727_Arja_PatchNaturalnessYe$$Add missing patch. Remove a couple of errors. moving ball to the front of the extreme list. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch55-math-faf99727_Arja_PatchNaturalnessYe$$Remove patch from refactoring .. Add missing patch. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch203-math-faf99727_Arja_PatchNaturalnessYe$$removed extreme . add ( points . get ( 0 ) ) from proguard. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch205-math-faf99727_Arja_PatchNaturalnessYe$$Fix NPE in ball on support from infinite loop. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch211-math-faf99727_Arja_PatchNaturalnessYe$$Remove a couple of debug files. Fix E3E4F1E3E2E .. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch161-math-faf99727_Arja_PatchNaturalnessYe$$removed patch for E3E. Fix the loop. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch77-math-faf99727_Arja_PatchNaturalnessYe$$removed a recursive loop. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch195-math-faf99727_Arja_PatchNaturalnessYe$$removed a redundant throw. removed extra step. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch48-math-faf99727_Arja_PatchNaturalnessYe$$Fix NPE in AffectEntityException. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch84-math-faf99727_Arja_PatchNaturalnessYe$$removed extraneous whitespace. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch150-math-faf99727_Arja_PatchNaturalnessYe$$removed patch from code. fix the infinite loop. Fix rebase. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch1-math-faf99727_Arja_PatchNaturalnessYe$$removed extraneous comment. fix the infinite loop. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch103-math-faf99727_Arja_PatchNaturalnessYe$$Fix NPE in AffectEntityException. Updated patch to fix focal length issue. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch15-math-faf99727_Arja_PatchNaturalnessYe$$Fix EAP. Remove a couple of errors. Fix rebase. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch156-math-faf99727_Arja_PatchNaturalnessYe$$removed extraneous line. removed patch from refactoring , see. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch47-math-faf99727_Arja_PatchNaturalnessYe$$removed null check. removed patch from refactoring , see. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch78-math-faf99727_Arja_PatchNaturalnessYe$$removed patch for E3E. moving extremes to the front ball. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch167-math-faf99727_Arja_PatchNaturalnessYe$$removed a redundant throw. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch71-math-faf99727_Arja_PatchNaturalnessYe$$Remove patch from refactoring .. removed extra extreme loop. Fix the loop. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch158-math-faf99727_Arja_PatchNaturalnessYe$$removed patch from refactoring , see. fix the edge case. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch82-math-faf99727_Arja_PatchNaturalnessYe$$Fix a potential NPE in AffectEntityClassPath. Readded missing support array in extremes. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch134-math-faf99727_Arja_PatchNaturalnessYe$$Remove a couple of errors. Readded missing support . clear ( ) after welzl encloser patch. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch179-math-faf99727_Arja_PatchNaturalnessYe$$Added patch for MathIllegalStateException. Remove a couple of debug files. Updated the zip code. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch183-math-faf99727_Arja_PatchNaturalnessYe$$Revert previous patch. removed patch from previous commit , because it probably didn ' t work. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch177-math-faf99727_Arja_PatchNaturalnessYe$$Added patch for MathIllegalStateException. removed patch from refactoring , see. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch124-math-faf99727_Arja_PatchNaturalnessYe$$Fix a bit of code. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch185-math-faf99727_Arja_PatchNaturalnessYe$$Remove a couple of errors. Fix EAP. Removed a moveToFrontBall that was not necessary , fixes ryas. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch149-math-faf99727_Arja_PatchNaturalnessYe$$Revert wrong patch. Remove patch for EulerBalls. Fix the for - loop of a bug in the geometry encloser patch .. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch182-math-faf99727_Arja_PatchNaturalnessYe$$removed extreme . add ( points . get ( 0 ) ) from proguard. Readded missing clear ( ) call. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch113-math-faf99727_Arja_PatchNaturalnessYe$$Fix NPE in newly added piece. Remove a couple of errors. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch209-math-faf99727_Arja_PatchNaturalnessYe$$removed a couple incorrect lines. removed extra extreme loop. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch26-math-faf99727_Arja_PatchNaturalnessYe$$removed patch for E3E. removed extra step. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch10-math-faf99727_Arja_PatchNaturalnessYe$$Remove a couple of errors. removed a couple incorrect lines. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch44-math-faf99727_Arja_PatchNaturalnessYe$$Fix E3e bug. removed patch for E3E. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch152-math-faf99727_Arja_PatchNaturalnessYe$$Remove a couple of errors. Fix EAP. $$0
math-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples$$patch43-math-faf99727_Arja_PatchNaturalnessYe$$removed patch for E3E. Updated patch for welzlEncloser. $$0
math-8dd22390$$Wrong parameter for first step size guess for Embedded Runge Kutta methods$$In a space application using DOP853 i detected what seems to be a bad parameter in the call to the method  initializeStep of class AdaptiveStepsizeIntegrator.  Here, DormandPrince853Integrator is a subclass for EmbeddedRungeKuttaIntegrator which perform the call to initializeStep at the beginning of its method integrate(...)  The problem comes from the array "scale" that is used as a parameter in the call off initializeStep(..)  Following the theory described by Hairer in his book "Solving Ordinary Differential Equations 1 : Nonstiff Problems", the scaling should be :  sci = Atol i + |y0i| * Rtoli  Whereas EmbeddedRungeKuttaIntegrator uses :  sci = Atoli  Note that the Gragg-Bulirsch-Stoer integrator uses the good implementation "sci = Atol i + |y0i| * Rtoli  " when he performs the call to the same method initializeStep(..)  In the method initializeStep, the error leads to a wrong step size h used to perform an  Euler step. Most of the time it is unvisible for the user. But in my space application the Euler step with this wrong step size h (much bigger than it should be)  makes an exception occur (my satellite hits the ground...)   To fix the bug, one should use the same algorithm as in the rescale method in GraggBulirschStoerIntegrator For exemple :   final double[] scale= new double[y0.length];;                      if (vecAbsoluteTolerance == null) {               for (int i = 0; i < scale.length; ++i) {                 final double yi = Math.max(Math.abs(y0[i]), Math.abs(y0[i]));                 scale[i] = scalAbsoluteTolerance + scalRelativeTolerance * yi;               }             } else {               for (int i = 0; i < scale.length; ++i) {                 final double yi = Math.max(Math.abs(y0[i]), Math.abs(y0[i]));                 scale[i] = vecAbsoluteTolerance[i] + vecRelativeTolerance[i] * yi;               }             }                      hNew = initializeStep(equations, forward, getOrder(), scale,                            stepStart, y, yDotK[0], yTmp, yDotK[1]);    Sorry for the length of this message, looking forward to hearing from you soon  Vincent Morand$$patch2-math-8dd22390_RSRepair_PatchNaturalnessYe$$increased the maximum growth from the patched code. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch1-math-f6dd42b4_RSRepair_PatchNaturalnessYe$$BrentSolver now uses verifyBracketing ( ). $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch3-math-f6dd42b4_RSRepair_PatchNaturalnessYe$$Fix null pointer exception in UnivariateRealSolverImpl . verifyInterval. revert accidently change. $$0
math-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch4-math-f6dd42b4_RSRepair_PatchNaturalnessYe$$Repair fix from # 77. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch30-math-596ccd59_RSRepair_PatchNaturalnessYe$$Remove the scaleInPlace ( ) from the array .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch1-math-596ccd59_RSRepair_PatchNaturalnessYe$$Remove unused code. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch15-math-596ccd59_RSRepair_PatchNaturalnessYe$$Reverted the change that removed from another PR to avoid additional NPE in the same way. Remove unused membership matrix .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch12-math-596ccd59_RSRepair_PatchNaturalnessYe$$Fix the build .. Remove a bit from the RS repair bug. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch24-math-596ccd59_RSRepair_PatchNaturalnessYe$$Fix bug in FuzzyKMeansClusterer .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch23-math-596ccd59_RSRepair_PatchNaturalnessYe$$Removed patch from fuzzykmeans clusterer. Fix one more test. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch7-math-596ccd59_RSRepair_PatchNaturalnessYe$$Fix the bug. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch9-math-596ccd59_RSRepair_PatchNaturalnessYe$$added missing checkNotNull check. Remove a couple incorrect entries. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch22-math-596ccd59_RSRepair_PatchNaturalnessYe$$Fix the indentation of FuzzyKMeansClusterer. Removed patch for clustering bug. added missing int. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch25-math-596ccd59_RSRepair_PatchNaturalnessYe$$Fix the build .. Fix a small bug in FuzzyKMeansClusterer. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch13-math-596ccd59_RSRepair_PatchNaturalnessYe$$Remove unused code. Fix one more test. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch14-math-596ccd59_RSRepair_PatchNaturalnessYe$$Normalize membership matrix in case of no points provided. Removed patch for max membership in FuzzyKMeansClusterer. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch32-math-596ccd59_RSRepair_PatchNaturalnessYe$$Remove minDistance from newly added piece. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch33-math-596ccd59_RSRepair_PatchNaturalnessYe$$Fix the for loop. Remove unused code. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch11-math-596ccd59_RSRepair_PatchNaturalnessYe$$Fixing the bug in FuzzyKMeansClusterer .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch16-math-596ccd59_RSRepair_PatchNaturalnessYe$$Remove unused code. Removed fixed distSqSum .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch20-math-596ccd59_RSRepair_PatchNaturalnessYe$$Removed too small exception. Fix the for - loop of FuzzyKMeansClusterer .. Fix the for loop. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch27-math-596ccd59_RSRepair_PatchNaturalnessYe$$removed empty line. Add a null check. Fix the build. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch5-math-596ccd59_RSRepair_PatchNaturalnessYe$$Fix the build. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch2-math-596ccd59_RSRepair_PatchNaturalnessYe$$throw exception if size < k. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch19-math-596ccd59_RSRepair_PatchNaturalnessYe$$removed empty line. Remove unused code. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch26-math-596ccd59_RSRepair_PatchNaturalnessYe$$Fix a bug in FuzzyKMeansClusterer with no points provided. Add a null check. Fix the indentation of FuzzyKMeansClusterer .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch21-math-596ccd59_RSRepair_PatchNaturalnessYe$$Fix assignedDifferently in FuzzyKMeansClusterer .. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch28-math-596ccd59_RSRepair_PatchNaturalnessYe$$Throw the right exception if fuzziness is not met .. Fix single line error. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch17-math-596ccd59_RSRepair_PatchNaturalnessYe$$Fix the compilation with Java 1 . 6. Fix membership matrix. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch10-math-596ccd59_RSRepair_PatchNaturalnessYe$$Remove a couple of unused patches. Removed patch from fuzzykmeans clusterer. Fix one more test. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch3-math-596ccd59_RSRepair_PatchNaturalnessYe$$Fix the issue of NPE in FuzzyKMeansClusterer .. Remove unused code. $$0
math-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;$$patch4-math-596ccd59_RSRepair_PatchNaturalnessYe$$Removed patch for clustering bug. Remove spurious j ++ for newClusters .. $$0
math-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);$$patch1-math-9e0c5ad4_RSRepair_PatchNaturalnessYe$$Fix the recurrence relation patch .. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch1-math-69273dca_RSRepair_PatchNaturalnessYe$$Fix RSRepair_Bug_dot_jar_Commons - Math_69273. $$0
math-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch2-math-69273dca_RSRepair_PatchNaturalnessYe$$Fix bug with last step. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch38-math-a06a1584_RSRepair_PatchNaturalnessYe$$fixed search error. Removed unused local variable .. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch39-math-a06a1584_RSRepair_PatchNaturalnessYe$$Fix error in previous patch. Fixed a bug in the plane accident patch .. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch30-math-a06a1584_RSRepair_PatchNaturalnessYe$$Added missing point exception. Removed patch from PolyhedronsSet. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch1-math-a06a1584_RSRepair_PatchNaturalnessYe$$Fix bug in JSF. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch6-math-a06a1584_RSRepair_PatchNaturalnessYe$$Fix RSRepair. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch15-math-a06a1584_RSRepair_PatchNaturalnessYe$$Fix BSP tree reference. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch23-math-a06a1584_RSRepair_PatchNaturalnessYe$$Changed the size of the cross section of the PolyhedronsSet .. Remove stray return statement. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch40-math-a06a1584_RSRepair_PatchNaturalnessYe$$Fix RSRepair. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch7-math-a06a1584_RSRepair_PatchNaturalnessYe$$Fix RSRepair. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch9-math-a06a1584_RSRepair_PatchNaturalnessYe$$Removed fixed accident in RSRepair , removed removed test. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch22-math-a06a1584_RSRepair_PatchNaturalnessYe$$Fixed a bug in RSRepair .. Remove stray patch. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch25-math-a06a1584_RSRepair_PatchNaturalnessYe$$Fix RS - 7715. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch13-math-a06a1584_RSRepair_PatchNaturalnessYe$$Added missing point exception. Removed a bit of a test code. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch14-math-a06a1584_RSRepair_PatchNaturalnessYe$$Removed stray line from previous fix .. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch32-math-a06a1584_RSRepair_PatchNaturalnessYe$$Removed a redundant check. Remove stray check for inside attribute. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch35-math-a06a1584_RSRepair_PatchNaturalnessYe$$Fixed a bug in the near branch of PolyhedronsSet. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch33-math-a06a1584_RSRepair_PatchNaturalnessYe$$Removed broken patch. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch11-math-a06a1584_RSRepair_PatchNaturalnessYe$$Removed a redundant empty line. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch29-math-a06a1584_RSRepair_PatchNaturalnessYe$$Add quat attribute to PolyhedronsSet. Remove stray check for inside attribute. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch16-math-a06a1584_RSRepair_PatchNaturalnessYe$$updated fix. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch18-math-a06a1584_RSRepair_PatchNaturalnessYe$$Removed a bit of a debug code. Removed fixed bug in RSRepair_Bug_dot_jar_Commons. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch27-math-a06a1584_RSRepair_PatchNaturalnessYe$$Add missing patch. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch5-math-a06a1584_RSRepair_PatchNaturalnessYe$$Fix RS - 7733 - missing final double [] []. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch2-math-a06a1584_RSRepair_PatchNaturalnessYe$$Changed size of the near branch to 3 . 0. Removed a bit of a hack to fix erroneous build .. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch19-math-a06a1584_RSRepair_PatchNaturalnessYe$$Fixed bug in RSRepair. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch28-math-a06a1584_RSRepair_PatchNaturalnessYe$$Removed a couple of debug printlns. Changed possible value of setBarycenter. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch17-math-a06a1584_RSRepair_PatchNaturalnessYe$$Removed a bit of a rebase. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch10-math-a06a1584_RSRepair_PatchNaturalnessYe$$Removed a unused variable. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch44-math-a06a1584_RSRepair_PatchNaturalnessYe$$Fix NPE in PolyhedronsSet .. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch3-math-a06a1584_RSRepair_PatchNaturalnessYe$$Fix broken patch. Removed a bit of a test code. $$0
math-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.$$patch43-math-a06a1584_RSRepair_PatchNaturalnessYe$$Fix RSRepair. $$0
